{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vectorize import *\n",
    "from src.lda import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données et pré-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe 10 000 articles Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pauline Donalda,  (March 5, 1882 – October 22,...</td>\n",
       "      <td>Pauline Donalda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a list of German football transfers in...</td>\n",
       "      <td>List of German football transfers summer 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "1  Pauline Donalda,  (March 5, 1882 – October 22,...   \n",
       "2  This is a list of German football transfers in...   \n",
       "3  Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4  Monique Ganderton (born August 6, 1980) is a C...   \n",
       "\n",
       "                                           Title  \n",
       "0                               Joseph Greenberg  \n",
       "1                                Pauline Donalda  \n",
       "2  List of German football transfers summer 2017  \n",
       "3                                  Lester Hudson  \n",
       "4                              Monique Ganderton  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 10000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\yanis\\\\OneDrive\\\\Documents\\\\ENSAE 3A\\\\Sequential MC\\\\SSM-PROJECT\\\\wiki_data.csv\",\n",
    "                    encoding='utf-8',\n",
    "                    delimiter=\";\")\n",
    "del data[\"Unnamed: 0\"]\n",
    "display(data.head())\n",
    "print(\"Number of articles: {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On tokenize chaque article: \"Joseph Harold Greenberg\" devient [\"Jospeh\", \"Harold\", \"Greenberg\"]\n",
    "2. On filtre les articles: on retire les articles composés de moins de 500 mots\n",
    "3. On vectorise le texte: on associe à chaque mot son indice dans le vocabulaire général.\n",
    "\n",
    "Exemple: si tous les articles peuvent contenir comme mots: [\"pomme\", \"poire\", \"chocolat\", \"eau\", \"banane\"], l'article [\"chocolat\", \"banane\", \"pomme\"] devient [2, 4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "      <td>[Joseph, Harold, Greenberg, (, May, 28, ,, 191...</td>\n",
       "      <td>[68836, 61034, 58682, 1182, 80975, 13183, 1242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "      <td>[Lester, Hudson, III, (, born, August, 7, ,, 1...</td>\n",
       "      <td>[75482, 63781, 64659, 1182, 134097, 26662, 181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "      <td>[Monique, Ganderton, (, born, August, 6, ,, 19...</td>\n",
       "      <td>[84017, 56054, 1182, 134097, 26662, 17386, 124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The white bikini of Ursula Andress (also known...</td>\n",
       "      <td>White bikini of Ursula Andress</td>\n",
       "      <td>[The, white, bikini, of, Ursula, Andress, (, a...</td>\n",
       "      <td>[115808, 188693, 133263, 165140, 120335, 24145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Breakout\" is a single from British act Swing ...</td>\n",
       "      <td>Breakout (Swing Out Sister song)</td>\n",
       "      <td>[``, Breakout, '', is, a, single, from, Britis...</td>\n",
       "      <td>[127812, 32984, 6, 156142, 127813, 178055, 149...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "3   Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4   Monique Ganderton (born August 6, 1980) is a C...   \n",
       "6   The white bikini of Ursula Andress (also known...   \n",
       "15  \"Breakout\" is a single from British act Swing ...   \n",
       "\n",
       "                               Title  \\\n",
       "0                   Joseph Greenberg   \n",
       "3                      Lester Hudson   \n",
       "4                  Monique Ganderton   \n",
       "6     White bikini of Ursula Andress   \n",
       "15  Breakout (Swing Out Sister song)   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "0   [Joseph, Harold, Greenberg, (, May, 28, ,, 191...   \n",
       "3   [Lester, Hudson, III, (, born, August, 7, ,, 1...   \n",
       "4   [Monique, Ganderton, (, born, August, 6, ,, 19...   \n",
       "6   [The, white, bikini, of, Ursula, Andress, (, a...   \n",
       "15  [``, Breakout, '', is, a, single, from, Britis...   \n",
       "\n",
       "                                      vectorized_text  \n",
       "0   [68836, 61034, 58682, 1182, 80975, 13183, 1242...  \n",
       "3   [75482, 63781, 64659, 1182, 134097, 26662, 181...  \n",
       "4   [84017, 56054, 1182, 134097, 26662, 17386, 124...  \n",
       "6   [115808, 188693, 133263, 165140, 120335, 24145...  \n",
       "15  [127812, 32984, 6, 156142, 127813, 178055, 149...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_data=vectorize_data(data=data, min_number_words=500)\n",
    "display(vectorized_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En attendant d'avoir $z^*_{1:T}$, on applique le LDA sur le corpus avec K=50 topics. Chaque mot a alors un poids relativement à chaque topic.\n",
    "\n",
    "Exemple: le mot banane a les poids suivant: {\"topic 1\": 0.3, \"topic 2\": 0.1, \"topic 3\": 0.05,...}. \n",
    "\n",
    "On associe au mot le topic avec le poids le plus élevé. En l'occurence 1. De cette façon, on crée index_topic=$(z_1,...,z_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "      <th>index_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "      <td>[Joseph, Harold, Greenberg, (, May, 28, ,, 191...</td>\n",
       "      <td>[68836, 61034, 58682, 1182, 80975, 13183, 1242...</td>\n",
       "      <td>[21, 21, 24, 21, 18, 18, 7, 18, 6, 18, 21, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "      <td>[Lester, Hudson, III, (, born, August, 7, ,, 1...</td>\n",
       "      <td>[75482, 63781, 64659, 1182, 134097, 26662, 181...</td>\n",
       "      <td>[21, 16, 18, 21, 11, 39, 21, 7, 16, 21, 10, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "      <td>[Monique, Ganderton, (, born, August, 6, ,, 19...</td>\n",
       "      <td>[84017, 56054, 1182, 134097, 26662, 17386, 124...</td>\n",
       "      <td>[30, 30, 21, 11, 39, 27, 7, 16, 21, 10, 7, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The white bikini of Ursula Andress (also known...</td>\n",
       "      <td>White bikini of Ursula Andress</td>\n",
       "      <td>[The, white, bikini, of, Ursula, Andress, (, a...</td>\n",
       "      <td>[115808, 188693, 133263, 165140, 120335, 24145...</td>\n",
       "      <td>[16, 7, 11, 34, 13, 13, 21, 19, 3, 15, 1, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Breakout\" is a single from British act Swing ...</td>\n",
       "      <td>Breakout (Swing Out Sister song)</td>\n",
       "      <td>[``, Breakout, '', is, a, single, from, Britis...</td>\n",
       "      <td>[127812, 32984, 6, 156142, 127813, 178055, 149...</td>\n",
       "      <td>[2, 2, 27, 10, 7, 2, 32, 41, 39, 2, 2, 6, 1, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "3   Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4   Monique Ganderton (born August 6, 1980) is a C...   \n",
       "6   The white bikini of Ursula Andress (also known...   \n",
       "15  \"Breakout\" is a single from British act Swing ...   \n",
       "\n",
       "                               Title  \\\n",
       "0                   Joseph Greenberg   \n",
       "3                      Lester Hudson   \n",
       "4                  Monique Ganderton   \n",
       "6     White bikini of Ursula Andress   \n",
       "15  Breakout (Swing Out Sister song)   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "0   [Joseph, Harold, Greenberg, (, May, 28, ,, 191...   \n",
       "3   [Lester, Hudson, III, (, born, August, 7, ,, 1...   \n",
       "4   [Monique, Ganderton, (, born, August, 6, ,, 19...   \n",
       "6   [The, white, bikini, of, Ursula, Andress, (, a...   \n",
       "15  [``, Breakout, '', is, a, single, from, Britis...   \n",
       "\n",
       "                                      vectorized_text  \\\n",
       "0   [68836, 61034, 58682, 1182, 80975, 13183, 1242...   \n",
       "3   [75482, 63781, 64659, 1182, 134097, 26662, 181...   \n",
       "4   [84017, 56054, 1182, 134097, 26662, 17386, 124...   \n",
       "6   [115808, 188693, 133263, 165140, 120335, 24145...   \n",
       "15  [127812, 32984, 6, 156142, 127813, 178055, 149...   \n",
       "\n",
       "                                          index_topic  \n",
       "0   [21, 21, 24, 21, 18, 18, 7, 18, 6, 18, 21, 7, ...  \n",
       "3   [21, 16, 18, 21, 11, 39, 21, 7, 16, 21, 10, 15...  \n",
       "4   [30, 30, 21, 11, 39, 27, 7, 16, 21, 10, 7, 1, ...  \n",
       "6   [16, 7, 11, 34, 13, 13, 21, 19, 3, 15, 1, 34, ...  \n",
       "15  [2, 2, 27, 10, 7, 2, 32, 41, 39, 2, 2, 6, 1, 4...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_TOPICS=50\n",
    "lda=LDA(num_topics=NUM_TOPICS, random_state=123)\n",
    "vectorized_data_lda=lda.run(vectorized_data)\n",
    "display(vectorized_data_lda.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique un padding, i.e une longueur maximale sur les articles ($x$) et les indices de topics ($z$). De façon arbitraire, on fixe le padding à 1000.\n",
    "* Si l'article/la liste de topics est composé de plus de 1000 caractères alors, on supprime les derniers\n",
    "* Si l'article/la liste de topics est composé de moins de 1000 caractères alors, on ajoute des 0 à la fin\n",
    "Cette opération est faite car la taille d'inpput de LSTM est unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=1000\n",
    "\n",
    "def apply_padding(sequence, max_length=SEQUENCE_LENGTH):\n",
    "    padded_sequence = sequence[:max_length] + [0] * max(0, max_length - len(sequence))\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_lda_padding=vectorized_data_lda.copy()\n",
    "vectorized_data_lda_padding['tokenized_text'] = vectorized_data_lda_padding['tokenized_text'].apply(apply_padding)\n",
    "vectorized_data_lda_padding['vectorized_text'] = vectorized_data_lda_padding['vectorized_text'].apply(apply_padding)\n",
    "vectorized_data_lda_padding['index_topic'] = vectorized_data_lda_padding['index_topic'].apply(apply_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un one hot encoding des listes d'indices de topics. Chaque liste $z$, de longueur SEQUENCE_LENGTH=200, est convertie en une matrice de dimensions SEQUENCE_LENGTH*NUM_TOPICS = $1000 \\times 50$. L'élement $(i,j)$ de cette matrice vaut 1 si le i-ème topic de la séquence correspond au j-ème topic dans l'ensemble des topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(topic_list, vocab_size):\n",
    "    return to_categorical(topic_list, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_z=vectorized_data_lda_padding['index_topic'].values\n",
    "array_z_one_hot_encoded = np.array([one_hot_encode_list(lst, NUM_TOPICS) for lst in array_z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array_z_one_hot_encoded correspond à l'échantillon de topics $\\mathcal{D}_z$ one-hot encodés. Chaque liste de topics $z=(z_1,...,z_T)$ de l'échantillon est une matrice de dimensions $1000 \\times 50$. Lorsque l'on aura implémenté le gibbs sampler, cet échantillon n'existera plus: en réalité, les $z$ ne sont pas observés. On remplacera l'échantillon par $z^*_{1:T}$.\n",
    "\n",
    "On prend un $z$ de l'échantillon en faisant comme si c'était un $z^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "z_one_hot=array_z_one_hot_encoded[0]\n",
    "print(z_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, model_length):\n",
    "        self.topics=topics\n",
    "        self.model_length=model_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.topics)-self.model_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_sequence=torch.tensor(self.topics[index:index+self.model_length, :])\n",
    "        target_sequence=torch.tensor(self.topics[index+1:index+self.model_length+1, :])\n",
    "\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size #dimension d'entrée (NUM_TOPICS)\n",
    "        self.hidden_size = hidden_size #nombre de neurones de la couche cachée\n",
    "        self.output_size = output_size #dimension d'outputs (NUM_TOPICS)\n",
    "        self.model_length=model_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        # self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        output=self.fc(output)\n",
    "        probabilities = F.softmax(output[:, -1, :], dim=1)\n",
    "        return probabilities, state\n",
    "    \n",
    "    def init_state(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size), #(NUM_LAYERS, BATCH SIZE, NUM_NEURONES)\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n",
    "    def train_model(self, dataset, optimizer, criterion):\n",
    "        state_h, state_c = self.init_state()\n",
    "        self.train()\n",
    "        for t, (x, y) in enumerate(dataset):\n",
    "            optimizer.zero_grad()\n",
    "            softmax , (state_h, state_c) = self(x, (state_h, state_c)) #softmax= p(z_{t+1}|z_1:t)\n",
    "            loss = criterion(softmax, y[:, -1, :])\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print({'loss': loss.item() })\n",
    "    \n",
    "    def predict_next_probability(self, input_sequence):\n",
    "        state_h, state_c = self.init_state()\n",
    "\n",
    "        # Forward pass jusqu'à t-1\n",
    "        for t in range(len(input_sequence)):\n",
    "            input_t = input_sequence[t].unsqueeze(0).unsqueeze(0)\n",
    "            _, (state_h, state_c) = self(input_t, (state_h, state_c))\n",
    "\n",
    "        # Obtenez les probabilités pour x_t\n",
    "        input_t = input_sequence[-1].unsqueeze(0).unsqueeze(0)\n",
    "        probabilities, _ = self(input_t, (state_h, state_c))\n",
    "\n",
    "        return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 50])\n",
      "torch.Size([200, 50])\n"
     ]
    }
   ],
   "source": [
    "MODEL_LENGTH=200\n",
    "dataset=Dataset(topics=z_one_hot, model_length=MODEL_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "print(dataset.__getitem__(0)[0].shape)\n",
    "print(dataset.__getitem__(0)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=64\n",
    "model=LSTM(input_size=NUM_TOPICS, hidden_size=HIDDEN_SIZE, output_size=NUM_TOPICS, model_length=MODEL_LENGTH)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.913480281829834}\n",
      "{'loss': 3.913555145263672}\n",
      "{'loss': 3.9127607345581055}\n",
      "{'loss': 3.9104163646698}\n",
      "{'loss': 3.9094271659851074}\n",
      "{'loss': 3.9140820503234863}\n",
      "{'loss': 3.9120399951934814}\n",
      "{'loss': 3.9104113578796387}\n",
      "{'loss': 3.9092724323272705}\n",
      "{'loss': 3.9130101203918457}\n",
      "{'loss': 3.909694194793701}\n",
      "{'loss': 3.9124979972839355}\n",
      "{'loss': 3.909458875656128}\n",
      "{'loss': 3.911994695663452}\n",
      "{'loss': 3.9093897342681885}\n",
      "{'loss': 3.910706043243408}\n",
      "{'loss': 3.913931369781494}\n",
      "{'loss': 3.9100348949432373}\n",
      "{'loss': 3.910282850265503}\n",
      "{'loss': 3.909198760986328}\n",
      "{'loss': 3.9102394580841064}\n",
      "{'loss': 3.9092965126037598}\n",
      "{'loss': 3.9106764793395996}\n",
      "{'loss': 3.9104933738708496}\n",
      "{'loss': 3.9138283729553223}\n",
      "{'loss': 3.913520097732544}\n",
      "{'loss': 3.912780284881592}\n",
      "{'loss': 3.9138669967651367}\n",
      "{'loss': 3.913709878921509}\n",
      "{'loss': 3.9098753929138184}\n",
      "{'loss': 3.91274356842041}\n",
      "{'loss': 3.9122300148010254}\n",
      "{'loss': 3.9126901626586914}\n",
      "{'loss': 3.9134511947631836}\n",
      "{'loss': 3.9100000858306885}\n",
      "{'loss': 3.9132823944091797}\n",
      "{'loss': 3.909576892852783}\n",
      "{'loss': 3.9122979640960693}\n",
      "{'loss': 3.910553455352783}\n",
      "{'loss': 3.912668228149414}\n",
      "{'loss': 3.912769317626953}\n",
      "{'loss': 3.910494804382324}\n",
      "{'loss': 3.9127089977264404}\n",
      "{'loss': 3.910581588745117}\n",
      "{'loss': 3.912799596786499}\n",
      "{'loss': 3.9098780155181885}\n",
      "{'loss': 3.912916660308838}\n",
      "{'loss': 3.912625312805176}\n",
      "{'loss': 3.9106857776641846}\n",
      "{'loss': 3.9135916233062744}\n",
      "{'loss': 3.909761667251587}\n",
      "{'loss': 3.909132242202759}\n",
      "{'loss': 3.9141087532043457}\n",
      "{'loss': 3.913390636444092}\n",
      "{'loss': 3.913093090057373}\n",
      "{'loss': 3.9137609004974365}\n",
      "{'loss': 3.909924268722534}\n",
      "{'loss': 3.9125802516937256}\n",
      "{'loss': 3.9126107692718506}\n",
      "{'loss': 3.913083791732788}\n",
      "{'loss': 3.9096312522888184}\n",
      "{'loss': 3.9096522331237793}\n",
      "{'loss': 3.909229278564453}\n",
      "{'loss': 3.913257360458374}\n",
      "{'loss': 3.913182258605957}\n",
      "{'loss': 3.9100160598754883}\n",
      "{'loss': 3.91202974319458}\n",
      "{'loss': 3.9096052646636963}\n",
      "{'loss': 3.913792610168457}\n",
      "{'loss': 3.9130215644836426}\n",
      "{'loss': 3.9106242656707764}\n",
      "{'loss': 3.9135897159576416}\n",
      "{'loss': 3.9098827838897705}\n",
      "{'loss': 3.9094834327697754}\n",
      "{'loss': 3.91044545173645}\n",
      "{'loss': 3.912008047103882}\n",
      "{'loss': 3.9100961685180664}\n",
      "{'loss': 3.9102210998535156}\n",
      "{'loss': 3.910738706588745}\n",
      "{'loss': 3.912963628768921}\n",
      "{'loss': 3.9137425422668457}\n",
      "{'loss': 3.912708282470703}\n",
      "{'loss': 3.9139788150787354}\n",
      "{'loss': 3.913757085800171}\n",
      "{'loss': 3.909862518310547}\n",
      "{'loss': 3.9133496284484863}\n",
      "{'loss': 3.9093592166900635}\n",
      "{'loss': 3.909467935562134}\n",
      "{'loss': 3.910052537918091}\n",
      "{'loss': 3.909200429916382}\n",
      "{'loss': 3.913834810256958}\n",
      "{'loss': 3.9131524562835693}\n",
      "{'loss': 3.9128947257995605}\n",
      "{'loss': 3.9096519947052}\n",
      "{'loss': 3.913628339767456}\n",
      "{'loss': 3.9135684967041016}\n",
      "{'loss': 3.9135642051696777}\n",
      "{'loss': 3.9095826148986816}\n",
      "{'loss': 3.909562110900879}\n",
      "{'loss': 3.9141979217529297}\n",
      "{'loss': 3.9132771492004395}\n",
      "{'loss': 3.9133851528167725}\n",
      "{'loss': 3.9097185134887695}\n",
      "{'loss': 3.912900924682617}\n",
      "{'loss': 3.9098403453826904}\n",
      "{'loss': 3.9092319011688232}\n",
      "{'loss': 3.9115593433380127}\n",
      "{'loss': 3.913283348083496}\n",
      "{'loss': 3.914097309112549}\n",
      "{'loss': 3.9097368717193604}\n",
      "{'loss': 3.9126734733581543}\n",
      "{'loss': 3.913520336151123}\n",
      "{'loss': 3.9134175777435303}\n",
      "{'loss': 3.9096248149871826}\n",
      "{'loss': 3.909682035446167}\n",
      "{'loss': 3.9103074073791504}\n",
      "{'loss': 3.9102623462677}\n",
      "{'loss': 3.9106690883636475}\n",
      "{'loss': 3.9138808250427246}\n",
      "{'loss': 3.913381576538086}\n",
      "{'loss': 3.914085626602173}\n",
      "{'loss': 3.9135172367095947}\n",
      "{'loss': 3.914466142654419}\n",
      "{'loss': 3.9136526584625244}\n",
      "{'loss': 3.910029172897339}\n",
      "{'loss': 3.9105260372161865}\n",
      "{'loss': 3.910381317138672}\n",
      "{'loss': 3.910250663757324}\n",
      "{'loss': 3.914146900177002}\n",
      "{'loss': 3.9101104736328125}\n",
      "{'loss': 3.911665678024292}\n",
      "{'loss': 3.9115865230560303}\n",
      "{'loss': 3.913405179977417}\n",
      "{'loss': 3.9139647483825684}\n",
      "{'loss': 3.9095890522003174}\n",
      "{'loss': 3.912707567214966}\n",
      "{'loss': 3.9121618270874023}\n",
      "{'loss': 3.9105544090270996}\n",
      "{'loss': 3.909470558166504}\n",
      "{'loss': 3.9100239276885986}\n",
      "{'loss': 3.909029960632324}\n",
      "{'loss': 3.9134480953216553}\n",
      "{'loss': 3.909472942352295}\n",
      "{'loss': 3.9138312339782715}\n",
      "{'loss': 3.910632848739624}\n",
      "{'loss': 3.914017915725708}\n",
      "{'loss': 3.910285472869873}\n",
      "{'loss': 3.9144246578216553}\n",
      "{'loss': 3.913872241973877}\n",
      "{'loss': 3.913144111633301}\n",
      "{'loss': 3.9133074283599854}\n",
      "{'loss': 3.9132113456726074}\n",
      "{'loss': 3.9132378101348877}\n",
      "{'loss': 3.9097695350646973}\n",
      "{'loss': 3.9096622467041016}\n",
      "{'loss': 3.9098079204559326}\n",
      "{'loss': 3.9107422828674316}\n",
      "{'loss': 3.9135890007019043}\n",
      "{'loss': 3.9140710830688477}\n",
      "{'loss': 3.9091174602508545}\n",
      "{'loss': 3.9128847122192383}\n",
      "{'loss': 3.909954309463501}\n",
      "{'loss': 3.9149861335754395}\n",
      "{'loss': 3.91355299949646}\n",
      "{'loss': 3.9104297161102295}\n",
      "{'loss': 3.9097297191619873}\n",
      "{'loss': 3.9100282192230225}\n",
      "{'loss': 3.909090518951416}\n",
      "{'loss': 3.9133999347686768}\n",
      "{'loss': 3.9135794639587402}\n",
      "{'loss': 3.913226366043091}\n",
      "{'loss': 3.9098026752471924}\n",
      "{'loss': 3.9135024547576904}\n",
      "{'loss': 3.9135453701019287}\n",
      "{'loss': 3.9097015857696533}\n",
      "{'loss': 3.9122207164764404}\n",
      "{'loss': 3.9133105278015137}\n",
      "{'loss': 3.9134345054626465}\n",
      "{'loss': 3.909928798675537}\n",
      "{'loss': 3.9132449626922607}\n",
      "{'loss': 3.9134435653686523}\n",
      "{'loss': 3.909738779067993}\n",
      "{'loss': 3.912731647491455}\n",
      "{'loss': 3.9097983837127686}\n",
      "{'loss': 3.9118032455444336}\n",
      "{'loss': 3.912522792816162}\n",
      "{'loss': 3.9133386611938477}\n",
      "{'loss': 3.909621477127075}\n",
      "{'loss': 3.912830114364624}\n",
      "{'loss': 3.913142442703247}\n",
      "{'loss': 3.914121389389038}\n",
      "{'loss': 3.909301519393921}\n",
      "{'loss': 3.9097158908843994}\n",
      "{'loss': 3.913018226623535}\n",
      "{'loss': 3.913343906402588}\n",
      "{'loss': 3.9130184650421143}\n",
      "{'loss': 3.9132752418518066}\n",
      "{'loss': 3.909801959991455}\n",
      "{'loss': 3.913205146789551}\n",
      "{'loss': 3.9124608039855957}\n",
      "{'loss': 3.9138741493225098}\n",
      "{'loss': 3.9118831157684326}\n",
      "{'loss': 3.913917303085327}\n",
      "{'loss': 3.9098904132843018}\n",
      "{'loss': 3.9096689224243164}\n",
      "{'loss': 3.909616708755493}\n",
      "{'loss': 3.914658784866333}\n",
      "{'loss': 3.914566993713379}\n",
      "{'loss': 3.914390802383423}\n",
      "{'loss': 3.9138996601104736}\n",
      "{'loss': 3.914069652557373}\n",
      "{'loss': 3.9098682403564453}\n",
      "{'loss': 3.914207696914673}\n",
      "{'loss': 3.9107141494750977}\n",
      "{'loss': 3.913034200668335}\n",
      "{'loss': 3.9101004600524902}\n",
      "{'loss': 3.9101524353027344}\n",
      "{'loss': 3.913464307785034}\n",
      "{'loss': 3.9142587184906006}\n",
      "{'loss': 3.9137191772460938}\n",
      "{'loss': 3.9142534732818604}\n",
      "{'loss': 3.913121461868286}\n",
      "{'loss': 3.9096298217773438}\n",
      "{'loss': 3.9105916023254395}\n",
      "{'loss': 3.9139654636383057}\n",
      "{'loss': 3.9128172397613525}\n",
      "{'loss': 3.9133737087249756}\n",
      "{'loss': 3.9139363765716553}\n",
      "{'loss': 3.9099669456481934}\n",
      "{'loss': 3.909214496612549}\n",
      "{'loss': 3.9094154834747314}\n",
      "{'loss': 3.9137167930603027}\n",
      "{'loss': 3.9135005474090576}\n",
      "{'loss': 3.908573627471924}\n",
      "{'loss': 3.913936138153076}\n",
      "{'loss': 3.9102938175201416}\n",
      "{'loss': 3.914098024368286}\n",
      "{'loss': 3.913404941558838}\n",
      "{'loss': 3.9139978885650635}\n",
      "{'loss': 3.9128499031066895}\n",
      "{'loss': 3.9090750217437744}\n",
      "{'loss': 3.9104065895080566}\n",
      "{'loss': 3.912721872329712}\n",
      "{'loss': 3.9136345386505127}\n",
      "{'loss': 3.90936541557312}\n",
      "{'loss': 3.910545825958252}\n",
      "{'loss': 3.91011118888855}\n",
      "{'loss': 3.9129977226257324}\n",
      "{'loss': 3.9122157096862793}\n",
      "{'loss': 3.909329652786255}\n",
      "{'loss': 3.9101903438568115}\n",
      "{'loss': 3.9129691123962402}\n",
      "{'loss': 3.9098258018493652}\n",
      "{'loss': 3.9127612113952637}\n",
      "{'loss': 3.9138052463531494}\n",
      "{'loss': 3.909876823425293}\n",
      "{'loss': 3.909298896789551}\n",
      "{'loss': 3.909935712814331}\n",
      "{'loss': 3.9131035804748535}\n",
      "{'loss': 3.909726619720459}\n",
      "{'loss': 3.9140658378601074}\n",
      "{'loss': 3.909895181655884}\n",
      "{'loss': 3.910198450088501}\n",
      "{'loss': 3.9105756282806396}\n",
      "{'loss': 3.910108804702759}\n",
      "{'loss': 3.9096152782440186}\n",
      "{'loss': 3.913799285888672}\n",
      "{'loss': 3.9139859676361084}\n",
      "{'loss': 3.9101710319519043}\n",
      "{'loss': 3.909425735473633}\n",
      "{'loss': 3.9130537509918213}\n",
      "{'loss': 3.912106513977051}\n",
      "{'loss': 3.9141111373901367}\n",
      "{'loss': 3.9129087924957275}\n",
      "{'loss': 3.9132869243621826}\n",
      "{'loss': 3.9099466800689697}\n",
      "{'loss': 3.912600517272949}\n",
      "{'loss': 3.9126052856445312}\n",
      "{'loss': 3.909705400466919}\n",
      "{'loss': 3.909959316253662}\n",
      "{'loss': 3.9093191623687744}\n",
      "{'loss': 3.912076473236084}\n",
      "{'loss': 3.9110918045043945}\n",
      "{'loss': 3.9094066619873047}\n",
      "{'loss': 3.913987159729004}\n",
      "{'loss': 3.9128170013427734}\n",
      "{'loss': 3.912731647491455}\n",
      "{'loss': 3.909743309020996}\n",
      "{'loss': 3.9094161987304688}\n",
      "{'loss': 3.9105513095855713}\n",
      "{'loss': 3.9097020626068115}\n",
      "{'loss': 3.9113588333129883}\n",
      "{'loss': 3.9126524925231934}\n",
      "{'loss': 3.9125826358795166}\n",
      "{'loss': 3.912550926208496}\n",
      "{'loss': 3.909545421600342}\n",
      "{'loss': 3.90944504737854}\n",
      "{'loss': 3.913052558898926}\n",
      "{'loss': 3.912527322769165}\n",
      "{'loss': 3.9141600131988525}\n",
      "{'loss': 3.9097278118133545}\n",
      "{'loss': 3.9093754291534424}\n",
      "{'loss': 3.912604331970215}\n",
      "{'loss': 3.9112164974212646}\n",
      "{'loss': 3.912177324295044}\n",
      "{'loss': 3.913461923599243}\n",
      "{'loss': 3.9103329181671143}\n",
      "{'loss': 3.910921096801758}\n",
      "{'loss': 3.9131290912628174}\n",
      "{'loss': 3.9134113788604736}\n",
      "{'loss': 3.9135332107543945}\n",
      "{'loss': 3.9099607467651367}\n",
      "{'loss': 3.9117608070373535}\n",
      "{'loss': 3.9118523597717285}\n",
      "{'loss': 3.910844564437866}\n",
      "{'loss': 3.911118984222412}\n",
      "{'loss': 3.9128851890563965}\n",
      "{'loss': 3.9133312702178955}\n",
      "{'loss': 3.9099373817443848}\n",
      "{'loss': 3.913402795791626}\n",
      "{'loss': 3.9108095169067383}\n",
      "{'loss': 3.9097249507904053}\n",
      "{'loss': 3.913675308227539}\n",
      "{'loss': 3.9099252223968506}\n",
      "{'loss': 3.909606456756592}\n",
      "{'loss': 3.9126975536346436}\n",
      "{'loss': 3.914308786392212}\n",
      "{'loss': 3.9103100299835205}\n",
      "{'loss': 3.913156509399414}\n",
      "{'loss': 3.9090638160705566}\n",
      "{'loss': 3.9129152297973633}\n",
      "{'loss': 3.9132795333862305}\n",
      "{'loss': 3.9097626209259033}\n",
      "{'loss': 3.9140243530273438}\n",
      "{'loss': 3.909860134124756}\n",
      "{'loss': 3.914364814758301}\n",
      "{'loss': 3.913299083709717}\n",
      "{'loss': 3.909992218017578}\n",
      "{'loss': 3.9109456539154053}\n",
      "{'loss': 3.9126651287078857}\n",
      "{'loss': 3.912830114364624}\n",
      "{'loss': 3.9134912490844727}\n",
      "{'loss': 3.914179563522339}\n",
      "{'loss': 3.9095518589019775}\n",
      "{'loss': 3.9118480682373047}\n",
      "{'loss': 3.9139461517333984}\n",
      "{'loss': 3.9142093658447266}\n",
      "{'loss': 3.9094793796539307}\n",
      "{'loss': 3.913994550704956}\n",
      "{'loss': 3.9132542610168457}\n",
      "{'loss': 3.914608955383301}\n",
      "{'loss': 3.909247875213623}\n",
      "{'loss': 3.9098727703094482}\n",
      "{'loss': 3.9126334190368652}\n",
      "{'loss': 3.913966655731201}\n",
      "{'loss': 3.9106996059417725}\n",
      "{'loss': 3.912585735321045}\n",
      "{'loss': 3.912543296813965}\n",
      "{'loss': 3.9136416912078857}\n",
      "{'loss': 3.910489082336426}\n",
      "{'loss': 3.909581422805786}\n",
      "{'loss': 3.9100520610809326}\n",
      "{'loss': 3.9126639366149902}\n",
      "{'loss': 3.9136273860931396}\n",
      "{'loss': 3.9092257022857666}\n",
      "{'loss': 3.91382098197937}\n",
      "{'loss': 3.909832715988159}\n",
      "{'loss': 3.910950183868408}\n",
      "{'loss': 3.9136452674865723}\n",
      "{'loss': 3.913850784301758}\n",
      "{'loss': 3.9093880653381348}\n",
      "{'loss': 3.910186529159546}\n",
      "{'loss': 3.912523031234741}\n",
      "{'loss': 3.9138565063476562}\n",
      "{'loss': 3.909730911254883}\n",
      "{'loss': 3.9127817153930664}\n",
      "{'loss': 3.912747383117676}\n",
      "{'loss': 3.91434645652771}\n",
      "{'loss': 3.9100522994995117}\n",
      "{'loss': 3.9097607135772705}\n",
      "{'loss': 3.913872241973877}\n",
      "{'loss': 3.9134271144866943}\n",
      "{'loss': 3.9138681888580322}\n",
      "{'loss': 3.9105000495910645}\n",
      "{'loss': 3.912616491317749}\n",
      "{'loss': 3.911369800567627}\n",
      "{'loss': 3.912806987762451}\n",
      "{'loss': 3.9129607677459717}\n",
      "{'loss': 3.91409969329834}\n",
      "{'loss': 3.913646936416626}\n",
      "{'loss': 3.909599781036377}\n",
      "{'loss': 3.9117422103881836}\n",
      "{'loss': 3.9097156524658203}\n",
      "{'loss': 3.9095776081085205}\n",
      "{'loss': 3.9144227504730225}\n",
      "{'loss': 3.912964344024658}\n",
      "{'loss': 3.9104201793670654}\n",
      "{'loss': 3.9138965606689453}\n",
      "{'loss': 3.913306951522827}\n",
      "{'loss': 3.9139585494995117}\n",
      "{'loss': 3.9136481285095215}\n",
      "{'loss': 3.909618616104126}\n",
      "{'loss': 3.9125869274139404}\n",
      "{'loss': 3.910377264022827}\n",
      "{'loss': 3.912597417831421}\n",
      "{'loss': 3.912313222885132}\n",
      "{'loss': 3.913459062576294}\n",
      "{'loss': 3.91438889503479}\n",
      "{'loss': 3.914072275161743}\n",
      "{'loss': 3.9099597930908203}\n",
      "{'loss': 3.9103219509124756}\n",
      "{'loss': 3.9131438732147217}\n",
      "{'loss': 3.913785696029663}\n",
      "{'loss': 3.9138619899749756}\n",
      "{'loss': 3.909996747970581}\n",
      "{'loss': 3.9106547832489014}\n",
      "{'loss': 3.9113802909851074}\n",
      "{'loss': 3.9101014137268066}\n",
      "{'loss': 3.912667989730835}\n",
      "{'loss': 3.9094114303588867}\n",
      "{'loss': 3.912503480911255}\n",
      "{'loss': 3.913787841796875}\n",
      "{'loss': 3.9103970527648926}\n",
      "{'loss': 3.9138081073760986}\n",
      "{'loss': 3.909651756286621}\n",
      "{'loss': 3.9099466800689697}\n",
      "{'loss': 3.9090487957000732}\n",
      "{'loss': 3.9101486206054688}\n",
      "{'loss': 3.910419464111328}\n",
      "{'loss': 3.91340970993042}\n",
      "{'loss': 3.909865379333496}\n",
      "{'loss': 3.9135100841522217}\n",
      "{'loss': 3.9103267192840576}\n",
      "{'loss': 3.9140594005584717}\n",
      "{'loss': 3.9099531173706055}\n",
      "{'loss': 3.9139630794525146}\n",
      "{'loss': 3.9126038551330566}\n",
      "{'loss': 3.9123759269714355}\n",
      "{'loss': 3.9109058380126953}\n",
      "{'loss': 3.9137837886810303}\n",
      "{'loss': 3.9133033752441406}\n",
      "{'loss': 3.9138500690460205}\n",
      "{'loss': 3.9139585494995117}\n",
      "{'loss': 3.9095497131347656}\n",
      "{'loss': 3.912896156311035}\n",
      "{'loss': 3.9141173362731934}\n",
      "{'loss': 3.9101083278656006}\n",
      "{'loss': 3.9118220806121826}\n",
      "{'loss': 3.9133598804473877}\n",
      "{'loss': 3.9099197387695312}\n",
      "{'loss': 3.912781000137329}\n",
      "{'loss': 3.9131855964660645}\n",
      "{'loss': 3.909696340560913}\n",
      "{'loss': 3.9126241207122803}\n",
      "{'loss': 3.9143548011779785}\n",
      "{'loss': 3.9139623641967773}\n",
      "{'loss': 3.9128029346466064}\n",
      "{'loss': 3.9097793102264404}\n",
      "{'loss': 3.9135689735412598}\n",
      "{'loss': 3.909224271774292}\n",
      "{'loss': 3.9105803966522217}\n",
      "{'loss': 3.9110677242279053}\n",
      "{'loss': 3.913905143737793}\n",
      "{'loss': 3.9128072261810303}\n",
      "{'loss': 3.909836530685425}\n",
      "{'loss': 3.912635087966919}\n",
      "{'loss': 3.910846471786499}\n",
      "{'loss': 3.9096338748931885}\n",
      "{'loss': 3.9103784561157227}\n",
      "{'loss': 3.9140758514404297}\n",
      "{'loss': 3.9140710830688477}\n",
      "{'loss': 3.9137752056121826}\n",
      "{'loss': 3.9099087715148926}\n",
      "{'loss': 3.910451650619507}\n",
      "{'loss': 3.9111244678497314}\n",
      "{'loss': 3.9140613079071045}\n",
      "{'loss': 3.9093594551086426}\n",
      "{'loss': 3.909776210784912}\n",
      "{'loss': 3.912783145904541}\n",
      "{'loss': 3.9136390686035156}\n",
      "{'loss': 3.909977912902832}\n",
      "{'loss': 3.9132542610168457}\n",
      "{'loss': 3.9134485721588135}\n",
      "{'loss': 3.9105138778686523}\n",
      "{'loss': 3.9090542793273926}\n",
      "{'loss': 3.9136688709259033}\n",
      "{'loss': 3.910259962081909}\n",
      "{'loss': 3.9142589569091797}\n",
      "{'loss': 3.912820816040039}\n",
      "{'loss': 3.9141716957092285}\n",
      "{'loss': 3.914400815963745}\n",
      "{'loss': 3.9138548374176025}\n",
      "{'loss': 3.908910036087036}\n",
      "{'loss': 3.9120829105377197}\n",
      "{'loss': 3.9102063179016113}\n",
      "{'loss': 3.9132771492004395}\n",
      "{'loss': 3.9108099937438965}\n",
      "{'loss': 3.9134082794189453}\n",
      "{'loss': 3.909785747528076}\n",
      "{'loss': 3.913121461868286}\n",
      "{'loss': 3.9096591472625732}\n",
      "{'loss': 3.912712812423706}\n",
      "{'loss': 3.909663677215576}\n",
      "{'loss': 3.9126312732696533}\n",
      "{'loss': 3.912777900695801}\n",
      "{'loss': 3.909857988357544}\n",
      "{'loss': 3.914368152618408}\n",
      "{'loss': 3.913928985595703}\n",
      "{'loss': 3.9137966632843018}\n",
      "{'loss': 3.9138050079345703}\n",
      "{'loss': 3.9100165367126465}\n",
      "{'loss': 3.9096946716308594}\n",
      "{'loss': 3.910985231399536}\n",
      "{'loss': 3.912851333618164}\n",
      "{'loss': 3.9136407375335693}\n",
      "{'loss': 3.914423704147339}\n",
      "{'loss': 3.914449453353882}\n",
      "{'loss': 3.9098474979400635}\n",
      "{'loss': 3.913905620574951}\n",
      "{'loss': 3.909693479537964}\n",
      "{'loss': 3.9095370769500732}\n",
      "{'loss': 3.913148880004883}\n",
      "{'loss': 3.912889003753662}\n",
      "{'loss': 3.91332745552063}\n",
      "{'loss': 3.909482002258301}\n",
      "{'loss': 3.9106905460357666}\n",
      "{'loss': 3.913130283355713}\n",
      "{'loss': 3.9106805324554443}\n",
      "{'loss': 3.913060188293457}\n",
      "{'loss': 3.913952350616455}\n",
      "{'loss': 3.9130003452301025}\n",
      "{'loss': 3.911545515060425}\n",
      "{'loss': 3.913586378097534}\n",
      "{'loss': 3.912846326828003}\n",
      "{'loss': 3.909562110900879}\n",
      "{'loss': 3.9104247093200684}\n",
      "{'loss': 3.9098150730133057}\n",
      "{'loss': 3.9125635623931885}\n",
      "{'loss': 3.911496639251709}\n",
      "{'loss': 3.911526679992676}\n",
      "{'loss': 3.9120705127716064}\n",
      "{'loss': 3.9140737056732178}\n",
      "{'loss': 3.911897897720337}\n",
      "{'loss': 3.9100141525268555}\n",
      "{'loss': 3.9143900871276855}\n",
      "{'loss': 3.9131674766540527}\n",
      "{'loss': 3.9140090942382812}\n",
      "{'loss': 3.9123752117156982}\n",
      "{'loss': 3.9132654666900635}\n",
      "{'loss': 3.911989212036133}\n",
      "{'loss': 3.9117982387542725}\n",
      "{'loss': 3.909675359725952}\n",
      "{'loss': 3.9131298065185547}\n",
      "{'loss': 3.913365364074707}\n",
      "{'loss': 3.9104862213134766}\n",
      "{'loss': 3.909097194671631}\n",
      "{'loss': 3.9103500843048096}\n",
      "{'loss': 3.913494348526001}\n",
      "{'loss': 3.9135546684265137}\n",
      "{'loss': 3.9133453369140625}\n",
      "{'loss': 3.9095654487609863}\n",
      "{'loss': 3.912224054336548}\n",
      "{'loss': 3.912677764892578}\n",
      "{'loss': 3.9133269786834717}\n",
      "{'loss': 3.9138941764831543}\n",
      "{'loss': 3.909679651260376}\n",
      "{'loss': 3.914231300354004}\n",
      "{'loss': 3.91037917137146}\n",
      "{'loss': 3.9121997356414795}\n",
      "{'loss': 3.910485029220581}\n",
      "{'loss': 3.9098923206329346}\n",
      "{'loss': 3.9124667644500732}\n",
      "{'loss': 3.909428834915161}\n",
      "{'loss': 3.909294605255127}\n",
      "{'loss': 3.912893295288086}\n",
      "{'loss': 3.912144184112549}\n",
      "{'loss': 3.913863182067871}\n",
      "{'loss': 3.912327527999878}\n",
      "{'loss': 3.9127347469329834}\n",
      "{'loss': 3.9136064052581787}\n",
      "{'loss': 3.909982442855835}\n",
      "{'loss': 3.9099154472351074}\n",
      "{'loss': 3.9139955043792725}\n",
      "{'loss': 3.9097421169281006}\n",
      "{'loss': 3.909575939178467}\n",
      "{'loss': 3.912118911743164}\n",
      "{'loss': 3.914294719696045}\n",
      "{'loss': 3.909564971923828}\n",
      "{'loss': 3.9097976684570312}\n",
      "{'loss': 3.9137279987335205}\n",
      "{'loss': 3.9123027324676514}\n",
      "{'loss': 3.91042160987854}\n",
      "{'loss': 3.9105398654937744}\n",
      "{'loss': 3.9137320518493652}\n",
      "{'loss': 3.912736415863037}\n",
      "{'loss': 3.9096007347106934}\n",
      "{'loss': 3.9093017578125}\n",
      "{'loss': 3.913944959640503}\n",
      "{'loss': 3.910109519958496}\n",
      "{'loss': 3.913151741027832}\n",
      "{'loss': 3.9129812717437744}\n",
      "{'loss': 3.9138011932373047}\n",
      "{'loss': 3.912421941757202}\n",
      "{'loss': 3.9132726192474365}\n",
      "{'loss': 3.9119746685028076}\n",
      "{'loss': 3.9117836952209473}\n",
      "{'loss': 3.91420316696167}\n",
      "{'loss': 3.9140734672546387}\n",
      "{'loss': 3.9106736183166504}\n",
      "{'loss': 3.91316294670105}\n",
      "{'loss': 3.9131875038146973}\n",
      "{'loss': 3.909067392349243}\n",
      "{'loss': 3.914241075515747}\n",
      "{'loss': 3.9107978343963623}\n",
      "{'loss': 3.912487506866455}\n",
      "{'loss': 3.9127511978149414}\n",
      "{'loss': 3.9107394218444824}\n",
      "{'loss': 3.9138622283935547}\n",
      "{'loss': 3.9100379943847656}\n",
      "{'loss': 3.9121804237365723}\n",
      "{'loss': 3.912905693054199}\n",
      "{'loss': 3.9131686687469482}\n",
      "{'loss': 3.913093090057373}\n",
      "{'loss': 3.914532423019409}\n",
      "{'loss': 3.9104340076446533}\n",
      "{'loss': 3.909693717956543}\n",
      "{'loss': 3.9096853733062744}\n",
      "{'loss': 3.9139699935913086}\n",
      "{'loss': 3.9119529724121094}\n",
      "{'loss': 3.913261890411377}\n",
      "{'loss': 3.9108595848083496}\n",
      "{'loss': 3.91005277633667}\n",
      "{'loss': 3.913637638092041}\n",
      "{'loss': 3.913259506225586}\n",
      "{'loss': 3.913145065307617}\n",
      "{'loss': 3.9098076820373535}\n",
      "{'loss': 3.9103689193725586}\n",
      "{'loss': 3.913076162338257}\n",
      "{'loss': 3.9113829135894775}\n",
      "{'loss': 3.9121010303497314}\n",
      "{'loss': 3.9126694202423096}\n",
      "{'loss': 3.909609079360962}\n",
      "{'loss': 3.913299560546875}\n",
      "{'loss': 3.913130283355713}\n",
      "{'loss': 3.9142725467681885}\n",
      "{'loss': 3.9099223613739014}\n",
      "{'loss': 3.912447690963745}\n",
      "{'loss': 3.9136595726013184}\n",
      "{'loss': 3.9092812538146973}\n",
      "{'loss': 3.9143154621124268}\n",
      "{'loss': 3.9106545448303223}\n",
      "{'loss': 3.911661386489868}\n",
      "{'loss': 3.9100990295410156}\n",
      "{'loss': 3.9142942428588867}\n",
      "{'loss': 3.9120662212371826}\n",
      "{'loss': 3.910466432571411}\n",
      "{'loss': 3.909406900405884}\n",
      "{'loss': 3.912943124771118}\n",
      "{'loss': 3.9090769290924072}\n",
      "{'loss': 3.9130349159240723}\n",
      "{'loss': 3.9132111072540283}\n",
      "{'loss': 3.912818670272827}\n",
      "{'loss': 3.913736343383789}\n",
      "{'loss': 3.90920352935791}\n",
      "{'loss': 3.9118635654449463}\n",
      "{'loss': 3.9098997116088867}\n",
      "{'loss': 3.913100481033325}\n",
      "{'loss': 3.91278076171875}\n",
      "{'loss': 3.9094462394714355}\n",
      "{'loss': 3.9099254608154297}\n",
      "{'loss': 3.913168430328369}\n",
      "{'loss': 3.913787603378296}\n",
      "{'loss': 3.9143476486206055}\n",
      "{'loss': 3.9105708599090576}\n",
      "{'loss': 3.9116146564483643}\n",
      "{'loss': 3.914235830307007}\n",
      "{'loss': 3.911987543106079}\n",
      "{'loss': 3.9101078510284424}\n",
      "{'loss': 3.9134485721588135}\n",
      "{'loss': 3.913405656814575}\n",
      "{'loss': 3.9095170497894287}\n",
      "{'loss': 3.9107558727264404}\n",
      "{'loss': 3.912595272064209}\n",
      "{'loss': 3.909817695617676}\n",
      "{'loss': 3.91267728805542}\n",
      "{'loss': 3.909788131713867}\n",
      "{'loss': 3.9102933406829834}\n",
      "{'loss': 3.9102299213409424}\n",
      "{'loss': 3.910163640975952}\n",
      "{'loss': 3.913855791091919}\n",
      "{'loss': 3.9143569469451904}\n",
      "{'loss': 3.909715175628662}\n",
      "{'loss': 3.913774013519287}\n",
      "{'loss': 3.9135677814483643}\n",
      "{'loss': 3.9099106788635254}\n",
      "{'loss': 3.9104366302490234}\n",
      "{'loss': 3.9095332622528076}\n",
      "{'loss': 3.911505937576294}\n",
      "{'loss': 3.9104299545288086}\n",
      "{'loss': 3.9092941284179688}\n",
      "{'loss': 3.909717559814453}\n",
      "{'loss': 3.9124221801757812}\n",
      "{'loss': 3.9094793796539307}\n",
      "{'loss': 3.9099628925323486}\n",
      "{'loss': 3.909127712249756}\n",
      "{'loss': 3.9122214317321777}\n",
      "{'loss': 3.9110193252563477}\n",
      "{'loss': 3.910634994506836}\n",
      "{'loss': 3.9137561321258545}\n",
      "{'loss': 3.914074420928955}\n",
      "{'loss': 3.910191297531128}\n",
      "{'loss': 3.9127230644226074}\n",
      "{'loss': 3.910358428955078}\n",
      "{'loss': 3.9131689071655273}\n",
      "{'loss': 3.913900375366211}\n",
      "{'loss': 3.91230845451355}\n",
      "{'loss': 3.9129178524017334}\n",
      "{'loss': 3.9100468158721924}\n",
      "{'loss': 3.9122154712677}\n",
      "{'loss': 3.9106953144073486}\n",
      "{'loss': 3.912952184677124}\n",
      "{'loss': 3.9106976985931396}\n",
      "{'loss': 3.9136807918548584}\n",
      "{'loss': 3.912891387939453}\n",
      "{'loss': 3.9103634357452393}\n",
      "{'loss': 3.9126012325286865}\n",
      "{'loss': 3.91229510307312}\n",
      "{'loss': 3.914207935333252}\n",
      "{'loss': 3.91007137298584}\n",
      "{'loss': 3.9099860191345215}\n",
      "{'loss': 3.9132444858551025}\n",
      "{'loss': 3.913785219192505}\n",
      "{'loss': 3.9142391681671143}\n",
      "{'loss': 3.9097635746002197}\n",
      "{'loss': 3.91395902633667}\n",
      "{'loss': 3.909665584564209}\n",
      "{'loss': 3.9095237255096436}\n",
      "{'loss': 3.9139013290405273}\n",
      "{'loss': 3.912935495376587}\n",
      "{'loss': 3.9094114303588867}\n",
      "{'loss': 3.9132955074310303}\n",
      "{'loss': 3.910735845565796}\n",
      "{'loss': 3.9130537509918213}\n",
      "{'loss': 3.9123830795288086}\n",
      "{'loss': 3.910463333129883}\n",
      "{'loss': 3.9135284423828125}\n",
      "{'loss': 3.9135022163391113}\n",
      "{'loss': 3.9116768836975098}\n",
      "{'loss': 3.910574197769165}\n",
      "{'loss': 3.9109861850738525}\n",
      "{'loss': 3.912937879562378}\n",
      "{'loss': 3.9100334644317627}\n",
      "{'loss': 3.9130396842956543}\n",
      "{'loss': 3.9133071899414062}\n",
      "{'loss': 3.910369634628296}\n",
      "{'loss': 3.910769462585449}\n",
      "{'loss': 3.9093735218048096}\n",
      "{'loss': 3.9097702503204346}\n",
      "{'loss': 3.9099652767181396}\n",
      "{'loss': 3.9128005504608154}\n",
      "{'loss': 3.909478187561035}\n",
      "{'loss': 3.910193681716919}\n",
      "{'loss': 3.910339593887329}\n",
      "{'loss': 3.913029670715332}\n",
      "{'loss': 3.9129886627197266}\n",
      "{'loss': 3.910865545272827}\n",
      "{'loss': 3.9132139682769775}\n",
      "{'loss': 3.909996747970581}\n",
      "{'loss': 3.912283182144165}\n",
      "{'loss': 3.9133846759796143}\n",
      "{'loss': 3.9127633571624756}\n",
      "{'loss': 3.913931369781494}\n",
      "{'loss': 3.9128525257110596}\n",
      "{'loss': 3.913327693939209}\n",
      "{'loss': 3.909966468811035}\n",
      "{'loss': 3.9097583293914795}\n",
      "{'loss': 3.9099245071411133}\n",
      "{'loss': 3.9109814167022705}\n",
      "{'loss': 3.914025068283081}\n",
      "{'loss': 3.9129087924957275}\n",
      "{'loss': 3.909850835800171}\n",
      "{'loss': 3.9138734340667725}\n",
      "{'loss': 3.909710168838501}\n",
      "{'loss': 3.9095406532287598}\n",
      "{'loss': 3.9122118949890137}\n",
      "{'loss': 3.909233808517456}\n",
      "{'loss': 3.9093518257141113}\n",
      "{'loss': 3.9133195877075195}\n",
      "{'loss': 3.9127347469329834}\n",
      "{'loss': 3.913722515106201}\n",
      "{'loss': 3.9143881797790527}\n",
      "{'loss': 3.913364887237549}\n",
      "{'loss': 3.909989833831787}\n",
      "{'loss': 3.9089865684509277}\n",
      "{'loss': 3.909701108932495}\n",
      "{'loss': 3.909132242202759}\n",
      "{'loss': 3.9134342670440674}\n",
      "{'loss': 3.90867280960083}\n",
      "{'loss': 3.9141266345977783}\n",
      "{'loss': 3.9099085330963135}\n"
     ]
    }
   ],
   "source": [
    "model.train_model(dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_one_hot_tensor = torch.FloatTensor(z_one_hot)\n",
    "z_one_hot_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0186, 0.0222, 0.0197, 0.0190, 0.0187, 0.0223, 0.0202, 0.0232, 0.0198,\n",
       "         0.0213, 0.0214, 0.0211, 0.0176, 0.0197, 0.0220, 0.0220, 0.0188, 0.0203,\n",
       "         0.0192, 0.0192, 0.0191, 0.0216, 0.0177, 0.0175, 0.0184, 0.0178, 0.0226,\n",
       "         0.0186, 0.0180, 0.0208, 0.0196, 0.0194, 0.0219, 0.0195, 0.0186, 0.0179,\n",
       "         0.0197, 0.0215, 0.0183, 0.0222, 0.0205, 0.0193, 0.0196, 0.0206, 0.0180,\n",
       "         0.0185, 0.0217, 0.0225, 0.0212, 0.0210]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_probability(z_one_hot_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour illustrer le fonctionnement, on prend le premier article vectorizé associé à la première liste de topics vectorizée. En théorie, cette liste de topics devrait être $z^*$. Pas besoin de one-hot-encoding. Le but est de trouver le maximum de vraisemblance $(\\hat{\\phi_z})$.\n",
    "\n",
    "$\\phi_z$ est une matrice de taille NUM_WORDS*NUM_TOPICS. $\\phi_z[i,j]=$ Pr(mot i | topic j). Le MLE est donné par la contrepartie empirique de ces probabilités: $\\frac{\\text{Nombre de mots i dans l'échantillon associés au topic j}}{\\sum_{i}\\text{Nombre de mots i dans l'échantillon associés au topic j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "[21 21 24 21 18 18  7 18  6 18 21  7 16 21 28 15  7 22  7  3 19 18 29  7\n",
      " 32  3 38 34  1  7 28 34 24 21 16 34 15 34 34 21 29 10 16 29 16 21 21 24\n",
      " 28 11 29 18 18  7 18 44  5 15 18 16  7 18 21 21 32 18  3 29 28  2 21 15\n",
      "  1 20 34 27  7 29  3  7 16 33 18 38  3 21 34 28 44  1  1 16  3  3 29 15\n",
      " 21  1  1 43 43  7 29 15 44  1  7 22  1 10 19  7  2 43 21 34 43 19 34 34\n",
      " 18 18 21 21  1 29  1 28  7 29 34  7 43 34  3 18 38 32  7 34 24 21 16  2\n",
      " 32 38 34 18 41  7 29 28 32 15  7 43 43  3  6 21 38 19 43 34 18 34 21  1\n",
      "  1 43 34 29 43 34  7 24 29 38 14  1 38 20 34 41  7  1 29 32  1 38 24 21\n",
      " 16 34 34 29 34 22 28  1]\n"
     ]
    }
   ],
   "source": [
    "array_z=vectorized_data_lda_padding['index_topic'].values\n",
    "z=np.array(array_z[0])\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "[ 68836  61034  58682   1182  80975  13183   1242   8867 192633  80975\n",
      "  18147   1242  11580   1183 188139 129921  23715 158664   1242 157188\n",
      " 159730 148870 152952 189165 138610 158665 185521 130019 183121 150335\n",
      " 137193 165140 157659   1434  75829  48696 158437 130019 144754   1182\n",
      "  79074 179225  20189  42585  11647   1183  68836  58682 188139 134097\n",
      " 165380  80975  13183   1242   8867 183896  68370 166977 154473  33460\n",
      "   1242  87590 126649   1434  62734 148179 151223 155610 188139 162918\n",
      "   1434  26358 183121 128842 165140   5402   1242 152281 150196 127813\n",
      " 168216 138612 154473 111425  60448   1434  61487 139234 183896 168675\n",
      " 183121 168216 149491 183590 152952 158437   1434  22055 148116 152752\n",
      " 175946   1242 152281 141424 183896 171364 127813 175938 135670 172111\n",
      " 183103 127813 162928 165428   1434  61487 145676 131477  40660 120058\n",
      " 154473  87590 126649   1434  47977 152952 176849 189547   1242 152281\n",
      " 131571 127813 137172 182536 135001  54494  31743 138610  23715  65796\n",
      " 157659   1434 125126 173042 149567  31743 130019 102904  30023   1242\n",
      " 152281 188139 128103 131208 127813 151022 180856 135001  81913  67235\n",
      "  62198 131477  88656 120058 154473  38872   1434  47977 183121 139893\n",
      " 165140 152952 151022 180871   1242  58682 142585 147885 129853 183121\n",
      "  61314 167594 165140  87901   1242 188639 152281 158003 183121  61314\n",
      " 157657   1434 115808 181106 165140 152952 143552 143301 188139 183121]\n"
     ]
    }
   ],
   "source": [
    "array_x=vectorized_data_lda_padding['vectorized_text'].values\n",
    "x=np.array(array_x[0])\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 193476\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "\n",
    "for token in vectorized_data_lda[\"tokenized_text\"]:\n",
    "    vocab.update([word for word in token])\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "NUM_WORDS=len(vocab)\n",
    "print('Unique words: {}'.format(NUM_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "word_topic_counts = np.zeros((NUM_WORDS, NUM_TOPICS))\n",
    "\n",
    "for i in range(len(x)):\n",
    "    word_topic_counts[x[i], z[i]] += 1\n",
    "\n",
    "MLE_phi_z = word_topic_counts/(np.sum(word_topic_counts, axis=0)+1e-6)\n",
    "print(MLE_phi_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MLE_SSM(x,z, n_vocab, n_topics):\n",
    "    word_topic_counts = np.zeros((n_vocab, n_topics))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        word_topic_counts[x[i], z[i]] += 1\n",
    "\n",
    "    MLE_phi_z = word_topic_counts/(np.sum(word_topic_counts, axis=0)+1e-6)\n",
    "    return MLE_phi_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 100  # Number of particles\n",
    "T = 1000  # Number of time steps\n",
    "\n",
    "z_particles = np.zeros((P, T + 1))  # Initialize particle paths\n",
    "alpha_particles = np.ones(P) / P  # Initialize normalized weights\n",
    "\n",
    "for t in range(1, T + 1):\n",
    "    a1_t_minus_1 = 1\n",
    "    z1_t = z_particles[0, :t]\n",
    "\n",
    "    for p in range(P):\n",
    "        alpha_t_minus_1=\n",
    "        softmax=model.predict_next_probability(z_particles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
