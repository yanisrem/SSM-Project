{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanis\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.vectorize import *\n",
    "from src.lda import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données et pré-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe 10 000 articles Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pauline Donalda,  (March 5, 1882 – October 22,...</td>\n",
       "      <td>Pauline Donalda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a list of German football transfers in...</td>\n",
       "      <td>List of German football transfers summer 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "1  Pauline Donalda,  (March 5, 1882 – October 22,...   \n",
       "2  This is a list of German football transfers in...   \n",
       "3  Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4  Monique Ganderton (born August 6, 1980) is a C...   \n",
       "\n",
       "                                           Title  \n",
       "0                               Joseph Greenberg  \n",
       "1                                Pauline Donalda  \n",
       "2  List of German football transfers summer 2017  \n",
       "3                                  Lester Hudson  \n",
       "4                              Monique Ganderton  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 10000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\yanis\\\\OneDrive\\\\Documents\\\\ENSAE 3A\\\\Sequential MC\\\\SSM-PROJECT\\\\wiki_data.csv\",\n",
    "                    encoding='utf-8',\n",
    "                    delimiter=\";\")\n",
    "del data[\"Unnamed: 0\"]\n",
    "display(data.head())\n",
    "print(\"Number of articles: {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On tokenize chaque article: \"Joseph Harold Greenberg\" devient [\"Jospeh\", \"Harold\", \"Greenberg\"]\n",
    "2. On filtre les articles: on retire les articles composés de moins de 500 mots\n",
    "3. On vectorise le texte: on associe à chaque mot son indice dans le vocabulaire général.\n",
    "\n",
    "Exemple: si tous les articles peuvent contenir comme mots: [\"pomme\", \"poire\", \"chocolat\", \"eau\", \"banane\"], l'article [\"chocolat\", \"banane\", \"pomme\"] devient [2, 4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "      <td>[Joseph, Harold, Greenberg, (, May, 28, ,, 191...</td>\n",
       "      <td>[68836, 61034, 58682, 1182, 80975, 13183, 1242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "      <td>[Lester, Hudson, III, (, born, August, 7, ,, 1...</td>\n",
       "      <td>[75482, 63781, 64659, 1182, 134097, 26662, 181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "      <td>[Monique, Ganderton, (, born, August, 6, ,, 19...</td>\n",
       "      <td>[84017, 56054, 1182, 134097, 26662, 17386, 124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The white bikini of Ursula Andress (also known...</td>\n",
       "      <td>White bikini of Ursula Andress</td>\n",
       "      <td>[The, white, bikini, of, Ursula, Andress, (, a...</td>\n",
       "      <td>[115808, 188693, 133263, 165140, 120335, 24145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Breakout\" is a single from British act Swing ...</td>\n",
       "      <td>Breakout (Swing Out Sister song)</td>\n",
       "      <td>[``, Breakout, '', is, a, single, from, Britis...</td>\n",
       "      <td>[127812, 32984, 6, 156142, 127813, 178055, 149...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "3   Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4   Monique Ganderton (born August 6, 1980) is a C...   \n",
       "6   The white bikini of Ursula Andress (also known...   \n",
       "15  \"Breakout\" is a single from British act Swing ...   \n",
       "\n",
       "                               Title  \\\n",
       "0                   Joseph Greenberg   \n",
       "3                      Lester Hudson   \n",
       "4                  Monique Ganderton   \n",
       "6     White bikini of Ursula Andress   \n",
       "15  Breakout (Swing Out Sister song)   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "0   [Joseph, Harold, Greenberg, (, May, 28, ,, 191...   \n",
       "3   [Lester, Hudson, III, (, born, August, 7, ,, 1...   \n",
       "4   [Monique, Ganderton, (, born, August, 6, ,, 19...   \n",
       "6   [The, white, bikini, of, Ursula, Andress, (, a...   \n",
       "15  [``, Breakout, '', is, a, single, from, Britis...   \n",
       "\n",
       "                                      vectorized_text  \n",
       "0   [68836, 61034, 58682, 1182, 80975, 13183, 1242...  \n",
       "3   [75482, 63781, 64659, 1182, 134097, 26662, 181...  \n",
       "4   [84017, 56054, 1182, 134097, 26662, 17386, 124...  \n",
       "6   [115808, 188693, 133263, 165140, 120335, 24145...  \n",
       "15  [127812, 32984, 6, 156142, 127813, 178055, 149...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_data=vectorize_data(data=data, min_number_words=500)\n",
    "display(vectorized_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En attendant d'avoir $z^*_{1:T}$, on applique le LDA sur le corpus avec K=50 topics. Chaque mot a alors un poids relativement à chaque topic.\n",
    "\n",
    "Exemple: le mot banane a les poids suivant: {\"topic 1\": 0.3, \"topic 2\": 0.1, \"topic 3\": 0.05,...}. \n",
    "\n",
    "On associe au mot le topic avec le poids le plus élevé. En l'occurence 1. De cette façon, on crée index_topic=$(z_1,...,z_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "      <th>index_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "      <td>[Joseph, Harold, Greenberg, (, May, 28, ,, 191...</td>\n",
       "      <td>[68836, 61034, 58682, 1182, 80975, 13183, 1242...</td>\n",
       "      <td>[21, 21, 24, 21, 18, 18, 7, 18, 6, 18, 21, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "      <td>[Lester, Hudson, III, (, born, August, 7, ,, 1...</td>\n",
       "      <td>[75482, 63781, 64659, 1182, 134097, 26662, 181...</td>\n",
       "      <td>[21, 16, 18, 21, 11, 39, 21, 7, 16, 21, 10, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "      <td>[Monique, Ganderton, (, born, August, 6, ,, 19...</td>\n",
       "      <td>[84017, 56054, 1182, 134097, 26662, 17386, 124...</td>\n",
       "      <td>[30, 30, 21, 11, 39, 27, 7, 16, 21, 10, 7, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The white bikini of Ursula Andress (also known...</td>\n",
       "      <td>White bikini of Ursula Andress</td>\n",
       "      <td>[The, white, bikini, of, Ursula, Andress, (, a...</td>\n",
       "      <td>[115808, 188693, 133263, 165140, 120335, 24145...</td>\n",
       "      <td>[16, 7, 11, 34, 13, 13, 21, 19, 3, 15, 1, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Breakout\" is a single from British act Swing ...</td>\n",
       "      <td>Breakout (Swing Out Sister song)</td>\n",
       "      <td>[``, Breakout, '', is, a, single, from, Britis...</td>\n",
       "      <td>[127812, 32984, 6, 156142, 127813, 178055, 149...</td>\n",
       "      <td>[2, 2, 27, 10, 7, 2, 32, 41, 39, 2, 2, 6, 1, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "3   Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4   Monique Ganderton (born August 6, 1980) is a C...   \n",
       "6   The white bikini of Ursula Andress (also known...   \n",
       "15  \"Breakout\" is a single from British act Swing ...   \n",
       "\n",
       "                               Title  \\\n",
       "0                   Joseph Greenberg   \n",
       "3                      Lester Hudson   \n",
       "4                  Monique Ganderton   \n",
       "6     White bikini of Ursula Andress   \n",
       "15  Breakout (Swing Out Sister song)   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "0   [Joseph, Harold, Greenberg, (, May, 28, ,, 191...   \n",
       "3   [Lester, Hudson, III, (, born, August, 7, ,, 1...   \n",
       "4   [Monique, Ganderton, (, born, August, 6, ,, 19...   \n",
       "6   [The, white, bikini, of, Ursula, Andress, (, a...   \n",
       "15  [``, Breakout, '', is, a, single, from, Britis...   \n",
       "\n",
       "                                      vectorized_text  \\\n",
       "0   [68836, 61034, 58682, 1182, 80975, 13183, 1242...   \n",
       "3   [75482, 63781, 64659, 1182, 134097, 26662, 181...   \n",
       "4   [84017, 56054, 1182, 134097, 26662, 17386, 124...   \n",
       "6   [115808, 188693, 133263, 165140, 120335, 24145...   \n",
       "15  [127812, 32984, 6, 156142, 127813, 178055, 149...   \n",
       "\n",
       "                                          index_topic  \n",
       "0   [21, 21, 24, 21, 18, 18, 7, 18, 6, 18, 21, 7, ...  \n",
       "3   [21, 16, 18, 21, 11, 39, 21, 7, 16, 21, 10, 15...  \n",
       "4   [30, 30, 21, 11, 39, 27, 7, 16, 21, 10, 7, 1, ...  \n",
       "6   [16, 7, 11, 34, 13, 13, 21, 19, 3, 15, 1, 34, ...  \n",
       "15  [2, 2, 27, 10, 7, 2, 32, 41, 39, 2, 2, 6, 1, 4...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_TOPICS=50\n",
    "lda=LDA(num_topics=NUM_TOPICS, random_state=123)\n",
    "vectorized_data_lda=lda.run(vectorized_data)\n",
    "display(vectorized_data_lda.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique un padding, i.e une longueur maximale sur les articles ($x$) et les indices de topics ($z$). De façon arbitraire, on fixe le padding à 200.\n",
    "* Si l'article/la liste de topics est composé de plus de 200 caractères alors, on supprime les derniers\n",
    "* Si l'article/la liste de topics est composé de moins de 200 caractères alors, on ajoute des 0 à la fin\n",
    "Cette opération est faite car la taille d'inpput de LSTM est unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=200\n",
    "\n",
    "def apply_padding(sequence, max_length=SEQUENCE_LENGTH):\n",
    "    padded_sequence = sequence[:max_length] + [0] * max(0, max_length - len(sequence))\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_lda_padding=vectorized_data_lda.copy()\n",
    "vectorized_data_lda_padding['tokenized_text'] = vectorized_data_lda_padding['tokenized_text'].apply(apply_padding)\n",
    "vectorized_data_lda_padding['vectorized_text'] = vectorized_data_lda_padding['vectorized_text'].apply(apply_padding)\n",
    "vectorized_data_lda_padding['index_topic'] = vectorized_data_lda_padding['index_topic'].apply(apply_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un one hot encoding des listes d'indices de topics. Chaque liste $z$, de longueur SEQUENCE_LENGTH=200, est convertie en une matrice de dimensions SEQUENCE_LENGTH*NUM_TOPICS = $200 \\times 50$. L'élement $(i,j)$ de cette matrice vaut 1 si le i-ème topic de la séquence correspond au j-ème topic dans l'ensemble des topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(topic_list, vocab_size):\n",
    "    return to_categorical(topic_list, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_z=vectorized_data_lda_padding['index_topic'].values\n",
    "array_z_one_hot_encoded = np.array([one_hot_encode_list(lst, NUM_TOPICS) for lst in array_z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array_z_one_hot_encoded correspond à l'échantillon de topics $\\mathcal{D}_z$ one-hot encodés. Chaque liste de topics $z=(z_1,...,z_T)$ de l'échantillon est une matrice de dimensions $200 \\times 50$. Lorsque l'on aura implémenté le gibbs sampler, cet échantillon n'existera plus: en réalité, les $z$ ne sont pas observés. On remplacera l'échantillon par $z^*_{1:T}$.\n",
    "\n",
    "On prend un $z$ de l'échantillon en faisant comme si c'était un $z^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 50)\n"
     ]
    }
   ],
   "source": [
    "z_one_hot=array_z_one_hot_encoded[0]\n",
    "print(z_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = z_one_hot[:-1,:]  # Toutes les lignes sauf la dernière\n",
    "y = z_one_hot[1:,:]   # Toutes les lignes sauf la première\n",
    "\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, labels):\n",
    "        self.topics=topics\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.topics[index,:], self.labels[index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size #dimension d'entrée (NUM_TOPICS)\n",
    "        self.hidden_size = hidden_size #nombre de neurones de la couche cachée\n",
    "        self.output_size = output_size #dimension d'outputs (NUM_TOPICS)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        output=self.fc(output)\n",
    "        probabilities = self.softmax(output)\n",
    "        return probabilities, state\n",
    "    \n",
    "    def init_state(self):\n",
    "        return (torch.zeros(1, self.hidden_size),\n",
    "                torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0=z_0: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "y_0=z_1: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "dataset=Dataset(topics=X_tensor, labels=y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "print(\"x_0=z_0: {}\".format(dataset.__getitem__(0)[0]))\n",
    "print(\"y_0=z_1: {}\".format(dataset.__getitem__(0)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=64\n",
    "model=LSTM(input_size=NUM_TOPICS, hidden_size=HIDDEN_SIZE, output_size=NUM_TOPICS)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 1, 'loss': 3.913787841796875}\n",
      "{'t': 2, 'loss': 3.909677028656006}\n",
      "{'t': 3, 'loss': 3.9135024547576904}\n",
      "{'t': 4, 'loss': 3.9133799076080322}\n",
      "{'t': 5, 'loss': 3.913633346557617}\n",
      "{'t': 6, 'loss': 3.9111168384552}\n",
      "{'t': 7, 'loss': 3.9134764671325684}\n",
      "{'t': 8, 'loss': 3.912024736404419}\n",
      "{'t': 9, 'loss': 3.9132909774780273}\n",
      "{'t': 10, 'loss': 3.9141995906829834}\n",
      "{'t': 11, 'loss': 3.9113969802856445}\n",
      "{'t': 12, 'loss': 3.9115915298461914}\n",
      "{'t': 13, 'loss': 3.9137651920318604}\n",
      "{'t': 14, 'loss': 3.911694288253784}\n",
      "{'t': 15, 'loss': 3.9090847969055176}\n",
      "{'t': 16, 'loss': 3.911741018295288}\n",
      "{'t': 17, 'loss': 3.910820484161377}\n",
      "{'t': 18, 'loss': 3.9118988513946533}\n",
      "{'t': 19, 'loss': 3.9136240482330322}\n",
      "{'t': 20, 'loss': 3.9113807678222656}\n",
      "{'t': 21, 'loss': 3.9130139350891113}\n",
      "{'t': 22, 'loss': 3.9100780487060547}\n",
      "{'t': 23, 'loss': 3.91141676902771}\n",
      "{'t': 24, 'loss': 3.9089269638061523}\n",
      "{'t': 25, 'loss': 3.9134366512298584}\n",
      "{'t': 26, 'loss': 3.9131181240081787}\n",
      "{'t': 27, 'loss': 3.9124667644500732}\n",
      "{'t': 28, 'loss': 3.911402940750122}\n",
      "{'t': 29, 'loss': 3.9114761352539062}\n",
      "{'t': 30, 'loss': 3.9118235111236572}\n",
      "{'t': 31, 'loss': 3.9125077724456787}\n",
      "{'t': 32, 'loss': 3.909074544906616}\n",
      "{'t': 33, 'loss': 3.9136040210723877}\n",
      "{'t': 34, 'loss': 3.911891460418701}\n",
      "{'t': 35, 'loss': 3.9121930599212646}\n",
      "{'t': 36, 'loss': 3.9088141918182373}\n",
      "{'t': 37, 'loss': 3.912355422973633}\n",
      "{'t': 38, 'loss': 3.912445068359375}\n",
      "{'t': 39, 'loss': 3.9140660762786865}\n",
      "{'t': 40, 'loss': 3.909907817840576}\n",
      "{'t': 41, 'loss': 3.9127066135406494}\n",
      "{'t': 42, 'loss': 3.9121294021606445}\n",
      "{'t': 43, 'loss': 3.9102070331573486}\n",
      "{'t': 44, 'loss': 3.9121084213256836}\n",
      "{'t': 45, 'loss': 3.9135959148406982}\n",
      "{'t': 46, 'loss': 3.9135615825653076}\n",
      "{'t': 47, 'loss': 3.9092588424682617}\n",
      "{'t': 48, 'loss': 3.9117958545684814}\n",
      "{'t': 49, 'loss': 3.9113609790802}\n",
      "{'t': 50, 'loss': 3.9097812175750732}\n",
      "{'t': 51, 'loss': 3.9131972789764404}\n",
      "{'t': 52, 'loss': 3.9134883880615234}\n",
      "{'t': 53, 'loss': 3.9111130237579346}\n",
      "{'t': 54, 'loss': 3.9134347438812256}\n",
      "{'t': 55, 'loss': 3.9139163494110107}\n",
      "{'t': 56, 'loss': 3.9134104251861572}\n",
      "{'t': 57, 'loss': 3.908653736114502}\n",
      "{'t': 58, 'loss': 3.912980794906616}\n",
      "{'t': 59, 'loss': 3.9116644859313965}\n",
      "{'t': 60, 'loss': 3.911658763885498}\n",
      "{'t': 61, 'loss': 3.9131088256835938}\n",
      "{'t': 62, 'loss': 3.9141764640808105}\n",
      "{'t': 63, 'loss': 3.9137918949127197}\n",
      "{'t': 64, 'loss': 3.9090182781219482}\n",
      "{'t': 65, 'loss': 3.913548231124878}\n",
      "{'t': 66, 'loss': 3.9133501052856445}\n",
      "{'t': 67, 'loss': 3.909613847732544}\n",
      "{'t': 68, 'loss': 3.911480188369751}\n",
      "{'t': 69, 'loss': 3.909618377685547}\n",
      "{'t': 70, 'loss': 3.913520097732544}\n",
      "{'t': 71, 'loss': 3.9089505672454834}\n",
      "{'t': 72, 'loss': 3.9117634296417236}\n",
      "{'t': 73, 'loss': 3.91099214553833}\n",
      "{'t': 74, 'loss': 3.912337303161621}\n",
      "{'t': 75, 'loss': 3.9124412536621094}\n",
      "{'t': 76, 'loss': 3.911310911178589}\n",
      "{'t': 77, 'loss': 3.910264730453491}\n",
      "{'t': 78, 'loss': 3.9136641025543213}\n",
      "{'t': 79, 'loss': 3.9116737842559814}\n",
      "{'t': 80, 'loss': 3.9118270874023438}\n",
      "{'t': 81, 'loss': 3.912571430206299}\n",
      "{'t': 82, 'loss': 3.912635326385498}\n",
      "{'t': 83, 'loss': 3.9124748706817627}\n",
      "{'t': 84, 'loss': 3.913638114929199}\n",
      "{'t': 85, 'loss': 3.913731813430786}\n",
      "{'t': 86, 'loss': 3.912053346633911}\n",
      "{'t': 87, 'loss': 3.9118688106536865}\n",
      "{'t': 88, 'loss': 3.913989305496216}\n",
      "{'t': 89, 'loss': 3.9113564491271973}\n",
      "{'t': 90, 'loss': 3.9112861156463623}\n",
      "{'t': 91, 'loss': 3.9118456840515137}\n",
      "{'t': 92, 'loss': 3.914052724838257}\n",
      "{'t': 93, 'loss': 3.9139158725738525}\n",
      "{'t': 94, 'loss': 3.90948224067688}\n",
      "{'t': 95, 'loss': 3.909226894378662}\n",
      "{'t': 96, 'loss': 3.913487434387207}\n",
      "{'t': 97, 'loss': 3.911754846572876}\n",
      "{'t': 98, 'loss': 3.911454916000366}\n",
      "{'t': 99, 'loss': 3.911005735397339}\n",
      "{'t': 100, 'loss': 3.9105842113494873}\n",
      "{'t': 101, 'loss': 3.9118082523345947}\n",
      "{'t': 102, 'loss': 3.9097490310668945}\n",
      "{'t': 103, 'loss': 3.9092695713043213}\n",
      "{'t': 104, 'loss': 3.9139583110809326}\n",
      "{'t': 105, 'loss': 3.911393165588379}\n",
      "{'t': 106, 'loss': 3.9116580486297607}\n",
      "{'t': 107, 'loss': 3.91064715385437}\n",
      "{'t': 108, 'loss': 3.9113965034484863}\n",
      "{'t': 109, 'loss': 3.912508010864258}\n",
      "{'t': 110, 'loss': 3.9115092754364014}\n",
      "{'t': 111, 'loss': 3.91167950630188}\n",
      "{'t': 112, 'loss': 3.909775733947754}\n",
      "{'t': 113, 'loss': 3.910287618637085}\n",
      "{'t': 114, 'loss': 3.914153814315796}\n",
      "{'t': 115, 'loss': 3.9121201038360596}\n",
      "{'t': 116, 'loss': 3.910339117050171}\n",
      "{'t': 117, 'loss': 3.9110324382781982}\n",
      "{'t': 118, 'loss': 3.9128949642181396}\n",
      "{'t': 119, 'loss': 3.912754535675049}\n",
      "{'t': 120, 'loss': 3.9132370948791504}\n",
      "{'t': 121, 'loss': 3.913517951965332}\n",
      "{'t': 122, 'loss': 3.9142656326293945}\n",
      "{'t': 123, 'loss': 3.9137730598449707}\n",
      "{'t': 124, 'loss': 3.9119274616241455}\n",
      "{'t': 125, 'loss': 3.9103288650512695}\n",
      "{'t': 126, 'loss': 3.9116249084472656}\n",
      "{'t': 127, 'loss': 3.9110453128814697}\n",
      "{'t': 128, 'loss': 3.91120982170105}\n",
      "{'t': 129, 'loss': 3.910207509994507}\n",
      "{'t': 130, 'loss': 3.9121620655059814}\n",
      "{'t': 131, 'loss': 3.9115564823150635}\n",
      "{'t': 132, 'loss': 3.9105401039123535}\n",
      "{'t': 133, 'loss': 3.912590742111206}\n",
      "{'t': 134, 'loss': 3.9141697883605957}\n",
      "{'t': 135, 'loss': 3.9131054878234863}\n",
      "{'t': 136, 'loss': 3.9127376079559326}\n",
      "{'t': 137, 'loss': 3.909052848815918}\n",
      "{'t': 138, 'loss': 3.911348819732666}\n",
      "{'t': 139, 'loss': 3.912456750869751}\n",
      "{'t': 140, 'loss': 3.908888816833496}\n",
      "{'t': 141, 'loss': 3.9135570526123047}\n",
      "{'t': 142, 'loss': 3.9119911193847656}\n",
      "{'t': 143, 'loss': 3.9093565940856934}\n",
      "{'t': 144, 'loss': 3.909334659576416}\n",
      "{'t': 145, 'loss': 3.912623405456543}\n",
      "{'t': 146, 'loss': 3.912430763244629}\n",
      "{'t': 147, 'loss': 3.913135528564453}\n",
      "{'t': 148, 'loss': 3.913879156112671}\n",
      "{'t': 149, 'loss': 3.9114410877227783}\n",
      "{'t': 150, 'loss': 3.910282611846924}\n",
      "{'t': 151, 'loss': 3.9112863540649414}\n",
      "{'t': 152, 'loss': 3.9088022708892822}\n",
      "{'t': 153, 'loss': 3.90922474861145}\n",
      "{'t': 154, 'loss': 3.9118340015411377}\n",
      "{'t': 155, 'loss': 3.910642147064209}\n",
      "{'t': 156, 'loss': 3.9103662967681885}\n",
      "{'t': 157, 'loss': 3.913888692855835}\n",
      "{'t': 158, 'loss': 3.9116458892822266}\n",
      "{'t': 159, 'loss': 3.913900375366211}\n",
      "{'t': 160, 'loss': 3.9129951000213623}\n",
      "{'t': 161, 'loss': 3.910980224609375}\n",
      "{'t': 162, 'loss': 3.9104018211364746}\n",
      "{'t': 163, 'loss': 3.912815809249878}\n",
      "{'t': 164, 'loss': 3.9132165908813477}\n",
      "{'t': 165, 'loss': 3.9124596118927}\n",
      "{'t': 166, 'loss': 3.9142143726348877}\n",
      "{'t': 167, 'loss': 3.9117867946624756}\n",
      "{'t': 168, 'loss': 3.911513328552246}\n",
      "{'t': 169, 'loss': 3.9107282161712646}\n",
      "{'t': 170, 'loss': 3.9125821590423584}\n",
      "{'t': 171, 'loss': 3.9100043773651123}\n",
      "{'t': 172, 'loss': 3.9108784198760986}\n",
      "{'t': 173, 'loss': 3.912546396255493}\n",
      "{'t': 174, 'loss': 3.911669969558716}\n",
      "{'t': 175, 'loss': 3.9088821411132812}\n",
      "{'t': 176, 'loss': 3.9097371101379395}\n",
      "{'t': 177, 'loss': 3.9129550457000732}\n",
      "{'t': 178, 'loss': 3.9142274856567383}\n",
      "{'t': 179, 'loss': 3.911796808242798}\n",
      "{'t': 180, 'loss': 3.912522554397583}\n",
      "{'t': 181, 'loss': 3.9114959239959717}\n",
      "{'t': 182, 'loss': 3.912476062774658}\n",
      "{'t': 183, 'loss': 3.913966178894043}\n",
      "{'t': 184, 'loss': 3.911548376083374}\n",
      "{'t': 185, 'loss': 3.9119532108306885}\n",
      "{'t': 186, 'loss': 3.910611629486084}\n",
      "{'t': 187, 'loss': 3.9088528156280518}\n",
      "{'t': 188, 'loss': 3.911463975906372}\n",
      "{'t': 189, 'loss': 3.9126241207122803}\n",
      "{'t': 190, 'loss': 3.908083200454712}\n",
      "{'t': 191, 'loss': 3.913590908050537}\n",
      "{'t': 192, 'loss': 3.912194013595581}\n",
      "{'t': 193, 'loss': 3.912179708480835}\n",
      "{'t': 194, 'loss': 3.9123785495758057}\n",
      "{'t': 195, 'loss': 3.9101784229278564}\n",
      "{'t': 196, 'loss': 3.9122474193573}\n",
      "{'t': 197, 'loss': 3.9108455181121826}\n",
      "{'t': 198, 'loss': 3.9117343425750732}\n",
      "{'t': 199, 'loss': 3.911386013031006}\n"
     ]
    }
   ],
   "source": [
    "state_h, state_c = model.init_state()\n",
    "for t, (z_t, z_t_next) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    softmax , (state_h, state_c) = model(z_t, (state_h, state_c)) #softmax= p(z_{t+1}|z_t)\n",
    "    loss = criterion(softmax, z_t_next)\n",
    "\n",
    "    index_z_t_next_pred = torch.multinomial(input=softmax[0], num_samples=1, replacement=True)\n",
    "    z_t_next_pred= torch.eye(len(softmax[0]))[index_z_t_next_pred] #one hot encoding du topic prédit\n",
    "    state_h = state_h.detach()\n",
    "    state_c = state_c.detach()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print({ 't': t+1,'loss': loss.item() })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour illustrer le fonctionnement, on prend le premier article vectorizé associé à la première liste de topics vectorizée. En théorie, cette liste de topics devrait être $z^*$. Pas besoin de one-hot-encoding. Le but est de trouver le maximum de vraisemblance $(\\hat{\\phi_z})$.\n",
    "\n",
    "$\\phi_z$ est une matrice de taille NUM_WORDS*NUM_TOPICS. $\\phi_z[i,j]=$ Pr(mot i | topic j). Le MLE est donné par la contrepartie empirique de ces probabilités: $\\frac{\\text{Nombre de mots i dans l'échantillon associés au topic j}}{\\sum_{i}\\text{Nombre de mots i dans l'échantillon associés au topic j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "[21 21 24 21 18 18  7 18  6 18 21  7 16 21 28 15  7 22  7  3 19 18 29  7\n",
      " 32  3 38 34  1  7 28 34 24 21 16 34 15 34 34 21 29 10 16 29 16 21 21 24\n",
      " 28 11 29 18 18  7 18 44  5 15 18 16  7 18 21 21 32 18  3 29 28  2 21 15\n",
      "  1 20 34 27  7 29  3  7 16 33 18 38  3 21 34 28 44  1  1 16  3  3 29 15\n",
      " 21  1  1 43 43  7 29 15 44  1  7 22  1 10 19  7  2 43 21 34 43 19 34 34\n",
      " 18 18 21 21  1 29  1 28  7 29 34  7 43 34  3 18 38 32  7 34 24 21 16  2\n",
      " 32 38 34 18 41  7 29 28 32 15  7 43 43  3  6 21 38 19 43 34 18 34 21  1\n",
      "  1 43 34 29 43 34  7 24 29 38 14  1 38 20 34 41  7  1 29 32  1 38 24 21\n",
      " 16 34 34 29 34 22 28  1]\n"
     ]
    }
   ],
   "source": [
    "array_z=vectorized_data_lda_padding['index_topic'].values\n",
    "z=np.array(array_z[0])\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "[ 68836  61034  58682   1182  80975  13183   1242   8867 192633  80975\n",
      "  18147   1242  11580   1183 188139 129921  23715 158664   1242 157188\n",
      " 159730 148870 152952 189165 138610 158665 185521 130019 183121 150335\n",
      " 137193 165140 157659   1434  75829  48696 158437 130019 144754   1182\n",
      "  79074 179225  20189  42585  11647   1183  68836  58682 188139 134097\n",
      " 165380  80975  13183   1242   8867 183896  68370 166977 154473  33460\n",
      "   1242  87590 126649   1434  62734 148179 151223 155610 188139 162918\n",
      "   1434  26358 183121 128842 165140   5402   1242 152281 150196 127813\n",
      " 168216 138612 154473 111425  60448   1434  61487 139234 183896 168675\n",
      " 183121 168216 149491 183590 152952 158437   1434  22055 148116 152752\n",
      " 175946   1242 152281 141424 183896 171364 127813 175938 135670 172111\n",
      " 183103 127813 162928 165428   1434  61487 145676 131477  40660 120058\n",
      " 154473  87590 126649   1434  47977 152952 176849 189547   1242 152281\n",
      " 131571 127813 137172 182536 135001  54494  31743 138610  23715  65796\n",
      " 157659   1434 125126 173042 149567  31743 130019 102904  30023   1242\n",
      " 152281 188139 128103 131208 127813 151022 180856 135001  81913  67235\n",
      "  62198 131477  88656 120058 154473  38872   1434  47977 183121 139893\n",
      " 165140 152952 151022 180871   1242  58682 142585 147885 129853 183121\n",
      "  61314 167594 165140  87901   1242 188639 152281 158003 183121  61314\n",
      " 157657   1434 115808 181106 165140 152952 143552 143301 188139 183121]\n"
     ]
    }
   ],
   "source": [
    "array_x=vectorized_data_lda_padding['vectorized_text'].values\n",
    "x=np.array(array_x[0])\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 193476\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "\n",
    "for token in vectorized_data_lda[\"tokenized_text\"]:\n",
    "    vocab.update([word for word in token])\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "NUM_WORDS=len(vocab)\n",
    "print('Unique words: {}'.format(NUM_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "word_topic_counts = np.zeros((NUM_WORDS, NUM_TOPICS))\n",
    "\n",
    "for i in range(len(x)):\n",
    "    word_topic_counts[x[i], z[i]] += 1\n",
    "\n",
    "MLE_phi_z = word_topic_counts/(np.sum(word_topic_counts, axis=0)+1e-6)\n",
    "print(MLE_phi_z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
