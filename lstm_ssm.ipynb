{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanis\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.vectorize import *\n",
    "from src.lda import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données et pré-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe 10 000 articles Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pauline Donalda,  (March 5, 1882 – October 22,...</td>\n",
       "      <td>Pauline Donalda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a list of German football transfers in...</td>\n",
       "      <td>List of German football transfers summer 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "1  Pauline Donalda,  (March 5, 1882 – October 22,...   \n",
       "2  This is a list of German football transfers in...   \n",
       "3  Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4  Monique Ganderton (born August 6, 1980) is a C...   \n",
       "\n",
       "                                           Title  \n",
       "0                               Joseph Greenberg  \n",
       "1                                Pauline Donalda  \n",
       "2  List of German football transfers summer 2017  \n",
       "3                                  Lester Hudson  \n",
       "4                              Monique Ganderton  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 10000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\yanis\\\\OneDrive\\\\Documents\\\\ENSAE 3A\\\\Sequential MC\\\\SSM-PROJECT\\\\wiki_data.csv\",\n",
    "                    encoding='utf-8',\n",
    "                    delimiter=\";\")\n",
    "del data[\"Unnamed: 0\"]\n",
    "display(data.head())\n",
    "print(\"Number of articles: {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On tokenize chaque article: \"Joseph Harold Greenberg\" devient [\"Jospeh\", \"Harold\", \"Greenberg\"]\n",
    "2. On filtre les articles: on retire les articles composés de moins de 500 mots\n",
    "3. On vectorise le texte: on associe à chaque mot son indice dans le vocabulaire général.\n",
    "\n",
    "Exemple: si tous les articles peuvent contenir comme mots: [\"pomme\", \"poire\", \"chocolat\", \"eau\", \"banane\"], l'article [\"chocolat\", \"banane\", \"pomme\"] devient [2, 4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "      <td>[Joseph, Harold, Greenberg, (, May, 28, ,, 191...</td>\n",
       "      <td>[68836, 61034, 58682, 1182, 80975, 13183, 1242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "      <td>[Lester, Hudson, III, (, born, August, 7, ,, 1...</td>\n",
       "      <td>[75482, 63781, 64659, 1182, 134097, 26662, 181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "      <td>[Monique, Ganderton, (, born, August, 6, ,, 19...</td>\n",
       "      <td>[84017, 56054, 1182, 134097, 26662, 17386, 124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The white bikini of Ursula Andress (also known...</td>\n",
       "      <td>White bikini of Ursula Andress</td>\n",
       "      <td>[The, white, bikini, of, Ursula, Andress, (, a...</td>\n",
       "      <td>[115808, 188693, 133263, 165140, 120335, 24145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Breakout\" is a single from British act Swing ...</td>\n",
       "      <td>Breakout (Swing Out Sister song)</td>\n",
       "      <td>[``, Breakout, '', is, a, single, from, Britis...</td>\n",
       "      <td>[127812, 32984, 6, 156142, 127813, 178055, 149...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "3   Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4   Monique Ganderton (born August 6, 1980) is a C...   \n",
       "6   The white bikini of Ursula Andress (also known...   \n",
       "15  \"Breakout\" is a single from British act Swing ...   \n",
       "\n",
       "                               Title  \\\n",
       "0                   Joseph Greenberg   \n",
       "3                      Lester Hudson   \n",
       "4                  Monique Ganderton   \n",
       "6     White bikini of Ursula Andress   \n",
       "15  Breakout (Swing Out Sister song)   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "0   [Joseph, Harold, Greenberg, (, May, 28, ,, 191...   \n",
       "3   [Lester, Hudson, III, (, born, August, 7, ,, 1...   \n",
       "4   [Monique, Ganderton, (, born, August, 6, ,, 19...   \n",
       "6   [The, white, bikini, of, Ursula, Andress, (, a...   \n",
       "15  [``, Breakout, '', is, a, single, from, Britis...   \n",
       "\n",
       "                                      vectorized_text  \n",
       "0   [68836, 61034, 58682, 1182, 80975, 13183, 1242...  \n",
       "3   [75482, 63781, 64659, 1182, 134097, 26662, 181...  \n",
       "4   [84017, 56054, 1182, 134097, 26662, 17386, 124...  \n",
       "6   [115808, 188693, 133263, 165140, 120335, 24145...  \n",
       "15  [127812, 32984, 6, 156142, 127813, 178055, 149...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_data=vectorize_data(data=data, min_number_words=500)\n",
    "display(vectorized_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En attendant d'avoir $z^*_{1:T}$, on applique le LDA sur le corpus avec K=50 topics. Chaque mot a alors un poids relativement à chaque topic.\n",
    "\n",
    "Exemple: le mot banane a les poids suivant: {\"topic 1\": 0.3, \"topic 2\": 0.1, \"topic 3\": 0.05,...}. \n",
    "\n",
    "On associe au mot le topic avec le poids le plus élevé. En l'occurence 1. De cette façon, on crée index_topic=$(z_1,...,z_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "      <th>index_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "      <td>[Joseph, Harold, Greenberg, (, May, 28, ,, 191...</td>\n",
       "      <td>[68836, 61034, 58682, 1182, 80975, 13183, 1242...</td>\n",
       "      <td>[21, 21, 24, 21, 18, 18, 7, 18, 6, 18, 21, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "      <td>[Lester, Hudson, III, (, born, August, 7, ,, 1...</td>\n",
       "      <td>[75482, 63781, 64659, 1182, 134097, 26662, 181...</td>\n",
       "      <td>[21, 16, 18, 21, 11, 39, 21, 7, 16, 21, 10, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "      <td>[Monique, Ganderton, (, born, August, 6, ,, 19...</td>\n",
       "      <td>[84017, 56054, 1182, 134097, 26662, 17386, 124...</td>\n",
       "      <td>[30, 30, 21, 11, 39, 27, 7, 16, 21, 10, 7, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The white bikini of Ursula Andress (also known...</td>\n",
       "      <td>White bikini of Ursula Andress</td>\n",
       "      <td>[The, white, bikini, of, Ursula, Andress, (, a...</td>\n",
       "      <td>[115808, 188693, 133263, 165140, 120335, 24145...</td>\n",
       "      <td>[16, 7, 11, 34, 13, 13, 21, 19, 3, 15, 1, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Breakout\" is a single from British act Swing ...</td>\n",
       "      <td>Breakout (Swing Out Sister song)</td>\n",
       "      <td>[``, Breakout, '', is, a, single, from, Britis...</td>\n",
       "      <td>[127812, 32984, 6, 156142, 127813, 178055, 149...</td>\n",
       "      <td>[2, 2, 27, 10, 7, 2, 32, 41, 39, 2, 2, 6, 1, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "3   Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4   Monique Ganderton (born August 6, 1980) is a C...   \n",
       "6   The white bikini of Ursula Andress (also known...   \n",
       "15  \"Breakout\" is a single from British act Swing ...   \n",
       "\n",
       "                               Title  \\\n",
       "0                   Joseph Greenberg   \n",
       "3                      Lester Hudson   \n",
       "4                  Monique Ganderton   \n",
       "6     White bikini of Ursula Andress   \n",
       "15  Breakout (Swing Out Sister song)   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "0   [Joseph, Harold, Greenberg, (, May, 28, ,, 191...   \n",
       "3   [Lester, Hudson, III, (, born, August, 7, ,, 1...   \n",
       "4   [Monique, Ganderton, (, born, August, 6, ,, 19...   \n",
       "6   [The, white, bikini, of, Ursula, Andress, (, a...   \n",
       "15  [``, Breakout, '', is, a, single, from, Britis...   \n",
       "\n",
       "                                      vectorized_text  \\\n",
       "0   [68836, 61034, 58682, 1182, 80975, 13183, 1242...   \n",
       "3   [75482, 63781, 64659, 1182, 134097, 26662, 181...   \n",
       "4   [84017, 56054, 1182, 134097, 26662, 17386, 124...   \n",
       "6   [115808, 188693, 133263, 165140, 120335, 24145...   \n",
       "15  [127812, 32984, 6, 156142, 127813, 178055, 149...   \n",
       "\n",
       "                                          index_topic  \n",
       "0   [21, 21, 24, 21, 18, 18, 7, 18, 6, 18, 21, 7, ...  \n",
       "3   [21, 16, 18, 21, 11, 39, 21, 7, 16, 21, 10, 15...  \n",
       "4   [30, 30, 21, 11, 39, 27, 7, 16, 21, 10, 7, 1, ...  \n",
       "6   [16, 7, 11, 34, 13, 13, 21, 19, 3, 15, 1, 34, ...  \n",
       "15  [2, 2, 27, 10, 7, 2, 32, 41, 39, 2, 2, 6, 1, 4...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_TOPICS=50\n",
    "lda=LDA(num_topics=NUM_TOPICS, random_state=123)\n",
    "vectorized_data_lda=lda.run(vectorized_data)\n",
    "display(vectorized_data_lda.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique un padding, i.e une longueur maximale sur les articles ($x$) et les indices de topics ($z$). De façon arbitraire, on fixe le padding à 1000.\n",
    "* Si l'article/la liste de topics est composé de plus de 1000 caractères alors, on supprime les derniers\n",
    "* Si l'article/la liste de topics est composé de moins de 1000 caractères alors, on ajoute des 0 à la fin\n",
    "Cette opération est faite car la taille d'inpput de LSTM est unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=1000\n",
    "\n",
    "def apply_padding(sequence, max_length=SEQUENCE_LENGTH):\n",
    "    padded_sequence = sequence[:max_length] + [0] * max(0, max_length - len(sequence))\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_lda_padding=vectorized_data_lda.copy()\n",
    "vectorized_data_lda_padding['tokenized_text'] = vectorized_data_lda_padding['tokenized_text'].apply(apply_padding)\n",
    "vectorized_data_lda_padding['vectorized_text'] = vectorized_data_lda_padding['vectorized_text'].apply(apply_padding)\n",
    "vectorized_data_lda_padding['index_topic'] = vectorized_data_lda_padding['index_topic'].apply(apply_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un one hot encoding des listes d'indices de topics. Chaque liste $z$, de longueur SEQUENCE_LENGTH=200, est convertie en une matrice de dimensions SEQUENCE_LENGTH*NUM_TOPICS = $1000 \\times 50$. L'élement $(i,j)$ de cette matrice vaut 1 si le i-ème topic de la séquence correspond au j-ème topic dans l'ensemble des topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(topic_list, vocab_size):\n",
    "    return to_categorical(topic_list, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_z=vectorized_data_lda_padding['index_topic'].values\n",
    "array_z_one_hot_encoded = np.array([one_hot_encode_list(lst, NUM_TOPICS) for lst in array_z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array_z_one_hot_encoded correspond à l'échantillon de topics $\\mathcal{D}_z$ one-hot encodés. Chaque liste de topics $z=(z_1,...,z_T)$ de l'échantillon est une matrice de dimensions $1000 \\times 50$. Lorsque l'on aura implémenté le gibbs sampler, cet échantillon n'existera plus: en réalité, les $z$ ne sont pas observés. On remplacera l'échantillon par $z^*_{1:T}$.\n",
    "\n",
    "On prend un $z$ de l'échantillon en faisant comme si c'était un $z^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "z_one_hot=array_z_one_hot_encoded[0]\n",
    "print(z_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, model_length):\n",
    "        self.topics=topics\n",
    "        self.model_length=model_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.topics)-self.model_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_sequence=torch.tensor(self.topics[index:index+self.model_length, :])\n",
    "        target_sequence=torch.tensor(self.topics[index+1:index+self.model_length+1, :])\n",
    "\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size #dimension d'entrée (NUM_TOPICS)\n",
    "        self.hidden_size = hidden_size #nombre de neurones de la couche cachée\n",
    "        self.output_size = output_size #dimension d'outputs (NUM_TOPICS)\n",
    "        self.model_length=model_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        # self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        output=self.fc(output)\n",
    "        probabilities = F.softmax(output[:, -1, :], dim=1)\n",
    "        return probabilities, state\n",
    "    \n",
    "    def init_state(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size), #(NUM_LAYERS, BATCH SIZE, NUM_NEURONES)\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n",
    "    def train_model(self, dataset, optimizer, criterion):\n",
    "        state_h, state_c = self.init_state()\n",
    "        self.train()\n",
    "        for t, (x, y) in enumerate(dataset):\n",
    "            optimizer.zero_grad()\n",
    "            softmax , (state_h, state_c) = self(x, (state_h, state_c)) #softmax= p(z_{t+1}|z_1:t)\n",
    "            loss = criterion(softmax, y[:, -1, :])\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print({'loss': loss.item() })\n",
    "    \n",
    "    def predict_next_probability(self, input_sequence):\n",
    "        state_h, state_c = self.init_state()\n",
    "\n",
    "        # Forward pass jusqu'à t-1\n",
    "        for t in range(len(input_sequence)):\n",
    "            input_t = input_sequence[t].unsqueeze(0).unsqueeze(0)\n",
    "            _, (state_h, state_c) = self(input_t, (state_h, state_c))\n",
    "\n",
    "        # Obtenez les probabilités pour x_t\n",
    "        input_t = input_sequence[-1].unsqueeze(0).unsqueeze(0)\n",
    "        probabilities, _ = self(input_t, (state_h, state_c))\n",
    "\n",
    "        return probabilities\n",
    "    \n",
    "    def sample_next_z(self, input_sequence):\n",
    "        proba = self.predict_next_probability(input_sequence)\n",
    "        return torch.multinomial(proba, 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 50])\n",
      "torch.Size([200, 50])\n"
     ]
    }
   ],
   "source": [
    "MODEL_LENGTH=200\n",
    "dataset=Dataset(topics=z_one_hot, model_length=MODEL_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "print(dataset.__getitem__(0)[0].shape)\n",
    "print(dataset.__getitem__(0)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=64\n",
    "model=LSTM(input_size=NUM_TOPICS, hidden_size=HIDDEN_SIZE, output_size=NUM_TOPICS, model_length=MODEL_LENGTH)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9110419750213623}\n",
      "{'loss': 3.9143152236938477}\n",
      "{'loss': 3.913928985595703}\n",
      "{'loss': 3.9124531745910645}\n",
      "{'loss': 3.9105522632598877}\n",
      "{'loss': 3.9129912853240967}\n",
      "{'loss': 3.9142117500305176}\n",
      "{'loss': 3.9132254123687744}\n",
      "{'loss': 3.910290479660034}\n",
      "{'loss': 3.9109063148498535}\n",
      "{'loss': 3.910454750061035}\n",
      "{'loss': 3.913980484008789}\n",
      "{'loss': 3.9103245735168457}\n",
      "{'loss': 3.9103775024414062}\n",
      "{'loss': 3.9103240966796875}\n",
      "{'loss': 3.911623239517212}\n",
      "{'loss': 3.9108684062957764}\n",
      "{'loss': 3.910801410675049}\n",
      "{'loss': 3.913306713104248}\n",
      "{'loss': 3.9101240634918213}\n",
      "{'loss': 3.9120521545410156}\n",
      "{'loss': 3.910109519958496}\n",
      "{'loss': 3.912790060043335}\n",
      "{'loss': 3.912604331970215}\n",
      "{'loss': 3.9132800102233887}\n",
      "{'loss': 3.9147961139678955}\n",
      "{'loss': 3.9139156341552734}\n",
      "{'loss': 3.9151620864868164}\n",
      "{'loss': 3.915220260620117}\n",
      "{'loss': 3.912921905517578}\n",
      "{'loss': 3.914076566696167}\n",
      "{'loss': 3.9107680320739746}\n",
      "{'loss': 3.9135546684265137}\n",
      "{'loss': 3.910661458969116}\n",
      "{'loss': 3.91335129737854}\n",
      "{'loss': 3.9133925437927246}\n",
      "{'loss': 3.9103736877441406}\n",
      "{'loss': 3.9107539653778076}\n",
      "{'loss': 3.912292718887329}\n",
      "{'loss': 3.913611888885498}\n",
      "{'loss': 3.9132955074310303}\n",
      "{'loss': 3.9120519161224365}\n",
      "{'loss': 3.9136455059051514}\n",
      "{'loss': 3.911719799041748}\n",
      "{'loss': 3.913818597793579}\n",
      "{'loss': 3.9101130962371826}\n",
      "{'loss': 3.913508892059326}\n",
      "{'loss': 3.9135680198669434}\n",
      "{'loss': 3.912388563156128}\n",
      "{'loss': 3.9149842262268066}\n",
      "{'loss': 3.91050386428833}\n",
      "{'loss': 3.9103505611419678}\n",
      "{'loss': 3.9130027294158936}\n",
      "{'loss': 3.9147634506225586}\n",
      "{'loss': 3.913321018218994}\n",
      "{'loss': 3.9133849143981934}\n",
      "{'loss': 3.910003900527954}\n",
      "{'loss': 3.9142143726348877}\n",
      "{'loss': 3.91377329826355}\n",
      "{'loss': 3.9133353233337402}\n",
      "{'loss': 3.9137356281280518}\n",
      "{'loss': 3.9122447967529297}\n",
      "{'loss': 3.9101386070251465}\n",
      "{'loss': 3.91263484954834}\n",
      "{'loss': 3.914741277694702}\n",
      "{'loss': 3.912288188934326}\n",
      "{'loss': 3.91257643699646}\n",
      "{'loss': 3.9101526737213135}\n",
      "{'loss': 3.913388252258301}\n",
      "{'loss': 3.9133222103118896}\n",
      "{'loss': 3.9108333587646484}\n",
      "{'loss': 3.913205623626709}\n",
      "{'loss': 3.9103736877441406}\n",
      "{'loss': 3.9101641178131104}\n",
      "{'loss': 3.912721633911133}\n",
      "{'loss': 3.910475730895996}\n",
      "{'loss': 3.9100544452667236}\n",
      "{'loss': 3.9122583866119385}\n",
      "{'loss': 3.9126904010772705}\n",
      "{'loss': 3.9132726192474365}\n",
      "{'loss': 3.913341522216797}\n",
      "{'loss': 3.913663148880005}\n",
      "{'loss': 3.9149041175842285}\n",
      "{'loss': 3.9150516986846924}\n",
      "{'loss': 3.9130008220672607}\n",
      "{'loss': 3.9148855209350586}\n",
      "{'loss': 3.9099552631378174}\n",
      "{'loss': 3.909700632095337}\n",
      "{'loss': 3.9120256900787354}\n",
      "{'loss': 3.9100589752197266}\n",
      "{'loss': 3.9134011268615723}\n",
      "{'loss': 3.9146792888641357}\n",
      "{'loss': 3.9140312671661377}\n",
      "{'loss': 3.9098849296569824}\n",
      "{'loss': 3.914947986602783}\n",
      "{'loss': 3.915050983428955}\n",
      "{'loss': 3.9150891304016113}\n",
      "{'loss': 3.9097845554351807}\n",
      "{'loss': 3.9096007347106934}\n",
      "{'loss': 3.9114627838134766}\n",
      "{'loss': 3.9144110679626465}\n",
      "{'loss': 3.914759874343872}\n",
      "{'loss': 3.9123339653015137}\n",
      "{'loss': 3.913633108139038}\n",
      "{'loss': 3.911177396774292}\n",
      "{'loss': 3.910095691680908}\n",
      "{'loss': 3.9105911254882812}\n",
      "{'loss': 3.9145724773406982}\n",
      "{'loss': 3.9094178676605225}\n",
      "{'loss': 3.913137197494507}\n",
      "{'loss': 3.91100811958313}\n",
      "{'loss': 3.9137825965881348}\n",
      "{'loss': 3.91359543800354}\n",
      "{'loss': 3.913182020187378}\n",
      "{'loss': 3.910201072692871}\n",
      "{'loss': 3.912682056427002}\n",
      "{'loss': 3.912623167037964}\n",
      "{'loss': 3.9135096073150635}\n",
      "{'loss': 3.913557767868042}\n",
      "{'loss': 3.9111530780792236}\n",
      "{'loss': 3.913424253463745}\n",
      "{'loss': 3.914466619491577}\n",
      "{'loss': 3.9109959602355957}\n",
      "{'loss': 3.913099765777588}\n",
      "{'loss': 3.913325786590576}\n",
      "{'loss': 3.9103734493255615}\n",
      "{'loss': 3.910568952560425}\n",
      "{'loss': 3.9106357097625732}\n",
      "{'loss': 3.9105324745178223}\n",
      "{'loss': 3.9100990295410156}\n",
      "{'loss': 3.9126241207122803}\n",
      "{'loss': 3.910597324371338}\n",
      "{'loss': 3.914480209350586}\n",
      "{'loss': 3.913710355758667}\n",
      "{'loss': 3.9103403091430664}\n",
      "{'loss': 3.913858652114868}\n",
      "{'loss': 3.9138500690460205}\n",
      "{'loss': 3.9125096797943115}\n",
      "{'loss': 3.9105944633483887}\n",
      "{'loss': 3.9137167930603027}\n",
      "{'loss': 3.9102632999420166}\n",
      "{'loss': 3.9129183292388916}\n",
      "{'loss': 3.9101638793945312}\n",
      "{'loss': 3.9134955406188965}\n",
      "{'loss': 3.91221022605896}\n",
      "{'loss': 3.9113271236419678}\n",
      "{'loss': 3.9101271629333496}\n",
      "{'loss': 3.9096262454986572}\n",
      "{'loss': 3.9124233722686768}\n",
      "{'loss': 3.914438247680664}\n",
      "{'loss': 3.913313865661621}\n",
      "{'loss': 3.914827346801758}\n",
      "{'loss': 3.9132213592529297}\n",
      "{'loss': 3.9111318588256836}\n",
      "{'loss': 3.910245895385742}\n",
      "{'loss': 3.9103405475616455}\n",
      "{'loss': 3.912593126296997}\n",
      "{'loss': 3.9136762619018555}\n",
      "{'loss': 3.910562038421631}\n",
      "{'loss': 3.910052537918091}\n",
      "{'loss': 3.9132916927337646}\n",
      "{'loss': 3.910351514816284}\n",
      "{'loss': 3.9092466831207275}\n",
      "{'loss': 3.9146971702575684}\n",
      "{'loss': 3.9128713607788086}\n",
      "{'loss': 3.9103572368621826}\n",
      "{'loss': 3.9131040573120117}\n",
      "{'loss': 3.9100914001464844}\n",
      "{'loss': 3.912914752960205}\n",
      "{'loss': 3.913785457611084}\n",
      "{'loss': 3.9147753715515137}\n",
      "{'loss': 3.9106454849243164}\n",
      "{'loss': 3.915022850036621}\n",
      "{'loss': 3.915090560913086}\n",
      "{'loss': 3.9097962379455566}\n",
      "{'loss': 3.910600185394287}\n",
      "{'loss': 3.914832592010498}\n",
      "{'loss': 3.914989948272705}\n",
      "{'loss': 3.9127793312072754}\n",
      "{'loss': 3.9148499965667725}\n",
      "{'loss': 3.9150185585021973}\n",
      "{'loss': 3.912325859069824}\n",
      "{'loss': 3.911750078201294}\n",
      "{'loss': 3.9095442295074463}\n",
      "{'loss': 3.9101595878601074}\n",
      "{'loss': 3.9117987155914307}\n",
      "{'loss': 3.9136266708374023}\n",
      "{'loss': 3.9100937843322754}\n",
      "{'loss': 3.9145288467407227}\n",
      "{'loss': 3.913210391998291}\n",
      "{'loss': 3.9089443683624268}\n",
      "{'loss': 3.910069704055786}\n",
      "{'loss': 3.9103989601135254}\n",
      "{'loss': 3.9116387367248535}\n",
      "{'loss': 3.913374185562134}\n",
      "{'loss': 3.9146814346313477}\n",
      "{'loss': 3.914865493774414}\n",
      "{'loss': 3.9128689765930176}\n",
      "{'loss': 3.9147915840148926}\n",
      "{'loss': 3.9116604328155518}\n",
      "{'loss': 3.913508653640747}\n",
      "{'loss': 3.9146647453308105}\n",
      "{'loss': 3.9129176139831543}\n",
      "{'loss': 3.9134902954101562}\n",
      "{'loss': 3.913297414779663}\n",
      "{'loss': 3.9131927490234375}\n",
      "{'loss': 3.9110891819000244}\n",
      "{'loss': 3.909842014312744}\n",
      "{'loss': 3.910083532333374}\n",
      "{'loss': 3.912668228149414}\n",
      "{'loss': 3.9137189388275146}\n",
      "{'loss': 3.9103739261627197}\n",
      "{'loss': 3.9110774993896484}\n",
      "{'loss': 3.913390636444092}\n",
      "{'loss': 3.9109947681427}\n",
      "{'loss': 3.912839651107788}\n",
      "{'loss': 3.912606954574585}\n",
      "{'loss': 3.9147560596466064}\n",
      "{'loss': 3.910670280456543}\n",
      "{'loss': 3.913201093673706}\n",
      "{'loss': 3.909733772277832}\n",
      "{'loss': 3.9145779609680176}\n",
      "{'loss': 3.9100725650787354}\n",
      "{'loss': 3.912574291229248}\n",
      "{'loss': 3.9103972911834717}\n",
      "{'loss': 3.914132833480835}\n",
      "{'loss': 3.9106905460357666}\n",
      "{'loss': 3.913369655609131}\n",
      "{'loss': 3.9135091304779053}\n",
      "{'loss': 3.9104695320129395}\n",
      "{'loss': 3.909986734390259}\n",
      "{'loss': 3.913931369781494}\n",
      "{'loss': 3.9137790203094482}\n",
      "{'loss': 3.910094976425171}\n",
      "{'loss': 3.9132590293884277}\n",
      "{'loss': 3.912259578704834}\n",
      "{'loss': 3.910372257232666}\n",
      "{'loss': 3.914391040802002}\n",
      "{'loss': 3.9136905670166016}\n",
      "{'loss': 3.911276340484619}\n",
      "{'loss': 3.9102373123168945}\n",
      "{'loss': 3.912163019180298}\n",
      "{'loss': 3.9136438369750977}\n",
      "{'loss': 3.914870023727417}\n",
      "{'loss': 3.910259246826172}\n",
      "{'loss': 3.913471221923828}\n",
      "{'loss': 3.9134466648101807}\n",
      "{'loss': 3.9147419929504395}\n",
      "{'loss': 3.9106807708740234}\n",
      "{'loss': 3.9103102684020996}\n",
      "{'loss': 3.9113259315490723}\n",
      "{'loss': 3.914760112762451}\n",
      "{'loss': 3.9104862213134766}\n",
      "{'loss': 3.9141387939453125}\n",
      "{'loss': 3.9130566120147705}\n",
      "{'loss': 3.9107658863067627}\n",
      "{'loss': 3.9103405475616455}\n",
      "{'loss': 3.9105496406555176}\n",
      "{'loss': 3.9113099575042725}\n",
      "{'loss': 3.9102184772491455}\n",
      "{'loss': 3.9134182929992676}\n",
      "{'loss': 3.9105303287506104}\n",
      "{'loss': 3.9132227897644043}\n",
      "{'loss': 3.909799814224243}\n",
      "{'loss': 3.9119791984558105}\n",
      "{'loss': 3.9102838039398193}\n",
      "{'loss': 3.913681983947754}\n",
      "{'loss': 3.9130096435546875}\n",
      "{'loss': 3.91056227684021}\n",
      "{'loss': 3.9101595878601074}\n",
      "{'loss': 3.9145140647888184}\n",
      "{'loss': 3.9141483306884766}\n",
      "{'loss': 3.913029432296753}\n",
      "{'loss': 3.913649082183838}\n",
      "{'loss': 3.9142866134643555}\n",
      "{'loss': 3.9102509021759033}\n",
      "{'loss': 3.913797378540039}\n",
      "{'loss': 3.91361403465271}\n",
      "{'loss': 3.909979820251465}\n",
      "{'loss': 3.91117787361145}\n",
      "{'loss': 3.9104344844818115}\n",
      "{'loss': 3.913512706756592}\n",
      "{'loss': 3.909825563430786}\n",
      "{'loss': 3.9101457595825195}\n",
      "{'loss': 3.9133543968200684}\n",
      "{'loss': 3.9138078689575195}\n",
      "{'loss': 3.9136650562286377}\n",
      "{'loss': 3.91036319732666}\n",
      "{'loss': 3.9101905822753906}\n",
      "{'loss': 3.912654399871826}\n",
      "{'loss': 3.9103851318359375}\n",
      "{'loss': 3.9105489253997803}\n",
      "{'loss': 3.913766860961914}\n",
      "{'loss': 3.913588047027588}\n",
      "{'loss': 3.9134254455566406}\n",
      "{'loss': 3.9137983322143555}\n",
      "{'loss': 3.913454055786133}\n",
      "{'loss': 3.9137086868286133}\n",
      "{'loss': 3.9104650020599365}\n",
      "{'loss': 3.9134771823883057}\n",
      "{'loss': 3.910649538040161}\n",
      "{'loss': 3.910259246826172}\n",
      "{'loss': 3.9136528968811035}\n",
      "{'loss': 3.9095005989074707}\n",
      "{'loss': 3.9141948223114014}\n",
      "{'loss': 3.9141924381256104}\n",
      "{'loss': 3.9116463661193848}\n",
      "{'loss': 3.9134786128997803}\n",
      "{'loss': 3.911156415939331}\n",
      "{'loss': 3.9139719009399414}\n",
      "{'loss': 3.914308547973633}\n",
      "{'loss': 3.91318678855896}\n",
      "{'loss': 3.9142231941223145}\n",
      "{'loss': 3.9102563858032227}\n",
      "{'loss': 3.91190505027771}\n",
      "{'loss': 3.913393020629883}\n",
      "{'loss': 3.9111475944519043}\n",
      "{'loss': 3.9139328002929688}\n",
      "{'loss': 3.9134397506713867}\n",
      "{'loss': 3.913259983062744}\n",
      "{'loss': 3.9134907722473145}\n",
      "{'loss': 3.910750150680542}\n",
      "{'loss': 3.913472890853882}\n",
      "{'loss': 3.9133501052856445}\n",
      "{'loss': 3.9132113456726074}\n",
      "{'loss': 3.9138007164001465}\n",
      "{'loss': 3.9102163314819336}\n",
      "{'loss': 3.9113242626190186}\n",
      "{'loss': 3.911022186279297}\n",
      "{'loss': 3.9104039669036865}\n",
      "{'loss': 3.914445161819458}\n",
      "{'loss': 3.9147815704345703}\n",
      "{'loss': 3.912386178970337}\n",
      "{'loss': 3.913595199584961}\n",
      "{'loss': 3.909992218017578}\n",
      "{'loss': 3.91357684135437}\n",
      "{'loss': 3.9144155979156494}\n",
      "{'loss': 3.910458564758301}\n",
      "{'loss': 3.9128637313842773}\n",
      "{'loss': 3.914006471633911}\n",
      "{'loss': 3.9129245281219482}\n",
      "{'loss': 3.9135024547576904}\n",
      "{'loss': 3.909529447555542}\n",
      "{'loss': 3.9132537841796875}\n",
      "{'loss': 3.9141664505004883}\n",
      "{'loss': 3.912810802459717}\n",
      "{'loss': 3.913869619369507}\n",
      "{'loss': 3.910386800765991}\n",
      "{'loss': 3.9137322902679443}\n",
      "{'loss': 3.9146595001220703}\n",
      "{'loss': 3.9112727642059326}\n",
      "{'loss': 3.9099576473236084}\n",
      "{'loss': 3.910428285598755}\n",
      "{'loss': 3.9142019748687744}\n",
      "{'loss': 3.913513660430908}\n",
      "{'loss': 3.9125540256500244}\n",
      "{'loss': 3.9136745929718018}\n",
      "{'loss': 3.913475751876831}\n",
      "{'loss': 3.914123296737671}\n",
      "{'loss': 3.91137433052063}\n",
      "{'loss': 3.910700559616089}\n",
      "{'loss': 3.9133336544036865}\n",
      "{'loss': 3.9111831188201904}\n",
      "{'loss': 3.9136531352996826}\n",
      "{'loss': 3.910295009613037}\n",
      "{'loss': 3.9135265350341797}\n",
      "{'loss': 3.9102439880371094}\n",
      "{'loss': 3.9096152782440186}\n",
      "{'loss': 3.9140920639038086}\n",
      "{'loss': 3.913280725479126}\n",
      "{'loss': 3.9103145599365234}\n",
      "{'loss': 3.9114530086517334}\n",
      "{'loss': 3.9138128757476807}\n",
      "{'loss': 3.912778854370117}\n",
      "{'loss': 3.913562059402466}\n",
      "{'loss': 3.9109442234039307}\n",
      "{'loss': 3.9134681224823}\n",
      "{'loss': 3.9106719493865967}\n",
      "{'loss': 3.910861015319824}\n",
      "{'loss': 3.910095691680908}\n",
      "{'loss': 3.9133849143981934}\n",
      "{'loss': 3.914750814437866}\n",
      "{'loss': 3.9135406017303467}\n",
      "{'loss': 3.911684513092041}\n",
      "{'loss': 3.913862705230713}\n",
      "{'loss': 3.9094250202178955}\n",
      "{'loss': 3.9137308597564697}\n",
      "{'loss': 3.912926435470581}\n",
      "{'loss': 3.9092354774475098}\n",
      "{'loss': 3.9130048751831055}\n",
      "{'loss': 3.9133496284484863}\n",
      "{'loss': 3.9142606258392334}\n",
      "{'loss': 3.913546562194824}\n",
      "{'loss': 3.9102375507354736}\n",
      "{'loss': 3.9097421169281006}\n",
      "{'loss': 3.910945415496826}\n",
      "{'loss': 3.9122045040130615}\n",
      "{'loss': 3.9132604598999023}\n",
      "{'loss': 3.9145305156707764}\n",
      "{'loss': 3.909480571746826}\n",
      "{'loss': 3.913025379180908}\n",
      "{'loss': 3.9134433269500732}\n",
      "{'loss': 3.914065361022949}\n",
      "{'loss': 3.9121906757354736}\n",
      "{'loss': 3.9137561321258545}\n",
      "{'loss': 3.910733938217163}\n",
      "{'loss': 3.9151413440704346}\n",
      "{'loss': 3.910959243774414}\n",
      "{'loss': 3.913347005844116}\n",
      "{'loss': 3.910144805908203}\n",
      "{'loss': 3.9116148948669434}\n",
      "{'loss': 3.914729595184326}\n",
      "{'loss': 3.913634777069092}\n",
      "{'loss': 3.913846254348755}\n",
      "{'loss': 3.9135866165161133}\n",
      "{'loss': 3.9106552600860596}\n",
      "{'loss': 3.9105260372161865}\n",
      "{'loss': 3.913417339324951}\n",
      "{'loss': 3.9138805866241455}\n",
      "{'loss': 3.9101603031158447}\n",
      "{'loss': 3.9140424728393555}\n",
      "{'loss': 3.9151499271392822}\n",
      "{'loss': 3.91260027885437}\n",
      "{'loss': 3.913445234298706}\n",
      "{'loss': 3.9105775356292725}\n",
      "{'loss': 3.913504123687744}\n",
      "{'loss': 3.9103875160217285}\n",
      "{'loss': 3.9119508266448975}\n",
      "{'loss': 3.9124693870544434}\n",
      "{'loss': 3.9146320819854736}\n",
      "{'loss': 3.9103994369506836}\n",
      "{'loss': 3.914992332458496}\n",
      "{'loss': 3.91353702545166}\n",
      "{'loss': 3.9135830402374268}\n",
      "{'loss': 3.912128210067749}\n",
      "{'loss': 3.909397602081299}\n",
      "{'loss': 3.9117088317871094}\n",
      "{'loss': 3.91424822807312}\n",
      "{'loss': 3.909583330154419}\n",
      "{'loss': 3.913191556930542}\n",
      "{'loss': 3.914390802383423}\n",
      "{'loss': 3.9094977378845215}\n",
      "{'loss': 3.9140560626983643}\n",
      "{'loss': 3.911623954772949}\n",
      "{'loss': 3.910818576812744}\n",
      "{'loss': 3.910588264465332}\n",
      "{'loss': 3.9101951122283936}\n",
      "{'loss': 3.910161256790161}\n",
      "{'loss': 3.9143738746643066}\n",
      "{'loss': 3.9104270935058594}\n",
      "{'loss': 3.9141013622283936}\n",
      "{'loss': 3.914429187774658}\n",
      "{'loss': 3.9134156703948975}\n",
      "{'loss': 3.91086483001709}\n",
      "{'loss': 3.913590669631958}\n",
      "{'loss': 3.9135797023773193}\n",
      "{'loss': 3.9136176109313965}\n",
      "{'loss': 3.9099926948547363}\n",
      "{'loss': 3.9148497581481934}\n",
      "{'loss': 3.910318613052368}\n",
      "{'loss': 3.9134786128997803}\n",
      "{'loss': 3.91351056098938}\n",
      "{'loss': 3.9134585857391357}\n",
      "{'loss': 3.911214828491211}\n",
      "{'loss': 3.910335063934326}\n",
      "{'loss': 3.9115288257598877}\n",
      "{'loss': 3.909789800643921}\n",
      "{'loss': 3.910393238067627}\n",
      "{'loss': 3.913313627243042}\n",
      "{'loss': 3.90969181060791}\n",
      "{'loss': 3.9141438007354736}\n",
      "{'loss': 3.9139418601989746}\n",
      "{'loss': 3.9098706245422363}\n",
      "{'loss': 3.910588264465332}\n",
      "{'loss': 3.909703254699707}\n",
      "{'loss': 3.9106228351593018}\n",
      "{'loss': 3.9099225997924805}\n",
      "{'loss': 3.9104835987091064}\n",
      "{'loss': 3.913997173309326}\n",
      "{'loss': 3.9139232635498047}\n",
      "{'loss': 3.9136269092559814}\n",
      "{'loss': 3.914818048477173}\n",
      "{'loss': 3.9150099754333496}\n",
      "{'loss': 3.913588523864746}\n",
      "{'loss': 3.910101890563965}\n",
      "{'loss': 3.9138636589050293}\n",
      "{'loss': 3.911623477935791}\n",
      "{'loss': 3.91281795501709}\n",
      "{'loss': 3.9136674404144287}\n",
      "{'loss': 3.910482883453369}\n",
      "{'loss': 3.909850597381592}\n",
      "{'loss': 3.9142327308654785}\n",
      "{'loss': 3.9100217819213867}\n",
      "{'loss': 3.910184621810913}\n",
      "{'loss': 3.9100115299224854}\n",
      "{'loss': 3.912785530090332}\n",
      "{'loss': 3.9126272201538086}\n",
      "{'loss': 3.914783239364624}\n",
      "{'loss': 3.9104440212249756}\n",
      "{'loss': 3.9116625785827637}\n",
      "{'loss': 3.9130382537841797}\n",
      "{'loss': 3.914048910140991}\n",
      "{'loss': 3.9102582931518555}\n",
      "{'loss': 3.9136135578155518}\n",
      "{'loss': 3.912909984588623}\n",
      "{'loss': 3.9101650714874268}\n",
      "{'loss': 3.9095234870910645}\n",
      "{'loss': 3.9142074584960938}\n",
      "{'loss': 3.914555549621582}\n",
      "{'loss': 3.9133784770965576}\n",
      "{'loss': 3.913402795791626}\n",
      "{'loss': 3.9132537841796875}\n",
      "{'loss': 3.9104323387145996}\n",
      "{'loss': 3.913964033126831}\n",
      "{'loss': 3.9138898849487305}\n",
      "{'loss': 3.910806894302368}\n",
      "{'loss': 3.909559965133667}\n",
      "{'loss': 3.913301706314087}\n",
      "{'loss': 3.9129374027252197}\n",
      "{'loss': 3.9133546352386475}\n",
      "{'loss': 3.913217782974243}\n",
      "{'loss': 3.914715528488159}\n",
      "{'loss': 3.913975238800049}\n",
      "{'loss': 3.9109363555908203}\n",
      "{'loss': 3.9102869033813477}\n",
      "{'loss': 3.913499116897583}\n",
      "{'loss': 3.914743185043335}\n",
      "{'loss': 3.9135372638702393}\n",
      "{'loss': 3.9148623943328857}\n",
      "{'loss': 3.9094457626342773}\n",
      "{'loss': 3.9147467613220215}\n",
      "{'loss': 3.910318613052368}\n",
      "{'loss': 3.9137706756591797}\n",
      "{'loss': 3.9139692783355713}\n",
      "{'loss': 3.9102344512939453}\n",
      "{'loss': 3.9124274253845215}\n",
      "{'loss': 3.913630962371826}\n",
      "{'loss': 3.913728713989258}\n",
      "{'loss': 3.9093191623687744}\n",
      "{'loss': 3.910365343093872}\n",
      "{'loss': 3.910506248474121}\n",
      "{'loss': 3.910425901412964}\n",
      "{'loss': 3.9104247093200684}\n",
      "{'loss': 3.9133987426757812}\n",
      "{'loss': 3.9131462574005127}\n",
      "{'loss': 3.9143946170806885}\n",
      "{'loss': 3.913639783859253}\n",
      "{'loss': 3.9109714031219482}\n",
      "{'loss': 3.9147865772247314}\n",
      "{'loss': 3.910736322402954}\n",
      "{'loss': 3.9108469486236572}\n",
      "{'loss': 3.912961483001709}\n",
      "{'loss': 3.914783000946045}\n",
      "{'loss': 3.9150023460388184}\n",
      "{'loss': 3.913534164428711}\n",
      "{'loss': 3.910022258758545}\n",
      "{'loss': 3.9115986824035645}\n",
      "{'loss': 3.9131922721862793}\n",
      "{'loss': 3.9143590927124023}\n",
      "{'loss': 3.913755416870117}\n",
      "{'loss': 3.910165309906006}\n",
      "{'loss': 3.9104433059692383}\n",
      "{'loss': 3.913588762283325}\n",
      "{'loss': 3.910635471343994}\n",
      "{'loss': 3.9133763313293457}\n",
      "{'loss': 3.9104230403900146}\n",
      "{'loss': 3.9108545780181885}\n",
      "{'loss': 3.911620855331421}\n",
      "{'loss': 3.910273313522339}\n",
      "{'loss': 3.912299871444702}\n",
      "{'loss': 3.913008451461792}\n",
      "{'loss': 3.9107751846313477}\n",
      "{'loss': 3.9102044105529785}\n",
      "{'loss': 3.910001039505005}\n",
      "{'loss': 3.9133524894714355}\n",
      "{'loss': 3.910200357437134}\n",
      "{'loss': 3.913635015487671}\n",
      "{'loss': 3.9110610485076904}\n",
      "{'loss': 3.913501501083374}\n",
      "{'loss': 3.9131693840026855}\n",
      "{'loss': 3.9103736877441406}\n",
      "{'loss': 3.9135279655456543}\n",
      "{'loss': 3.9129843711853027}\n",
      "{'loss': 3.9133622646331787}\n",
      "{'loss': 3.913210153579712}\n",
      "{'loss': 3.9112019538879395}\n",
      "{'loss': 3.9134366512298584}\n",
      "{'loss': 3.9106383323669434}\n",
      "{'loss': 3.9104645252227783}\n",
      "{'loss': 3.914034128189087}\n",
      "{'loss': 3.9106433391571045}\n",
      "{'loss': 3.911182403564453}\n",
      "{'loss': 3.9125499725341797}\n",
      "{'loss': 3.9100468158721924}\n",
      "{'loss': 3.9139883518218994}\n",
      "{'loss': 3.9102683067321777}\n",
      "{'loss': 3.910095691680908}\n",
      "{'loss': 3.9134936332702637}\n",
      "{'loss': 3.9137065410614014}\n",
      "{'loss': 3.914645195007324}\n",
      "{'loss': 3.911285638809204}\n",
      "{'loss': 3.9137163162231445}\n",
      "{'loss': 3.911085605621338}\n",
      "{'loss': 3.9146804809570312}\n",
      "{'loss': 3.910759449005127}\n",
      "{'loss': 3.9108591079711914}\n",
      "{'loss': 3.9121713638305664}\n",
      "{'loss': 3.910425901412964}\n",
      "{'loss': 3.91009783744812}\n",
      "{'loss': 3.9142327308654785}\n",
      "{'loss': 3.9111528396606445}\n",
      "{'loss': 3.910102367401123}\n",
      "{'loss': 3.9131155014038086}\n",
      "{'loss': 3.9099178314208984}\n",
      "{'loss': 3.9103870391845703}\n",
      "{'loss': 3.913689136505127}\n",
      "{'loss': 3.913382053375244}\n",
      "{'loss': 3.913384437561035}\n",
      "{'loss': 3.9133522510528564}\n",
      "{'loss': 3.9116554260253906}\n",
      "{'loss': 3.910626173019409}\n",
      "{'loss': 3.9143829345703125}\n",
      "{'loss': 3.9111900329589844}\n",
      "{'loss': 3.9136950969696045}\n",
      "{'loss': 3.9118282794952393}\n",
      "{'loss': 3.910508871078491}\n",
      "{'loss': 3.910132884979248}\n",
      "{'loss': 3.9109914302825928}\n",
      "{'loss': 3.910393714904785}\n",
      "{'loss': 3.914428472518921}\n",
      "{'loss': 3.910212993621826}\n",
      "{'loss': 3.913137435913086}\n",
      "{'loss': 3.913757801055908}\n",
      "{'loss': 3.9147088527679443}\n",
      "{'loss': 3.9130773544311523}\n",
      "{'loss': 3.910214424133301}\n",
      "{'loss': 3.9118335247039795}\n",
      "{'loss': 3.9147403240203857}\n",
      "{'loss': 3.912898540496826}\n",
      "{'loss': 3.910240650177002}\n",
      "{'loss': 3.9136886596679688}\n",
      "{'loss': 3.913376569747925}\n",
      "{'loss': 3.9149489402770996}\n",
      "{'loss': 3.9111931324005127}\n",
      "{'loss': 3.913792133331299}\n",
      "{'loss': 3.910349130630493}\n",
      "{'loss': 3.9139318466186523}\n",
      "{'loss': 3.9149434566497803}\n",
      "{'loss': 3.9103336334228516}\n",
      "{'loss': 3.9111826419830322}\n",
      "{'loss': 3.9101037979125977}\n",
      "{'loss': 3.9103846549987793}\n",
      "{'loss': 3.9133810997009277}\n",
      "{'loss': 3.910888195037842}\n",
      "{'loss': 3.9101786613464355}\n",
      "{'loss': 3.909564733505249}\n",
      "{'loss': 3.9099509716033936}\n",
      "{'loss': 3.914306402206421}\n",
      "{'loss': 3.9100778102874756}\n",
      "{'loss': 3.9145517349243164}\n",
      "{'loss': 3.911281108856201}\n",
      "{'loss': 3.9137802124023438}\n",
      "{'loss': 3.9148383140563965}\n",
      "{'loss': 3.910263776779175}\n",
      "{'loss': 3.910236358642578}\n",
      "{'loss': 3.913209915161133}\n",
      "{'loss': 3.9147181510925293}\n",
      "{'loss': 3.913930654525757}\n",
      "{'loss': 3.9099667072296143}\n",
      "{'loss': 3.9113428592681885}\n",
      "{'loss': 3.9149043560028076}\n",
      "{'loss': 3.9135847091674805}\n",
      "{'loss': 3.9110262393951416}\n",
      "{'loss': 3.909865140914917}\n",
      "{'loss': 3.9105026721954346}\n",
      "{'loss': 3.910674571990967}\n",
      "{'loss': 3.910416603088379}\n",
      "{'loss': 3.9107325077056885}\n",
      "{'loss': 3.913625717163086}\n",
      "{'loss': 3.9130589962005615}\n",
      "{'loss': 3.91011905670166}\n",
      "{'loss': 3.9102489948272705}\n",
      "{'loss': 3.9104740619659424}\n",
      "{'loss': 3.9132206439971924}\n",
      "{'loss': 3.913700819015503}\n",
      "{'loss': 3.910240411758423}\n",
      "{'loss': 3.9107577800750732}\n",
      "{'loss': 3.910787582397461}\n",
      "{'loss': 3.9119584560394287}\n",
      "{'loss': 3.9110267162323}\n",
      "{'loss': 3.9094107151031494}\n",
      "{'loss': 3.9114017486572266}\n",
      "{'loss': 3.9139177799224854}\n",
      "{'loss': 3.913987398147583}\n",
      "{'loss': 3.913675546646118}\n",
      "{'loss': 3.910613536834717}\n",
      "{'loss': 3.910003185272217}\n",
      "{'loss': 3.9103827476501465}\n",
      "{'loss': 3.912400484085083}\n",
      "{'loss': 3.910475492477417}\n",
      "{'loss': 3.91046404838562}\n",
      "{'loss': 3.9141123294830322}\n",
      "{'loss': 3.910236120223999}\n",
      "{'loss': 3.9112343788146973}\n",
      "{'loss': 3.9103140830993652}\n",
      "{'loss': 3.9090611934661865}\n",
      "{'loss': 3.912755012512207}\n",
      "{'loss': 3.912553071975708}\n",
      "{'loss': 3.913259983062744}\n",
      "{'loss': 3.9101221561431885}\n",
      "{'loss': 3.910374402999878}\n",
      "{'loss': 3.9140639305114746}\n",
      "{'loss': 3.9119699001312256}\n",
      "{'loss': 3.9146058559417725}\n",
      "{'loss': 3.9136507511138916}\n",
      "{'loss': 3.9110217094421387}\n",
      "{'loss': 3.910879373550415}\n",
      "{'loss': 3.913219928741455}\n",
      "{'loss': 3.9116148948669434}\n",
      "{'loss': 3.9093661308288574}\n",
      "{'loss': 3.9139065742492676}\n",
      "{'loss': 3.9132232666015625}\n",
      "{'loss': 3.913346290588379}\n",
      "{'loss': 3.91363787651062}\n",
      "{'loss': 3.912324905395508}\n",
      "{'loss': 3.9136621952056885}\n",
      "{'loss': 3.9106605052948}\n",
      "{'loss': 3.914137601852417}\n",
      "{'loss': 3.9098501205444336}\n",
      "{'loss': 3.911320924758911}\n",
      "{'loss': 3.9147417545318604}\n",
      "{'loss': 3.913522243499756}\n",
      "{'loss': 3.9092650413513184}\n",
      "{'loss': 3.9132509231567383}\n",
      "{'loss': 3.91298246383667}\n",
      "{'loss': 3.913304328918457}\n",
      "{'loss': 3.9131815433502197}\n",
      "{'loss': 3.913724899291992}\n",
      "{'loss': 3.9137485027313232}\n",
      "{'loss': 3.910097360610962}\n",
      "{'loss': 3.9109339714050293}\n",
      "{'loss': 3.913590431213379}\n",
      "{'loss': 3.9110233783721924}\n",
      "{'loss': 3.9145731925964355}\n",
      "{'loss': 3.9110677242279053}\n",
      "{'loss': 3.914390802383423}\n",
      "{'loss': 3.914457321166992}\n",
      "{'loss': 3.9102721214294434}\n",
      "{'loss': 3.909372329711914}\n",
      "{'loss': 3.9119136333465576}\n",
      "{'loss': 3.911062240600586}\n",
      "{'loss': 3.9133193492889404}\n",
      "{'loss': 3.914447069168091}\n",
      "{'loss': 3.9148054122924805}\n",
      "{'loss': 3.9136300086975098}\n",
      "{'loss': 3.9110186100006104}\n",
      "{'loss': 3.910078525543213}\n",
      "{'loss': 3.910569667816162}\n",
      "{'loss': 3.910710334777832}\n",
      "{'loss': 3.9140210151672363}\n",
      "{'loss': 3.9102866649627686}\n",
      "{'loss': 3.911475419998169}\n",
      "{'loss': 3.9120571613311768}\n",
      "{'loss': 3.9136273860931396}\n",
      "{'loss': 3.9147839546203613}\n",
      "{'loss': 3.909639835357666}\n",
      "{'loss': 3.9144508838653564}\n",
      "{'loss': 3.9119341373443604}\n",
      "{'loss': 3.914297580718994}\n",
      "{'loss': 3.9132721424102783}\n",
      "{'loss': 3.911039352416992}\n",
      "{'loss': 3.9135379791259766}\n",
      "{'loss': 3.913442611694336}\n",
      "{'loss': 3.914241313934326}\n",
      "{'loss': 3.9102623462677}\n",
      "{'loss': 3.910663366317749}\n",
      "{'loss': 3.910750389099121}\n",
      "{'loss': 3.9096221923828125}\n",
      "{'loss': 3.910522222518921}\n",
      "{'loss': 3.9141197204589844}\n",
      "{'loss': 3.9135308265686035}\n",
      "{'loss': 3.913076639175415}\n",
      "{'loss': 3.913384437561035}\n",
      "{'loss': 3.9132204055786133}\n",
      "{'loss': 3.911695957183838}\n",
      "{'loss': 3.9098575115203857}\n",
      "{'loss': 3.909661054611206}\n",
      "{'loss': 3.9146976470947266}\n",
      "{'loss': 3.91420316696167}\n",
      "{'loss': 3.9150798320770264}\n",
      "{'loss': 3.9132211208343506}\n",
      "{'loss': 3.914659023284912}\n",
      "{'loss': 3.910444974899292}\n",
      "{'loss': 3.9102556705474854}\n",
      "{'loss': 3.9102377891540527}\n",
      "{'loss': 3.909933567047119}\n",
      "{'loss': 3.9140818119049072}\n",
      "{'loss': 3.9099161624908447}\n",
      "{'loss': 3.9110519886016846}\n",
      "{'loss': 3.9106357097625732}\n"
     ]
    }
   ],
   "source": [
    "model.train_model(dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_one_hot_tensor = torch.FloatTensor(z_one_hot)\n",
    "z_one_hot_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0187, 0.0221, 0.0180, 0.0207, 0.0183, 0.0178, 0.0196, 0.0219, 0.0235,\n",
       "         0.0221, 0.0198, 0.0218, 0.0238, 0.0207, 0.0188, 0.0213, 0.0204, 0.0211,\n",
       "         0.0187, 0.0181, 0.0223, 0.0188, 0.0229, 0.0231, 0.0184, 0.0182, 0.0190,\n",
       "         0.0175, 0.0184, 0.0192, 0.0171, 0.0211, 0.0201, 0.0184, 0.0171, 0.0184,\n",
       "         0.0188, 0.0213, 0.0188, 0.0205, 0.0212, 0.0187, 0.0181, 0.0213, 0.0214,\n",
       "         0.0198, 0.0215, 0.0189, 0.0222, 0.0204]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_probability(z_one_hot_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01866646, 0.02212409, 0.01800099, 0.02065765, 0.01831837,\n",
       "       0.01777819, 0.01962738, 0.02190126, 0.02348709, 0.02208743,\n",
       "       0.01979189, 0.02176312, 0.02375273, 0.02074218, 0.01878493,\n",
       "       0.02132924, 0.02036189, 0.02106654, 0.01868558, 0.01806166,\n",
       "       0.02233759, 0.01879182, 0.02288273, 0.02312025, 0.01842999,\n",
       "       0.01815892, 0.01897769, 0.01746651, 0.01839547, 0.01919946,\n",
       "       0.01708745, 0.02105109, 0.02012437, 0.01837844, 0.01713775,\n",
       "       0.01840677, 0.01877698, 0.02133535, 0.01881841, 0.02047467,\n",
       "       0.02121725, 0.01867016, 0.01805095, 0.02134924, 0.0213901 ,\n",
       "       0.0198335 , 0.02154742, 0.01893549, 0.02224816, 0.02041735],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_next_probability(z_one_hot_tensor).detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample_next_z(z_one_hot_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSM:\n",
    "    def __init__(self, num_words, num_topics, T):\n",
    "        self.num_words = num_words\n",
    "        self.num_topics = num_topics\n",
    "        self.T = T\n",
    "        self.phi = np.zeros((T, num_words, num_topics))\n",
    "\n",
    "    def compute_MLE_SSM(self, ech_x, ech_z):\n",
    "        def compute_MLE_SSM_time_t_zt(t, z_t, ech_x, ech_z, num_words):\n",
    "            phi = np.zeros(num_words)\n",
    "            list_probas = []\n",
    "            for j in range(1,num_words+1):\n",
    "                num = len(np.where((ech_x[:,t-1]==j)&(ech_z[:,t-1]==z_t))[0])/ech_x.shape[0]\n",
    "                phi[j-1] = num\n",
    "                list_probas.append(num)\n",
    "\n",
    "            denom = np.sum(np.array(list_probas))\n",
    "            phi = phi / (denom + 1e-6)\n",
    "            return phi\n",
    "        \n",
    "        def compute_MLE_SSM_time_t(t, ech_x, ech_z, num_words, num_topics):\n",
    "            phi = np.zeros((num_words, num_topics))\n",
    "            for k in range(1, num_topics+1):\n",
    "                z_t = k\n",
    "                phi_zt = compute_MLE_SSM_time_t_zt(t=t, z_t=z_t, ech_x=ech_x, ech_z=ech_z, num_words=num_words)\n",
    "                phi[:,k-1] = phi_zt\n",
    "            return phi\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            self.phi[t] = compute_MLE_SSM_time_t(t, ech_x, ech_z, self.num_words, self.num_topics)\n",
    "    \n",
    "    def predict_proba(self, t, z_t):\n",
    "        return self.phi[t-1][:,z_t-1]\n",
    "    \n",
    "    def sample_xt(self, t, z_t):\n",
    "        proba = self.predict_proba(t, z_t)\n",
    "        sampled_xt = np.random.choice(len(proba), p=proba)\n",
    "        return sampled_xt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43, 12, 25, ..., 15, 34, 44],\n",
       "       [46,  1, 25, ...,  5, 26, 48],\n",
       "       [ 5, 45,  1, ..., 22, 28,  6],\n",
       "       ...,\n",
       "       [25, 15, 17, ..., 21, 17,  8],\n",
       "       [25, 12, 39, ..., 27, 44, 43],\n",
       "       [21, 47, 40, ..., 30, 42,  1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 10\n",
    "T = 200\n",
    "ech_z = np.random.randint(1, NUM_TOPICS, size=(num_samples, T))\n",
    "ech_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70, 490, 153, ..., 126,  29, 403],\n",
       "       [ 60, 345, 163, ..., 147,  26, 187],\n",
       "       [ 84, 456, 115, ..., 461, 147,  18],\n",
       "       ...,\n",
       "       [366, 224, 215, ...,  97, 237,  65],\n",
       "       [376, 166, 441, ..., 408, 367,  78],\n",
       "       [195,  82, 337, ..., 375, 436, 119]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_WORDS = 500\n",
    "ech_x = np.random.randint(1, NUM_WORDS, size=(num_samples, T))\n",
    "ech_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm = SSM(num_words=NUM_WORDS, num_topics=NUM_TOPICS, T=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 500, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssm.compute_MLE_SSM(ech_x, ech_z)\n",
    "ssm.phi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\phi[t,i,j]=$ probabilité au temps $t$ que le mot $i$ apparaisse sachant le topic $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zt=10\n",
    "t=4\n",
    "ssm.predict_proba(t, zt).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    z_1_t_minus_1=z_1_t_minus_1-1\n",
    "    z_one_hot = to_categorical(z_1_t_minus_1, num_classes=num_topics)\n",
    "    z_one_hot_tensor = torch.FloatTensor(z_one_hot)\n",
    "    softmax = lstm.predict_next_probability(z_one_hot_tensor).detach().numpy()[0]\n",
    "    phi_t = ssm.phi[t-1]\n",
    "    alpha = np.array([np.dot(softmax, phi_t[j,:]) for j in range(num_voc)])\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha_normalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    num = compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\n",
    "    denom = np.sum(num)\n",
    "    return num/(denom+1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gamma_unnormalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    z_1_t_minus_1=z_1_t_minus_1-1\n",
    "    z_one_hot = to_categorical(z_1_t_minus_1, num_classes=num_topics)\n",
    "    z_one_hot_tensor = torch.FloatTensor(z_one_hot)\n",
    "    softmax = lstm.predict_next_probability(z_one_hot_tensor).detach().numpy()[0]\n",
    "    phi_t = ssm.phi[t-1]\n",
    "    phi_xt = phi_t[xt-1, :]\n",
    "    return np.multiply(softmax, phi_xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gamma_normalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    num = compute_gamma_unnormalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm)\n",
    "    denom = np.sum(num)\n",
    "    return num/(denom+1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initialisation\n",
    "P=50\n",
    "NUM_TOPICS=50\n",
    "NUM_WORDS=200\n",
    "T=20\n",
    "\n",
    "lstm = model\n",
    "ssm = ssm\n",
    "\n",
    "Z_matrix=np.zeros((P, T+1))\n",
    "alpha_matrix=np.zeros((P,T+1))\n",
    "ancestor_matrix=np.zeros((P,T+1))\n",
    "\n",
    "\n",
    "z_1_T_star = np.random.choice(a=range(1,NUM_TOPICS+1), size=T)\n",
    "x = np.random.choice(a=range(1,NUM_WORDS+1), size=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_gibbs(P, num_topics, num_words, T, lstm_model, ssm_model, x, previous_z_1_T_star):\n",
    "    ##Init\n",
    "    Z_matrix=np.zeros((P, T+1))\n",
    "    alpha_matrix=np.zeros((P,T+1))\n",
    "    ancestor_matrix=np.zeros((P,T+1))\n",
    "    ##t=0\n",
    "    z_0 = np.random.choice(a=range(1,num_topics+1), size=P)\n",
    "    alpha_0 = np.repeat(1/P, P)\n",
    "    Z_matrix[:,0] = z_0\n",
    "    alpha_matrix[:,0] = alpha_0\n",
    "\n",
    "    #z[k:n]: du k-ème au n-1 ème\n",
    "    for t in range(1,T+1):\n",
    "        print(t)\n",
    "        a_t_minus_1 = 1 #ok\n",
    "        z_1_t = previous_z_1_T_star[:t] #ok\n",
    "        ancestor_matrix[0,t-1] = a_t_minus_1 #ok\n",
    "        Z_matrix[0, 1:t+1] = z_1_t #ok\n",
    "\n",
    "        for p in range(2,P+1):\n",
    "            alpha_t_minus_1_p=alpha_matrix[:,t-1]\n",
    "            #a_t_minus_1_p = np.random.choice(a=range(1,P+1), p=alpha_t_minus_1_p, size=1)[0] #ok\n",
    "            a_t_minus_1_p = np.argmax(alpha_t_minus_1_p)+1\n",
    "            ancestor_matrix[p-1, t-1] = a_t_minus_1_p #ok\n",
    "            if t ==1:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "                z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "                gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                                xt=x[t-1], \n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics = num_topics,\n",
    "                                                lstm = lstm_model,\n",
    "                                                ssm = ssm_model)\n",
    "                z_t_p = np.argmax(gamma_t_p)+1\n",
    "                z_1_t_p = z_t_p\n",
    "            else:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1] #ok\n",
    "                gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                                xt=x[t-1], \n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics = num_topics,\n",
    "                                                lstm = lstm_model,\n",
    "                                                ssm = ssm_model)\n",
    "                z_t_p = np.argmax(gamma_t_p)+1\n",
    "                #z_t_p = np.random.choice(a=range(1, NUM_TOPICS+1), p=gamma_t_p, size=1)[0]\n",
    "                z_1_t_p = np.append(z_1_t_minus_1_a_t_minus_1_p, z_t_p)\n",
    "            \n",
    "            Z_matrix[p-1, 1:t+1] = z_1_t_p\n",
    "\n",
    "        \n",
    "        for p in range(1, P+1):\n",
    "            a_t_minus_1_p = ancestor_matrix[p-1, t-1]\n",
    "            if t ==1:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "                z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "            else:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1]\n",
    "            \n",
    "            alpha_t_p = compute_alpha_normalized(t=t,\n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics=num_topics,\n",
    "                                                num_voc=num_words,\n",
    "                                                lstm=lstm,\n",
    "                                                ssm=ssm)\n",
    "            alpha_t_p = alpha_t_p[x[t-1]-1]\n",
    "            alpha_matrix[p-1,t] = alpha_t_p\n",
    "    alpha_T=alpha_matrix[:,-1]\n",
    "    r = np.argmax(alpha_T)+1\n",
    "    alpha_T_r = alpha_T[r-1, -1]\n",
    "    z_1_T = Z_matrix[alpha_T_r-1, 1:]\n",
    "    return z_1_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "##t=0\n",
    "z_0 = np.random.choice(a=range(1,NUM_TOPICS+1), size=P)\n",
    "alpha_0 = np.repeat(1/P, P)\n",
    "Z_matrix[:,0] = z_0\n",
    "alpha_matrix[:,0] = alpha_0\n",
    "\n",
    "#z[k:n]: du k-ème au n-1 ème\n",
    "for t in range(1,T+1):\n",
    "    print(t)\n",
    "    a_t_minus_1 = 1 #ok\n",
    "    z_1_t = z_1_T_star[:t] #ok\n",
    "    ancestor_matrix[0,t-1] = a_t_minus_1 #ok\n",
    "    Z_matrix[0, 1:t+1] = z_1_t #ok\n",
    "\n",
    "    for p in range(2,P+1):\n",
    "        alpha_t_minus_1_p=alpha_matrix[:,t-1]\n",
    "        #a_t_minus_1_p = np.random.choice(a=range(1,P+1), p=alpha_t_minus_1_p, size=1)[0] #ok\n",
    "        a_t_minus_1_p = np.argmax(alpha_t_minus_1_p)+1\n",
    "        ancestor_matrix[p-1, t-1] = a_t_minus_1_p #ok\n",
    "        if t ==1:\n",
    "            z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "            z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "            gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                             xt=x[t-1], \n",
    "                                             z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                             num_topics = NUM_TOPICS,\n",
    "                                             lstm = lstm,\n",
    "                                             ssm = ssm)\n",
    "            z_t_p = np.argmax(gamma_t_p)+1\n",
    "            z_1_t_p = z_t_p\n",
    "        else:\n",
    "            z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1] #ok\n",
    "            gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                             xt=x[t-1], \n",
    "                                             z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                             num_topics = NUM_TOPICS,\n",
    "                                             lstm = lstm,\n",
    "                                             ssm = ssm)\n",
    "            z_t_p = np.argmax(gamma_t_p)+1\n",
    "            #z_t_p = np.random.choice(a=range(1, NUM_TOPICS+1), p=gamma_t_p, size=1)[0]\n",
    "            z_1_t_p = np.append(z_1_t_minus_1_a_t_minus_1_p, z_t_p)\n",
    "        \n",
    "        Z_matrix[p-1, 1:t+1] = z_1_t_p\n",
    "\n",
    "    \n",
    "    for p in range(1, P+1):\n",
    "        a_t_minus_1_p = ancestor_matrix[p-1, t-1]\n",
    "        if t ==1:\n",
    "            z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "            z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "        else:\n",
    "            z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1]\n",
    "        \n",
    "        alpha_t_p = compute_alpha_normalized(t=t,\n",
    "                                             z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                             num_topics=NUM_TOPICS,\n",
    "                                             num_voc=NUM_WORDS,\n",
    "                                             lstm=lstm,\n",
    "                                             ssm=ssm)\n",
    "        alpha_t_p = alpha_t_p[x[t-1]-1]\n",
    "        alpha_matrix[p-1,t] = alpha_t_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_T=alpha_matrix[:,-1]\n",
    "r = np.argmax(alpha_T)+1\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_T_r = alpha_T[r-1]\n",
    "alpha_T_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_T_r = alpha_T[r-1]\n",
    "z_1_T = Z_matrix[int(alpha_T_r)-1, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 193476\n"
     ]
    }
   ],
   "source": [
    "# vocab = set()\n",
    "\n",
    "# for token in vectorized_data_lda[\"tokenized_text\"]:\n",
    "#     vocab.update([word for word in token])\n",
    "\n",
    "# vocab = sorted(vocab)\n",
    "# NUM_WORDS=len(vocab)\n",
    "# print('Unique words: {}'.format(NUM_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_MLE_SSM_time_t_zt(t, z_t, ech_x, ech_z, num_words):\n",
    "#     phi = np.zeros(num_words)\n",
    "#     list_probas = []\n",
    "#     for j in range(1,num_words+1):\n",
    "#         num = len(np.where((ech_x[:,t-1]==j)&(ech_z[:,t-1]==z_t))[0])/ech_x.shape[0]\n",
    "#         phi[j-1] = num\n",
    "#         list_probas.append(num)\n",
    "\n",
    "#     denom = np.sum(np.array(list_probas))\n",
    "#     phi = phi / (denom + 1e-6)\n",
    "#     return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_MLE_SSM_time_t(t, ech_x, ech_z, num_words, num_topics):\n",
    "#     phi = np.zeros((num_words, num_topics))\n",
    "#     for k in range(1, num_topics+1):\n",
    "#         z_t = k\n",
    "#         phi_zt = compute_MLE_SSM_time_t_zt(t=t, z_t=z_t, ech_x=ech_x, ech_z=ech_z, num_words=num_words)\n",
    "#         phi[:,k-1] = phi_zt\n",
    "#     return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_MLE_SSM(ech_x, ech_z, num_words, num_topics, T):\n",
    "#     phi = np.empty((0, num_words, num_topics))\n",
    "#     for t in range(T):\n",
    "#         phi_t = compute_MLE_SSM_time_t(t, ech_x, ech_z, num_words, num_topics)\n",
    "#         phi = np.concatenate([phi, phi_t[np.newaxis, :, :]])\n",
    "#     return phi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
