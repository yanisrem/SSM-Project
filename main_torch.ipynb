{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données et pré-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 5000\n",
    "max_review_length = 100\n",
    "INDEX_FROM = 3\n",
    "\n",
    "# --- Import the IMDB data and only consider the ``top_words``` most used words\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "np.load.__defaults__=(None, False, True, 'ASCII')\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train[0]): 100\n",
      "len(X_train[1]): 100\n",
      "X_train[0]: [   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      "    2   16]\n"
     ]
    }
   ],
   "source": [
    "# --- truncate and pad input sequences\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post', truncating='post', value=0)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post', truncating='post', value=0)\n",
    "\n",
    "print(\"len(X_train[0]):\", len(X_train[0]))\n",
    "print(\"len(X_train[1]):\", len(X_train[1]))\n",
    "print(\"X_train[0]:\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, model_length):\n",
    "        self.topics=topics\n",
    "        self.model_length=model_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.topics)-self.model_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_sequence=torch.tensor(self.topics[index:index+self.model_length, :])\n",
    "        target_sequence=torch.tensor(self.topics[index+1:index+self.model_length+1, :])\n",
    "\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size #dimension d'entrée (NUM_TOPICS)\n",
    "        self.hidden_size = hidden_size #nombre de neurones de la couche cachée\n",
    "        self.output_size = output_size #dimension d'outputs (NUM_TOPICS)\n",
    "        self.model_length=model_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        # self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        output=self.fc(output)\n",
    "        probabilities = F.softmax(output[:, -1, :], dim=1)\n",
    "        return probabilities, state\n",
    "    \n",
    "    def init_state(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size), #(NUM_LAYERS, BATCH SIZE, NUM_NEURONES)\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n",
    "    def train_model(self, dataset, optimizer, criterion):\n",
    "        state_h, state_c = self.init_state()\n",
    "        self.train()\n",
    "        for t, (x, y) in enumerate(dataset):\n",
    "            optimizer.zero_grad()\n",
    "            softmax , (state_h, state_c) = self(x, (state_h, state_c)) #softmax= p(z_{t+1}|z_1:t)\n",
    "            loss = criterion(softmax, y[:, -1, :])\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print({'loss': loss.item() })\n",
    "    \n",
    "    def predict_next_probability(self, input_sequence):\n",
    "        state_h, state_c = self.init_state()\n",
    "\n",
    "        # Forward pass jusqu'à t-1\n",
    "        for t in range(len(input_sequence)):\n",
    "            input_t = input_sequence[t].unsqueeze(0).unsqueeze(0)\n",
    "            _, (state_h, state_c) = self(input_t, (state_h, state_c))\n",
    "\n",
    "        # Obtenez les probabilités pour x_t\n",
    "        input_t = input_sequence[-1].unsqueeze(0).unsqueeze(0)\n",
    "        probabilities, _ = self(input_t, (state_h, state_c))\n",
    "\n",
    "        return probabilities\n",
    "    \n",
    "    def sample_next_z(self, input_sequence):\n",
    "        proba = self.predict_next_probability(input_sequence)\n",
    "        return torch.multinomial(proba, 1).item()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SSMPytorch:\n",
    "    def __init__(self, num_words, num_topics, T):\n",
    "        self.num_words = num_words\n",
    "        self.num_topics = num_topics\n",
    "        self.T = T\n",
    "        self.phi = torch.randn(T, num_words, num_topics) * 0.01\n",
    "        self.phi = torch.exp(self.phi - torch.max(self.phi, dim=1, keepdim=True).values)\n",
    "        self.phi = self.phi / torch.sum(self.phi, dim=1, keepdim=True)\n",
    "\n",
    "    def compute_MLE_SSM(self, ech_x, ech_z):\n",
    "        def compute_MLE_SSM_time_t(t, ech_x, ech_z, num_words, num_topics):\n",
    "            ech_x_t = ech_x[:, t]\n",
    "            ech_z_t = ech_z[:, t]\n",
    "            proba_matrix = torch.zeros((num_words, num_topics))\n",
    "            proba_matrix.scatter_add_(0, (ech_x_t - 1).unsqueeze(0), torch.nn.functional.one_hot(ech_z_t - 1, num_classes=num_topics).float())\n",
    "            row_sums = proba_matrix.sum(dim=0, keepdim=True)\n",
    "            proba_matrix_normalized = proba_matrix / (row_sums + 1e-6)\n",
    "            self.phi[t] = proba_matrix_normalized\n",
    "\n",
    "        for t in range(self.T):\n",
    "            compute_MLE_SSM_time_t(t, ech_x, ech_z, self.num_words, self.num_topics)\n",
    "\n",
    "    def predict_proba(self, t, z_t):\n",
    "        return self.phi[t - 1][:, z_t - 1]\n",
    "\n",
    "    def sample_xt(self, t, z_t):\n",
    "        proba = self.predict_proba(t, z_t)\n",
    "        sampled_xt = torch.multinomial(proba, 1).item()\n",
    "        return sampled_xt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    z_1_t_minus_1 = z_1_t_minus_1 - 1\n",
    "    z_one_hot = F.one_hot(z_1_t_minus_1, num_classes=num_topics).float()\n",
    "    softmax = lstm.predict_next_probability(z_one_hot).detach()\n",
    "    phi_t = ssm.phi[t - 1]\n",
    "    alpha = torch.tensor([torch.matmul(softmax, phi_t[j, :]) for j in range(num_voc)])\n",
    "    return alpha\n",
    "\n",
    "def compute_alpha_normalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    num = compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\n",
    "    denom = torch.sum(num) + 1e-6\n",
    "    return num / denom\n",
    "\n",
    "def compute_gamma_unnormalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    z_1_t_minus_1 = z_1_t_minus_1 - 1\n",
    "    z_one_hot = F.one_hot(z_1_t_minus_1, num_classes=num_topics).float()\n",
    "    softmax = lstm.predict_next_probability(z_one_hot).detach()\n",
    "    phi_t = ssm.phi[t - 1]\n",
    "    phi_xt = phi_t[xt - 1, :]\n",
    "    return torch.mul(softmax, phi_xt)\n",
    "\n",
    "def compute_gamma_normalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    num = compute_gamma_unnormalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm)\n",
    "    denom = torch.sum(num) + 1e-6\n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_gibbs(P, num_topics, num_words, T, lstm_model, ssm_model, x, previous_z_1_T_star):\n",
    "    # Init\n",
    "    Z_matrix = torch.zeros((P, T + 1), dtype=torch.long)\n",
    "    alpha_matrix = torch.zeros((P, T + 1), dtype=torch.float)\n",
    "    ancestor_matrix = torch.ones((P, T + 1), dtype=torch.long)\n",
    "\n",
    "    # t=0\n",
    "    z_0 = torch.randint(1, num_topics + 1, (P,))\n",
    "    alpha_0 = torch.full((P,), 1 / P)\n",
    "    Z_matrix[:, 0] = z_0\n",
    "    alpha_matrix[:, 0] = alpha_0\n",
    "\n",
    "    for t in range(1, T + 1):\n",
    "        a_t_minus_1 = torch.tensor(1)\n",
    "        z_1_t = previous_z_1_T_star[:t]\n",
    "        ancestor_matrix[0, t - 1] = a_t_minus_1\n",
    "        Z_matrix[0, 1:t + 1] = z_1_t\n",
    "\n",
    "        for p in range(2, P + 1):\n",
    "            alpha_t_minus_1_p = alpha_matrix[:, t - 1]\n",
    "            try:\n",
    "                a_t_minus_1_p = torch.multinomial(alpha_t_minus_1_p, 1).item()+1\n",
    "            except:\n",
    "                a_t_minus_1_p = torch.multinomial(alpha_t_minus_1_p / alpha_t_minus_1_p.sum(), 1).item()+1\n",
    "\n",
    "            ancestor_matrix[p - 1, t - 1] = a_t_minus_1_p\n",
    "\n",
    "            if t == 1:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p) - 1, 0]\n",
    "                z_1_t_minus_1_a_t_minus_1_p = torch.tensor([z_1_t_minus_1_a_t_minus_1_p])\n",
    "                gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                                 xt=x[t - 1],\n",
    "                                                 z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                 num_topics=num_topics,\n",
    "                                                 lstm=lstm_model,\n",
    "                                                 ssm=ssm_model)\n",
    "                \n",
    "                try:\n",
    "                    z_t_p = torch.multinomial(gamma_t_p, 1).item()+1\n",
    "                except:\n",
    "                    z_t_p = torch.multinomial(gamma_t_p / gamma_t_p.sum(), 1).item()+1\n",
    "                z_1_t_p = z_t_p\n",
    "            else:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p) - 1, 1:t]\n",
    "                gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                                    xt=x[t - 1],\n",
    "                                                    z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                    num_topics=num_topics,\n",
    "                                                    lstm=lstm_model,\n",
    "                                                    ssm=ssm_model)\n",
    "\n",
    "                try:\n",
    "                    z_t_p = torch.multinomial(gamma_t_p, 1).item()+1\n",
    "                except:\n",
    "                    z_t_p = torch.multinomial(gamma_t_p / gamma_t_p.sum(), 1).item()+1\n",
    "\n",
    "                z_1_t_p = torch.cat([z_1_t_minus_1_a_t_minus_1_p, torch.tensor([z_t_p])])\n",
    "\n",
    "            Z_matrix[p - 1, 1:t + 1] = z_1_t_p\n",
    "\n",
    "        for p in range(1, P + 1):\n",
    "            a_t_minus_1_p = ancestor_matrix[p - 1, t - 1]\n",
    "            if t == 1:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p) - 1, 0]\n",
    "                z_1_t_minus_1_a_t_minus_1_p = torch.tensor([z_1_t_minus_1_a_t_minus_1_p])\n",
    "            else:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p) - 1, 1:(t - 1) + 1]\n",
    "\n",
    "            alpha_t_p = compute_alpha_normalized(t=t,\n",
    "                                                 z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                 num_topics=num_topics,\n",
    "                                                 num_voc=num_words,\n",
    "                                                 lstm=lstm_model,\n",
    "                                                 ssm=ssm_model)\n",
    "            alpha_t_p = alpha_t_p[x[t - 1] - 1]\n",
    "            alpha_matrix[p - 1, t] = alpha_t_p\n",
    "\n",
    "        alpha_matrix[:, t] = alpha_matrix[:, t] / (alpha_matrix[:, t].sum() + 1e-6)\n",
    "\n",
    "    alpha_T = alpha_matrix[:, -1]\n",
    "    alpha_T = alpha_T / (alpha_T.sum() + 1e-6)\n",
    "\n",
    "    try:\n",
    "        r = torch.multinomial(alpha_T, 1).item()+1\n",
    "    except:\n",
    "        r = torch.multinomial(alpha_T / alpha_T.sum(), 1).item()+1\n",
    "\n",
    "    a_T_r = ancestor_matrix[int(r) - 1, -1]\n",
    "    z_1_T = Z_matrix[int(a_T_r) - 1, 1:]\n",
    "\n",
    "    return z_1_T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=64\n",
    "SEQUENCE_LENGTH = 100\n",
    "MODEL_LENGTH = 10\n",
    "NUM_TOPICS = 50\n",
    "NUM_WORDS = 5000\n",
    "N_ITER_EM = 1\n",
    "NUM_PARTICULES = 10\n",
    "\n",
    "lstm_model=LSTM(input_size=NUM_TOPICS, hidden_size=HIDDEN_SIZE, output_size=NUM_TOPICS, model_length=MODEL_LENGTH)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "ssm_model = SSMPytorch(num_words=NUM_WORDS, num_topics=NUM_TOPICS, T=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m         previous_z_1_T_star \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, NUM_TOPICS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, (SEQUENCE_LENGTH,))\n\u001b[0;32m      8\u001b[0m x_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train[i], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m----> 9\u001b[0m z_star \u001b[38;5;241m=\u001b[39m particle_gibbs(NUM_PARTICULES, NUM_TOPICS, NUM_WORDS, SEQUENCE_LENGTH, lstm_model, ssm_model, x_tensor, previous_z_1_T_star)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     12\u001b[0m     z_star_train \u001b[38;5;241m=\u001b[39m z_star\n",
      "Cell \u001b[1;32mIn[63], line 69\u001b[0m, in \u001b[0;36mparticle_gibbs\u001b[1;34m(P, num_topics, num_words, T, lstm_model, ssm_model, x, previous_z_1_T_star)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     z_1_t_minus_1_a_t_minus_1_p \u001b[38;5;241m=\u001b[39m Z_matrix[\u001b[38;5;28mint\u001b[39m(a_t_minus_1_p) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m:(t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 69\u001b[0m alpha_t_p \u001b[38;5;241m=\u001b[39m compute_alpha_normalized(t\u001b[38;5;241m=\u001b[39mt,\n\u001b[0;32m     70\u001b[0m                                      z_1_t_minus_1\u001b[38;5;241m=\u001b[39mz_1_t_minus_1_a_t_minus_1_p,\n\u001b[0;32m     71\u001b[0m                                      num_topics\u001b[38;5;241m=\u001b[39mnum_topics,\n\u001b[0;32m     72\u001b[0m                                      num_voc\u001b[38;5;241m=\u001b[39mnum_words,\n\u001b[0;32m     73\u001b[0m                                      lstm\u001b[38;5;241m=\u001b[39mlstm_model,\n\u001b[0;32m     74\u001b[0m                                      ssm\u001b[38;5;241m=\u001b[39mssm_model)\n\u001b[0;32m     75\u001b[0m alpha_t_p \u001b[38;5;241m=\u001b[39m alpha_t_p[x[t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     76\u001b[0m alpha_matrix[p \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, t] \u001b[38;5;241m=\u001b[39m alpha_t_p\n",
      "Cell \u001b[1;32mIn[59], line 10\u001b[0m, in \u001b[0;36mcompute_alpha_normalized\u001b[1;34m(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_alpha_normalized\u001b[39m(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n\u001b[1;32m---> 10\u001b[0m     num \u001b[38;5;241m=\u001b[39m compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\n\u001b[0;32m     11\u001b[0m     denom \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(num) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num \u001b[38;5;241m/\u001b[39m denom\n",
      "Cell \u001b[1;32mIn[59], line 6\u001b[0m, in \u001b[0;36mcompute_alpha_unnormalized\u001b[1;34m(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\u001b[0m\n\u001b[0;32m      4\u001b[0m softmax \u001b[38;5;241m=\u001b[39m lstm\u001b[38;5;241m.\u001b[39mpredict_next_probability(z_one_hot)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m      5\u001b[0m phi_t \u001b[38;5;241m=\u001b[39m ssm\u001b[38;5;241m.\u001b[39mphi[t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([torch\u001b[38;5;241m.\u001b[39mmatmul(softmax, phi_t[j, :]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_voc)])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "Cell \u001b[1;32mIn[59], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m softmax \u001b[38;5;241m=\u001b[39m lstm\u001b[38;5;241m.\u001b[39mpredict_next_probability(z_one_hot)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m      5\u001b[0m phi_t \u001b[38;5;241m=\u001b[39m ssm\u001b[38;5;241m.\u001b[39mphi[t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([torch\u001b[38;5;241m.\u001b[39mmatmul(softmax, phi_t[j, :]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_voc)])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "z_star_train = []\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    if i == 1:\n",
    "        break\n",
    "    print(i)\n",
    "    for iter in range(N_ITER_EM):\n",
    "        if iter == 0:\n",
    "            previous_z_1_T_star = torch.randint(1, NUM_TOPICS + 1, (SEQUENCE_LENGTH,))\n",
    "    x_tensor = torch.tensor(X_train[i], dtype=torch.long)\n",
    "    z_star = particle_gibbs(NUM_PARTICULES, NUM_TOPICS, NUM_WORDS, SEQUENCE_LENGTH, lstm_model, ssm_model, x_tensor, previous_z_1_T_star)\n",
    "    \n",
    "    if i == 0:\n",
    "        z_star_train = z_star\n",
    "    else:\n",
    "        z_star_train = torch.vstack((z_star_train, z_star))\n",
    "    previous_z_1_T_star = z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Initialisation\n",
    "# P=50\n",
    "# NUM_TOPICS=50\n",
    "# NUM_WORDS=5000\n",
    "# T=100\n",
    "\n",
    "# lstm = lstm_model\n",
    "# ssm = ssm_model\n",
    "\n",
    "# Z_matrix=np.zeros((P, T+1))\n",
    "# alpha_matrix=np.zeros((P,T+1))\n",
    "# ancestor_matrix=np.zeros((P,T+1))\n",
    "\n",
    "\n",
    "# z_1_T_star = np.random.choice(a=range(1,NUM_TOPICS+1), size=T)\n",
    "# x = X_train[0]\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##t=0\n",
    "# z_0 = np.random.choice(a=range(1,NUM_TOPICS+1), size=P)\n",
    "# alpha_0 = np.repeat(1/P, P)\n",
    "# Z_matrix[:,0] = z_0\n",
    "# alpha_matrix[:,0] = alpha_0\n",
    "\n",
    "# #z[k:n]: du k-ème au n-1 ème\n",
    "# for t in range(1,T+1):\n",
    "#     print(t)\n",
    "#     a_t_minus_1 = 1 #ok\n",
    "#     z_1_t = z_1_T_star[:t] #ok\n",
    "#     ancestor_matrix[0,t-1] = a_t_minus_1 #ok\n",
    "#     Z_matrix[0, 1:t+1] = z_1_t #ok\n",
    "\n",
    "#     for p in range(2,P+1):\n",
    "#         alpha_t_minus_1_p=alpha_matrix[:,t-1]\n",
    "#         #a_t_minus_1_p = np.random.choice(a=range(1,P+1), p=alpha_t_minus_1_p, size=1)[0] #ok\n",
    "#         a_t_minus_1_p = np.argmax(alpha_t_minus_1_p)+1\n",
    "#         ancestor_matrix[p-1, t-1] = a_t_minus_1_p #ok\n",
    "#         if t ==1:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "#             gamma_t_p = compute_gamma_normalized(t=t,\n",
    "#                                              xt=x[t-1], \n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics = NUM_TOPICS,\n",
    "#                                              lstm = lstm,\n",
    "#                                              ssm = ssm)\n",
    "#             z_t_p = np.argmax(gamma_t_p)+1\n",
    "#             z_1_t_p = z_t_p\n",
    "#         else:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1] #ok\n",
    "#             gamma_t_p = compute_gamma_normalized(t=t,\n",
    "#                                              xt=x[t-1], \n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics = NUM_TOPICS,\n",
    "#                                              lstm = lstm,\n",
    "#                                              ssm = ssm)\n",
    "#             z_t_p = np.argmax(gamma_t_p)+1\n",
    "#             #z_t_p = np.random.choice(a=range(1, NUM_TOPICS+1), p=gamma_t_p, size=1)[0]\n",
    "#             z_1_t_p = np.append(z_1_t_minus_1_a_t_minus_1_p, z_t_p)\n",
    "        \n",
    "#         Z_matrix[p-1, 1:t+1] = z_1_t_p\n",
    "\n",
    "    \n",
    "#     for p in range(1, P+1):\n",
    "#         a_t_minus_1_p = ancestor_matrix[p-1, t-1]\n",
    "#         if t ==1:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "#         else:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1]\n",
    "        \n",
    "#         alpha_t_p = compute_alpha_normalized(t=t,\n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics=NUM_TOPICS,\n",
    "#                                              num_voc=NUM_WORDS,\n",
    "#                                              lstm=lstm,\n",
    "#                                              ssm=ssm)\n",
    "#         alpha_t_p = alpha_t_p[x[t-1]-1]\n",
    "#         alpha_matrix[p-1,t] = alpha_t_p\n",
    "\n",
    "# alpha_T=alpha_matrix[:,-1]\n",
    "# alpha_T = alpha_T / np.sum(alpha_T+1e-6)\n",
    "# r = np.argmax(alpha_T)+1\n",
    "# a_T_r = ancestor_matrix[int(r)-1, -1]\n",
    "# z_1_T = Z_matrix[int(a_T_r)-1, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
