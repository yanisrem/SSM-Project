{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanis\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.vectorize import *\n",
    "from src.lda import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données et pré-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe 10 000 articles Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pauline Donalda,  (March 5, 1882 – October 22,...</td>\n",
       "      <td>Pauline Donalda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a list of German football transfers in...</td>\n",
       "      <td>List of German football transfers summer 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "1  Pauline Donalda,  (March 5, 1882 – October 22,...   \n",
       "2  This is a list of German football transfers in...   \n",
       "3  Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4  Monique Ganderton (born August 6, 1980) is a C...   \n",
       "\n",
       "                                           Title  \n",
       "0                               Joseph Greenberg  \n",
       "1                                Pauline Donalda  \n",
       "2  List of German football transfers summer 2017  \n",
       "3                                  Lester Hudson  \n",
       "4                              Monique Ganderton  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 10000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\yanis\\\\OneDrive\\\\Documents\\\\ENSAE 3A\\\\Sequential MC\\\\SSM-PROJECT\\\\wiki_data.csv\",\n",
    "                    encoding='utf-8',\n",
    "                    delimiter=\";\")\n",
    "del data[\"Unnamed: 0\"]\n",
    "display(data.head())\n",
    "print(\"Number of articles: {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On tokenize chaque article: \"Joseph Harold Greenberg\" devient [\"Jospeh\", \"Harold\", \"Greenberg\"]\n",
    "2. On filtre les articles: on retire les articles composés de moins de 500 mots\n",
    "3. On vectorise le texte: on associe à chaque mot son indice dans le vocabulaire général.\n",
    "\n",
    "Exemple: si tous les articles peuvent contenir comme mots: [\"pomme\", \"poire\", \"chocolat\", \"eau\", \"banane\"], l'article [\"chocolat\", \"banane\", \"pomme\"] devient [2, 4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "      <td>[Joseph, Harold, Greenberg, (, May, 28, ,, 191...</td>\n",
       "      <td>[68836, 61034, 58682, 1182, 80975, 13183, 1242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "      <td>[Lester, Hudson, III, (, born, August, 7, ,, 1...</td>\n",
       "      <td>[75482, 63781, 64659, 1182, 134097, 26662, 181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "      <td>[Monique, Ganderton, (, born, August, 6, ,, 19...</td>\n",
       "      <td>[84017, 56054, 1182, 134097, 26662, 17386, 124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The white bikini of Ursula Andress (also known...</td>\n",
       "      <td>White bikini of Ursula Andress</td>\n",
       "      <td>[The, white, bikini, of, Ursula, Andress, (, a...</td>\n",
       "      <td>[115808, 188693, 133263, 165140, 120335, 24145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Breakout\" is a single from British act Swing ...</td>\n",
       "      <td>Breakout (Swing Out Sister song)</td>\n",
       "      <td>[``, Breakout, '', is, a, single, from, Britis...</td>\n",
       "      <td>[127812, 32984, 6, 156142, 127813, 178055, 149...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "3   Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4   Monique Ganderton (born August 6, 1980) is a C...   \n",
       "6   The white bikini of Ursula Andress (also known...   \n",
       "15  \"Breakout\" is a single from British act Swing ...   \n",
       "\n",
       "                               Title  \\\n",
       "0                   Joseph Greenberg   \n",
       "3                      Lester Hudson   \n",
       "4                  Monique Ganderton   \n",
       "6     White bikini of Ursula Andress   \n",
       "15  Breakout (Swing Out Sister song)   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "0   [Joseph, Harold, Greenberg, (, May, 28, ,, 191...   \n",
       "3   [Lester, Hudson, III, (, born, August, 7, ,, 1...   \n",
       "4   [Monique, Ganderton, (, born, August, 6, ,, 19...   \n",
       "6   [The, white, bikini, of, Ursula, Andress, (, a...   \n",
       "15  [``, Breakout, '', is, a, single, from, Britis...   \n",
       "\n",
       "                                      vectorized_text  \n",
       "0   [68836, 61034, 58682, 1182, 80975, 13183, 1242...  \n",
       "3   [75482, 63781, 64659, 1182, 134097, 26662, 181...  \n",
       "4   [84017, 56054, 1182, 134097, 26662, 17386, 124...  \n",
       "6   [115808, 188693, 133263, 165140, 120335, 24145...  \n",
       "15  [127812, 32984, 6, 156142, 127813, 178055, 149...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_data=vectorize_data(data=data, min_number_words=500)\n",
    "display(vectorized_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En attendant d'avoir $z^*_{1:T}$, on applique le LDA sur le corpus avec K=50 topics. Chaque mot a alors un poids relativement à chaque topic.\n",
    "\n",
    "Exemple: le mot banane a les poids suivant: {\"topic 1\": 0.3, \"topic 2\": 0.1, \"topic 3\": 0.05,...}. \n",
    "\n",
    "On associe au mot le topic avec le poids le plus élevé. En l'occurence 1. De cette façon, on crée index_topic=$(z_1,...,z_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "      <th>index_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Harold Greenberg (May 28, 1915 – May 7,...</td>\n",
       "      <td>Joseph Greenberg</td>\n",
       "      <td>[Joseph, Harold, Greenberg, (, May, 28, ,, 191...</td>\n",
       "      <td>[68836, 61034, 58682, 1182, 80975, 13183, 1242...</td>\n",
       "      <td>[21, 21, 24, 21, 18, 18, 7, 18, 6, 18, 21, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lester Hudson III (born August 7, 1984) is an ...</td>\n",
       "      <td>Lester Hudson</td>\n",
       "      <td>[Lester, Hudson, III, (, born, August, 7, ,, 1...</td>\n",
       "      <td>[75482, 63781, 64659, 1182, 134097, 26662, 181...</td>\n",
       "      <td>[21, 16, 18, 21, 11, 39, 21, 7, 16, 21, 10, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monique Ganderton (born August 6, 1980) is a C...</td>\n",
       "      <td>Monique Ganderton</td>\n",
       "      <td>[Monique, Ganderton, (, born, August, 6, ,, 19...</td>\n",
       "      <td>[84017, 56054, 1182, 134097, 26662, 17386, 124...</td>\n",
       "      <td>[30, 30, 21, 11, 39, 27, 7, 16, 21, 10, 7, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The white bikini of Ursula Andress (also known...</td>\n",
       "      <td>White bikini of Ursula Andress</td>\n",
       "      <td>[The, white, bikini, of, Ursula, Andress, (, a...</td>\n",
       "      <td>[115808, 188693, 133263, 165140, 120335, 24145...</td>\n",
       "      <td>[16, 7, 11, 34, 13, 13, 21, 19, 3, 15, 1, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Breakout\" is a single from British act Swing ...</td>\n",
       "      <td>Breakout (Swing Out Sister song)</td>\n",
       "      <td>[``, Breakout, '', is, a, single, from, Britis...</td>\n",
       "      <td>[127812, 32984, 6, 156142, 127813, 178055, 149...</td>\n",
       "      <td>[2, 2, 27, 10, 7, 2, 32, 41, 39, 2, 2, 6, 1, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   Joseph Harold Greenberg (May 28, 1915 – May 7,...   \n",
       "3   Lester Hudson III (born August 7, 1984) is an ...   \n",
       "4   Monique Ganderton (born August 6, 1980) is a C...   \n",
       "6   The white bikini of Ursula Andress (also known...   \n",
       "15  \"Breakout\" is a single from British act Swing ...   \n",
       "\n",
       "                               Title  \\\n",
       "0                   Joseph Greenberg   \n",
       "3                      Lester Hudson   \n",
       "4                  Monique Ganderton   \n",
       "6     White bikini of Ursula Andress   \n",
       "15  Breakout (Swing Out Sister song)   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "0   [Joseph, Harold, Greenberg, (, May, 28, ,, 191...   \n",
       "3   [Lester, Hudson, III, (, born, August, 7, ,, 1...   \n",
       "4   [Monique, Ganderton, (, born, August, 6, ,, 19...   \n",
       "6   [The, white, bikini, of, Ursula, Andress, (, a...   \n",
       "15  [``, Breakout, '', is, a, single, from, Britis...   \n",
       "\n",
       "                                      vectorized_text  \\\n",
       "0   [68836, 61034, 58682, 1182, 80975, 13183, 1242...   \n",
       "3   [75482, 63781, 64659, 1182, 134097, 26662, 181...   \n",
       "4   [84017, 56054, 1182, 134097, 26662, 17386, 124...   \n",
       "6   [115808, 188693, 133263, 165140, 120335, 24145...   \n",
       "15  [127812, 32984, 6, 156142, 127813, 178055, 149...   \n",
       "\n",
       "                                          index_topic  \n",
       "0   [21, 21, 24, 21, 18, 18, 7, 18, 6, 18, 21, 7, ...  \n",
       "3   [21, 16, 18, 21, 11, 39, 21, 7, 16, 21, 10, 15...  \n",
       "4   [30, 30, 21, 11, 39, 27, 7, 16, 21, 10, 7, 1, ...  \n",
       "6   [16, 7, 11, 34, 13, 13, 21, 19, 3, 15, 1, 34, ...  \n",
       "15  [2, 2, 27, 10, 7, 2, 32, 41, 39, 2, 2, 6, 1, 4...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_TOPICS=50\n",
    "lda=LDA(num_topics=NUM_TOPICS, random_state=123)\n",
    "vectorized_data_lda=lda.run(vectorized_data)\n",
    "display(vectorized_data_lda.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique un padding, i.e une longueur maximale sur les articles ($x$) et les indices de topics ($z$). De façon arbitraire, on fixe le padding à 200.\n",
    "* Si l'article/la liste de topics est composé de plus de 1000 caractères alors, on supprime les derniers\n",
    "* Si l'article/la liste de topics est composé de moins de 1000 caractères alors, on ajoute des 0 à la fin\n",
    "Cette opération est faite car la taille d'inpput de LSTM est unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=200\n",
    "\n",
    "def apply_padding(sequence, max_length=SEQUENCE_LENGTH):\n",
    "    padded_sequence = sequence[:max_length] + [0] * max(0, max_length - len(sequence))\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_lda_padding=vectorized_data_lda.copy()\n",
    "vectorized_data_lda_padding['tokenized_text'] = vectorized_data_lda_padding['tokenized_text'].apply(apply_padding)\n",
    "vectorized_data_lda_padding['vectorized_text'] = vectorized_data_lda_padding['vectorized_text'].apply(apply_padding)\n",
    "vectorized_data_lda_padding['index_topic'] = vectorized_data_lda_padding['index_topic'].apply(apply_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un one hot encoding des listes d'indices de topics. Chaque liste $z$, de longueur SEQUENCE_LENGTH=200, est convertie en une matrice de dimensions SEQUENCE_LENGTH*NUM_TOPICS = $200 \\times 50$. L'élement $(i,j)$ de cette matrice vaut 1 si le i-ème topic de la séquence correspond au j-ème topic dans l'ensemble des topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(topic_list, vocab_size):\n",
    "    return to_categorical(topic_list, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_z=vectorized_data_lda_padding['index_topic'].values\n",
    "array_z_one_hot_encoded = np.array([one_hot_encode_list(lst, NUM_TOPICS) for lst in array_z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array_z_one_hot_encoded correspond à l'échantillon de topics $\\mathcal{D}_z$ one-hot encodés. Chaque liste de topics $z=(z_1,...,z_T)$ de l'échantillon est une matrice de dimensions $200 \\times 50$. Lorsque l'on aura implémenté le gibbs sampler, cet échantillon n'existera plus: en réalité, les $z$ ne sont pas observés. On remplacera l'échantillon par $z^*_{1:T}$.\n",
    "\n",
    "On prend un $z$ de l'échantillon en faisant comme si c'était un $z^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 50)\n"
     ]
    }
   ],
   "source": [
    "z_one_hot=array_z_one_hot_encoded[0]\n",
    "print(z_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = z_one_hot[:-1,:]  # Toutes les lignes sauf la dernière\n",
    "y = z_one_hot[1:,:]   # Toutes les lignes sauf la première\n",
    "\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, labels):\n",
    "        self.topics=topics\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.topics[index,:], self.labels[index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size #dimension d'entrée (NUM_TOPICS)\n",
    "        self.hidden_size = hidden_size #nombre de neurones de la couche cachée\n",
    "        self.output_size = output_size #dimension d'outputs (NUM_TOPICS)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        output=self.fc(output)\n",
    "        probabilities = self.softmax(output)\n",
    "        return probabilities, state\n",
    "    \n",
    "    def init_state(self):\n",
    "        return (torch.zeros(1, self.hidden_size),\n",
    "                torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0=z_0: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "y_0=z_1: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "dataset=Dataset(topics=X_tensor, labels=y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "print(\"x_0=z_0: {}\".format(dataset.__getitem__(0)[0]))\n",
    "print(\"y_0=z_1: {}\".format(dataset.__getitem__(0)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=64\n",
    "model=LSTM(input_size=NUM_TOPICS, hidden_size=HIDDEN_SIZE, output_size=NUM_TOPICS)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 1, 'loss': 3.9132745265960693}\n",
      "{'t': 2, 'loss': 3.910295248031616}\n",
      "{'t': 3, 'loss': 3.913569450378418}\n",
      "{'t': 4, 'loss': 3.9122750759124756}\n",
      "{'t': 5, 'loss': 3.91237211227417}\n",
      "{'t': 6, 'loss': 3.9097931385040283}\n",
      "{'t': 7, 'loss': 3.9124233722686768}\n",
      "{'t': 8, 'loss': 3.913525342941284}\n",
      "{'t': 9, 'loss': 3.912706136703491}\n",
      "{'t': 10, 'loss': 3.9136691093444824}\n",
      "{'t': 11, 'loss': 3.909810781478882}\n",
      "{'t': 12, 'loss': 3.9135122299194336}\n",
      "{'t': 13, 'loss': 3.9137938022613525}\n",
      "{'t': 14, 'loss': 3.9132325649261475}\n",
      "{'t': 15, 'loss': 3.913527011871338}\n",
      "{'t': 16, 'loss': 3.9097132682800293}\n",
      "{'t': 17, 'loss': 3.914539098739624}\n",
      "{'t': 18, 'loss': 3.9100239276885986}\n",
      "{'t': 19, 'loss': 3.9137091636657715}\n",
      "{'t': 20, 'loss': 3.911336660385132}\n",
      "{'t': 21, 'loss': 3.9121599197387695}\n",
      "{'t': 22, 'loss': 3.911240577697754}\n",
      "{'t': 23, 'loss': 3.909759521484375}\n",
      "{'t': 24, 'loss': 3.9141972064971924}\n",
      "{'t': 25, 'loss': 3.913372039794922}\n",
      "{'t': 26, 'loss': 3.913803815841675}\n",
      "{'t': 27, 'loss': 3.909743070602417}\n",
      "{'t': 28, 'loss': 3.913881301879883}\n",
      "{'t': 29, 'loss': 3.909903049468994}\n",
      "{'t': 30, 'loss': 3.9133799076080322}\n",
      "{'t': 31, 'loss': 3.90972638130188}\n",
      "{'t': 32, 'loss': 3.910097122192383}\n",
      "{'t': 33, 'loss': 3.913438081741333}\n",
      "{'t': 34, 'loss': 3.9135565757751465}\n",
      "{'t': 35, 'loss': 3.9098386764526367}\n",
      "{'t': 36, 'loss': 3.9139184951782227}\n",
      "{'t': 37, 'loss': 3.9097678661346436}\n",
      "{'t': 38, 'loss': 3.9096975326538086}\n",
      "{'t': 39, 'loss': 3.9133987426757812}\n",
      "{'t': 40, 'loss': 3.9122233390808105}\n",
      "{'t': 41, 'loss': 3.9139389991760254}\n",
      "{'t': 42, 'loss': 3.9138317108154297}\n",
      "{'t': 43, 'loss': 3.91153883934021}\n",
      "{'t': 44, 'loss': 3.913860321044922}\n",
      "{'t': 45, 'loss': 3.913660764694214}\n",
      "{'t': 46, 'loss': 3.913588523864746}\n",
      "{'t': 47, 'loss': 3.910468578338623}\n",
      "{'t': 48, 'loss': 3.9137089252471924}\n",
      "{'t': 49, 'loss': 3.9107837677001953}\n",
      "{'t': 50, 'loss': 3.912034511566162}\n",
      "{'t': 51, 'loss': 3.9124038219451904}\n",
      "{'t': 52, 'loss': 3.9124293327331543}\n",
      "{'t': 53, 'loss': 3.909825325012207}\n",
      "{'t': 54, 'loss': 3.9124276638031006}\n",
      "{'t': 55, 'loss': 3.9118905067443848}\n",
      "{'t': 56, 'loss': 3.911012887954712}\n",
      "{'t': 57, 'loss': 3.9138622283935547}\n",
      "{'t': 58, 'loss': 3.9122917652130127}\n",
      "{'t': 59, 'loss': 3.9135653972625732}\n",
      "{'t': 60, 'loss': 3.9098644256591797}\n",
      "{'t': 61, 'loss': 3.9121434688568115}\n",
      "{'t': 62, 'loss': 3.9137539863586426}\n",
      "{'t': 63, 'loss': 3.913644552230835}\n",
      "{'t': 64, 'loss': 3.9142372608184814}\n",
      "{'t': 65, 'loss': 3.9123706817626953}\n",
      "{'t': 66, 'loss': 3.9135568141937256}\n",
      "{'t': 67, 'loss': 3.9115185737609863}\n",
      "{'t': 68, 'loss': 3.9134790897369385}\n",
      "{'t': 69, 'loss': 3.9121761322021484}\n",
      "{'t': 70, 'loss': 3.9134085178375244}\n",
      "{'t': 71, 'loss': 3.9135775566101074}\n",
      "{'t': 72, 'loss': 3.913759708404541}\n",
      "{'t': 73, 'loss': 3.913606643676758}\n",
      "{'t': 74, 'loss': 3.9097089767456055}\n",
      "{'t': 75, 'loss': 3.91396427154541}\n",
      "{'t': 76, 'loss': 3.9096527099609375}\n",
      "{'t': 77, 'loss': 3.9117653369903564}\n",
      "{'t': 78, 'loss': 3.913668155670166}\n",
      "{'t': 79, 'loss': 3.910050630569458}\n",
      "{'t': 80, 'loss': 3.9133996963500977}\n",
      "{'t': 81, 'loss': 3.9096760749816895}\n",
      "{'t': 82, 'loss': 3.9124484062194824}\n",
      "{'t': 83, 'loss': 3.914029121398926}\n",
      "{'t': 84, 'loss': 3.913829803466797}\n",
      "{'t': 85, 'loss': 3.9134154319763184}\n",
      "{'t': 86, 'loss': 3.9095301628112793}\n",
      "{'t': 87, 'loss': 3.9133729934692383}\n",
      "{'t': 88, 'loss': 3.911918878555298}\n",
      "{'t': 89, 'loss': 3.913620710372925}\n",
      "{'t': 90, 'loss': 3.9136805534362793}\n",
      "{'t': 91, 'loss': 3.913533926010132}\n",
      "{'t': 92, 'loss': 3.91392183303833}\n",
      "{'t': 93, 'loss': 3.9143307209014893}\n",
      "{'t': 94, 'loss': 3.9114811420440674}\n",
      "{'t': 95, 'loss': 3.9141392707824707}\n",
      "{'t': 96, 'loss': 3.9134671688079834}\n",
      "{'t': 97, 'loss': 3.913844347000122}\n",
      "{'t': 98, 'loss': 3.913752317428589}\n",
      "{'t': 99, 'loss': 3.914848566055298}\n",
      "{'t': 100, 'loss': 3.914825916290283}\n",
      "{'t': 101, 'loss': 3.909856081008911}\n",
      "{'t': 102, 'loss': 3.911637783050537}\n",
      "{'t': 103, 'loss': 3.91426944732666}\n",
      "{'t': 104, 'loss': 3.9114036560058594}\n",
      "{'t': 105, 'loss': 3.9135351181030273}\n",
      "{'t': 106, 'loss': 3.909795045852661}\n",
      "{'t': 107, 'loss': 3.9145891666412354}\n",
      "{'t': 108, 'loss': 3.9135353565216064}\n",
      "{'t': 109, 'loss': 3.914384365081787}\n",
      "{'t': 110, 'loss': 3.9115750789642334}\n",
      "{'t': 111, 'loss': 3.910153388977051}\n",
      "{'t': 112, 'loss': 3.9120213985443115}\n",
      "{'t': 113, 'loss': 3.9143712520599365}\n",
      "{'t': 114, 'loss': 3.9135894775390625}\n",
      "{'t': 115, 'loss': 3.9098095893859863}\n",
      "{'t': 116, 'loss': 3.9146623611450195}\n",
      "{'t': 117, 'loss': 3.911459445953369}\n",
      "{'t': 118, 'loss': 3.9102706909179688}\n",
      "{'t': 119, 'loss': 3.909900426864624}\n",
      "{'t': 120, 'loss': 3.911790370941162}\n",
      "{'t': 121, 'loss': 3.912130832672119}\n",
      "{'t': 122, 'loss': 3.913640022277832}\n",
      "{'t': 123, 'loss': 3.913550853729248}\n",
      "{'t': 124, 'loss': 3.91389799118042}\n",
      "{'t': 125, 'loss': 3.9120988845825195}\n",
      "{'t': 126, 'loss': 3.9135560989379883}\n",
      "{'t': 127, 'loss': 3.9135916233062744}\n",
      "{'t': 128, 'loss': 3.9101150035858154}\n",
      "{'t': 129, 'loss': 3.9116218090057373}\n",
      "{'t': 130, 'loss': 3.909743309020996}\n",
      "{'t': 131, 'loss': 3.9099795818328857}\n",
      "{'t': 132, 'loss': 3.9147233963012695}\n",
      "{'t': 133, 'loss': 3.9101526737213135}\n",
      "{'t': 134, 'loss': 3.9137113094329834}\n",
      "{'t': 135, 'loss': 3.91233491897583}\n",
      "{'t': 136, 'loss': 3.9140124320983887}\n",
      "{'t': 137, 'loss': 3.9139914512634277}\n",
      "{'t': 138, 'loss': 3.90993332862854}\n",
      "{'t': 139, 'loss': 3.9101927280426025}\n",
      "{'t': 140, 'loss': 3.910144805908203}\n",
      "{'t': 141, 'loss': 3.913504123687744}\n",
      "{'t': 142, 'loss': 3.9135472774505615}\n",
      "{'t': 143, 'loss': 3.9123661518096924}\n",
      "{'t': 144, 'loss': 3.914379358291626}\n",
      "{'t': 145, 'loss': 3.9141035079956055}\n",
      "{'t': 146, 'loss': 3.9098520278930664}\n",
      "{'t': 147, 'loss': 3.9120428562164307}\n",
      "{'t': 148, 'loss': 3.909846305847168}\n",
      "{'t': 149, 'loss': 3.9098711013793945}\n",
      "{'t': 150, 'loss': 3.9115116596221924}\n",
      "{'t': 151, 'loss': 3.913485288619995}\n",
      "{'t': 152, 'loss': 3.9141268730163574}\n",
      "{'t': 153, 'loss': 3.9139113426208496}\n",
      "{'t': 154, 'loss': 3.909656286239624}\n",
      "{'t': 155, 'loss': 3.914743423461914}\n",
      "{'t': 156, 'loss': 3.9148569107055664}\n",
      "{'t': 157, 'loss': 3.9142515659332275}\n",
      "{'t': 158, 'loss': 3.912811279296875}\n",
      "{'t': 159, 'loss': 3.9134318828582764}\n",
      "{'t': 160, 'loss': 3.914149045944214}\n",
      "{'t': 161, 'loss': 3.911339521408081}\n",
      "{'t': 162, 'loss': 3.9143056869506836}\n",
      "{'t': 163, 'loss': 3.910175085067749}\n",
      "{'t': 164, 'loss': 3.912057638168335}\n",
      "{'t': 165, 'loss': 3.9097156524658203}\n",
      "{'t': 166, 'loss': 3.91344952583313}\n",
      "{'t': 167, 'loss': 3.9138739109039307}\n",
      "{'t': 168, 'loss': 3.91375732421875}\n",
      "{'t': 169, 'loss': 3.9147894382476807}\n",
      "{'t': 170, 'loss': 3.9098849296569824}\n",
      "{'t': 171, 'loss': 3.9119083881378174}\n",
      "{'t': 172, 'loss': 3.9147729873657227}\n",
      "{'t': 173, 'loss': 3.9098832607269287}\n",
      "{'t': 174, 'loss': 3.909933567047119}\n",
      "{'t': 175, 'loss': 3.909956932067871}\n",
      "{'t': 176, 'loss': 3.911742687225342}\n",
      "{'t': 177, 'loss': 3.914334297180176}\n",
      "{'t': 178, 'loss': 3.908277988433838}\n",
      "{'t': 179, 'loss': 3.9137842655181885}\n",
      "{'t': 180, 'loss': 3.914241313934326}\n",
      "{'t': 181, 'loss': 3.9133081436157227}\n",
      "{'t': 182, 'loss': 3.909726858139038}\n",
      "{'t': 183, 'loss': 3.9097721576690674}\n",
      "{'t': 184, 'loss': 3.9099605083465576}\n",
      "{'t': 185, 'loss': 3.913804054260254}\n",
      "{'t': 186, 'loss': 3.9117164611816406}\n",
      "{'t': 187, 'loss': 3.914391040802002}\n",
      "{'t': 188, 'loss': 3.9138665199279785}\n",
      "{'t': 189, 'loss': 3.914210557937622}\n",
      "{'t': 190, 'loss': 3.910238742828369}\n",
      "{'t': 191, 'loss': 3.9133548736572266}\n",
      "{'t': 192, 'loss': 3.9135780334472656}\n",
      "{'t': 193, 'loss': 3.9098212718963623}\n",
      "{'t': 194, 'loss': 3.9097084999084473}\n",
      "{'t': 195, 'loss': 3.9119696617126465}\n",
      "{'t': 196, 'loss': 3.909675121307373}\n",
      "{'t': 197, 'loss': 3.9144983291625977}\n",
      "{'t': 198, 'loss': 3.9136650562286377}\n",
      "{'t': 199, 'loss': 3.913647413253784}\n"
     ]
    }
   ],
   "source": [
    "state_h, state_c = model.init_state()\n",
    "for t, (z_t, z_t_next) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    softmax , (state_h, state_c) = model(z_t, (state_h, state_c)) #softmax= p(z_{t+1}|z_t)\n",
    "    loss = criterion(softmax, z_t_next)\n",
    "\n",
    "    index_z_t_next_pred = torch.multinomial(input=softmax[0], num_samples=1, replacement=True)\n",
    "    z_t_next_pred= torch.eye(len(softmax[0]))[index_z_t_next_pred] #one hot encoding du topic prédit\n",
    "    state_h = state_h.detach()\n",
    "    state_c = state_c.detach()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print({ 't': t+1,'loss': loss.item() })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
