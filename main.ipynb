{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données et pré-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 5000\n",
    "max_review_length = 100\n",
    "INDEX_FROM = 3\n",
    "\n",
    "# --- Import the IMDB data and only consider the ``top_words``` most used words\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "np.load.__defaults__=(None, False, True, 'ASCII')\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train[0]): 100\n",
      "len(X_train[1]): 100\n",
      "X_train[0]: [   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      "    2   16]\n"
     ]
    }
   ],
   "source": [
    "# --- truncate and pad input sequences\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post', truncating='post', value=0)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post', truncating='post', value=0)\n",
    "\n",
    "print(\"len(X_train[0]):\", len(X_train[0]))\n",
    "print(\"len(X_train[1]):\", len(X_train[1]))\n",
    "print(\"X_train[0]:\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, model_length):\n",
    "        self.topics=topics\n",
    "        self.model_length=model_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.topics)-self.model_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_sequence=torch.tensor(self.topics[index:index+self.model_length, :])\n",
    "        target_sequence=torch.tensor(self.topics[index+1:index+self.model_length+1, :])\n",
    "\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size #dimension d'entrée (NUM_TOPICS)\n",
    "        self.hidden_size = hidden_size #nombre de neurones de la couche cachée\n",
    "        self.output_size = output_size #dimension d'outputs (NUM_TOPICS)\n",
    "        self.model_length=model_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        # self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        output=self.fc(output)\n",
    "        probabilities = F.softmax(output[:, -1, :], dim=1)\n",
    "        return probabilities, state\n",
    "    \n",
    "    def init_state(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size), #(NUM_LAYERS, BATCH SIZE, NUM_NEURONES)\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n",
    "    def train_model(self, dataset, optimizer, criterion):\n",
    "        state_h, state_c = self.init_state()\n",
    "        self.train()\n",
    "        for t, (x, y) in enumerate(dataset):\n",
    "            optimizer.zero_grad()\n",
    "            softmax , (state_h, state_c) = self(x, (state_h, state_c)) #softmax= p(z_{t+1}|z_1:t)\n",
    "            loss = criterion(softmax, y[:, -1, :])\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def predict_next_probability(self, input_sequence):\n",
    "        state_h, state_c = self.init_state()\n",
    "\n",
    "        # Forward pass jusqu'à t-1\n",
    "        for t in range(len(input_sequence)):\n",
    "            input_t = input_sequence[t].unsqueeze(0).unsqueeze(0)\n",
    "            _, (state_h, state_c) = self(input_t, (state_h, state_c))\n",
    "\n",
    "        # Obtenez les probabilités pour x_t\n",
    "        input_t = input_sequence[-1].unsqueeze(0).unsqueeze(0)\n",
    "        probabilities, _ = self(input_t, (state_h, state_c))\n",
    "\n",
    "        return probabilities\n",
    "    \n",
    "    def sample_next_z(self, input_sequence):\n",
    "        proba = self.predict_next_probability(input_sequence)\n",
    "        return torch.multinomial(proba, 1).item()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSM:\n",
    "    def __init__(self, num_words, num_topics, T):\n",
    "        self.num_words = num_words\n",
    "        self.num_topics = num_topics\n",
    "        self.T = T\n",
    "        self.phi = np.random.randn(T, num_words, num_topics) * 0.01\n",
    "        self.phi = np.exp(self.phi - np.max(self.phi, axis=1, keepdims=True))\n",
    "        self.phi = self.phi / np.sum(self.phi, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_MLE_SSM(self, ech_x, ech_z):\n",
    "        def compute_MLE_SSM_time_t(t, ech_x, ech_z, num_words, num_topics):\n",
    "            ech_x_t = ech_x[:,t]\n",
    "            ech_z_t = ech_z[:,t]\n",
    "            proba_matrix = np.zeros((num_words, num_topics))\n",
    "            np.add.at(proba_matrix, (ech_x_t - 1, ech_z_t - 1), 1)\n",
    "            row_sums = proba_matrix.sum(axis=0, keepdims=True)\n",
    "            proba_matrix_normalized = proba_matrix / (row_sums + 1e-6)\n",
    "            self.phi[t] = proba_matrix_normalized\n",
    "\n",
    "        for t in range(self.T):\n",
    "            compute_MLE_SSM_time_t(t, ech_x, ech_z, self.num_words, self.num_topics)\n",
    "    \n",
    "    def predict_proba(self, t, z_t):\n",
    "        return self.phi[t-1][:,z_t-1]\n",
    "    \n",
    "    def sample_xt(self, t, z_t):\n",
    "        proba = self.predict_proba(t, z_t)\n",
    "        sampled_xt = np.random.choice(len(proba), p=proba)\n",
    "        return sampled_xt\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    z_1_t_minus_1=z_1_t_minus_1-1\n",
    "    z_one_hot = to_categorical(z_1_t_minus_1, num_classes=num_topics)\n",
    "    z_one_hot_tensor = torch.FloatTensor(z_one_hot)\n",
    "    softmax = lstm.predict_next_probability(z_one_hot_tensor).detach().numpy()[0]\n",
    "    phi_t = ssm.phi[t-1]\n",
    "    alpha = np.array([np.dot(softmax, phi_t[j,:]) for j in range(num_voc)])\n",
    "    return alpha\n",
    "\n",
    "def compute_alpha_normalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    num = compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\n",
    "    denom = np.sum(num) + 1e-6\n",
    "    return num/denom\n",
    "\n",
    "def compute_gamma_unnormalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    z_1_t_minus_1=z_1_t_minus_1-1\n",
    "    z_one_hot = to_categorical(z_1_t_minus_1, num_classes=num_topics)\n",
    "    z_one_hot_tensor = torch.FloatTensor(z_one_hot)\n",
    "    softmax = lstm.predict_next_probability(z_one_hot_tensor).detach().numpy()[0]\n",
    "    phi_t = ssm.phi[t-1]\n",
    "    phi_xt = phi_t[xt-1, :]\n",
    "    return np.multiply(softmax, phi_xt)\n",
    "\n",
    "def compute_gamma_normalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    num = compute_gamma_unnormalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm)\n",
    "    denom = np.sum(num) + 1e-6\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_gibbs(P, num_topics, num_words, T, lstm_model, ssm_model, x, previous_z_1_T_star):\n",
    "    ##Init\n",
    "    Z_matrix=np.zeros((P, T+1))\n",
    "    alpha_matrix=np.zeros((P,T+1))\n",
    "    ancestor_matrix=np.ones((P,T+1))\n",
    "    ##t=0\n",
    "    z_0 = np.random.choice(a=range(1,num_topics+1), size=P)\n",
    "    alpha_0 = np.repeat(1/P, P)\n",
    "    Z_matrix[:,0] = z_0\n",
    "    alpha_matrix[:,0] = alpha_0\n",
    "\n",
    "    #z[k:n]: du k-ème au n-1 ème\n",
    "    for t in range(1,T+1):\n",
    "        a_t_minus_1 = 1 #ok\n",
    "        z_1_t = previous_z_1_T_star[:t] #ok\n",
    "        ancestor_matrix[0,t-1] = a_t_minus_1 #ok\n",
    "        Z_matrix[0, 1:t+1] = z_1_t #ok\n",
    "\n",
    "        for p in range(2,P+1):\n",
    "            alpha_t_minus_1_p=alpha_matrix[:,t-1]\n",
    "            try:\n",
    "                a_t_minus_1_p = np.random.choice(a=range(1,P+1), p=alpha_t_minus_1_p, size=1)[0] #ok\n",
    "            except:\n",
    "                a_t_minus_1_p = np.random.choice(a=range(1,P+1), p=alpha_t_minus_1_p/np.sum(alpha_t_minus_1_p), size=1)[0] #ok\n",
    "            #a_t_minus_1_p = np.argmax(alpha_t_minus_1_p)+1\n",
    "            ancestor_matrix[p-1, t-1] = a_t_minus_1_p #ok\n",
    "            if t ==1:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "                z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "                gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                                xt=x[t-1], \n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics = num_topics,\n",
    "                                                lstm = lstm_model,\n",
    "                                                ssm = ssm_model)\n",
    "                #z_t_p = np.argmax(gamma_t_p)+1\n",
    "                try:\n",
    "                    z_t_p = np.random.choice(a = range(1, num_topics+1), p = gamma_t_p, size = 1)[0]\n",
    "                except:\n",
    "                    z_t_p = np.random.choice(a = range(1, num_topics+1), p = gamma_t_p/np.sum(gamma_t_p), size = 1)[0]\n",
    "                z_1_t_p = z_t_p\n",
    "            else:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1] #ok\n",
    "                gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                                xt=x[t-1], \n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics = num_topics,\n",
    "                                                lstm = lstm_model,\n",
    "                                                ssm = ssm_model)\n",
    "                #z_t_p = np.argmax(gamma_t_p)+1\n",
    "                try: \n",
    "                    z_t_p = np.random.choice(a=range(1, num_topics+1), p=gamma_t_p, size=1)[0]\n",
    "                except:\n",
    "                    z_t_p = np.random.choice(a=range(1, num_topics+1), p=gamma_t_p/np.sum(gamma_t_p), size=1)[0]\n",
    "                z_1_t_p = np.append(z_1_t_minus_1_a_t_minus_1_p, z_t_p)\n",
    "            \n",
    "            Z_matrix[p-1, 1:t+1] = z_1_t_p\n",
    "\n",
    "        \n",
    "        for p in range(1, P+1):\n",
    "            a_t_minus_1_p = ancestor_matrix[p-1, t-1]\n",
    "            if t ==1:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "                z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "            else:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1]\n",
    "            \n",
    "            alpha_t_p = compute_alpha_normalized(t=t,\n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics=num_topics,\n",
    "                                                num_voc=num_words,\n",
    "                                                lstm=lstm_model,\n",
    "                                                ssm=ssm_model)\n",
    "            alpha_t_p = alpha_t_p[x[t-1]-1]\n",
    "            alpha_matrix[p-1,t] = alpha_t_p\n",
    "        alpha_matrix[:, t] = (alpha_matrix[:, t]) / np.sum((alpha_matrix[:, t]+1e-6))\n",
    "    alpha_T = alpha_matrix[:,-1]\n",
    "    alpha_T = (alpha_T) / np.sum(alpha_T+ 1e-6)\n",
    "    try: \n",
    "        r = np.random.choice(a = range(1, P+1), p = alpha_T, size=1)[0]\n",
    "    except:\n",
    "        r = np.random.choice(a = range(1, P+1), p = alpha_T/np.sum(alpha_T), size=1)[0]\n",
    "    # r = np.argmax(alpha_T)+1\n",
    "    a_T_r = ancestor_matrix[int(r)-1, -1]\n",
    "    z_1_T = Z_matrix[int(a_T_r)-1, 1:]\n",
    "    return z_1_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=64\n",
    "SEQUENCE_LENGTH = 100\n",
    "MODEL_LENGTH = 10\n",
    "NUM_TOPICS = 50\n",
    "NUM_WORDS = 5000\n",
    "N_ITER_EM = 1\n",
    "NUM_PARTICULES = 10\n",
    "\n",
    "lstm_model=LSTM(input_size=NUM_TOPICS, hidden_size=HIDDEN_SIZE, output_size=NUM_TOPICS, model_length=MODEL_LENGTH)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "ssm_model = SSM(num_words=NUM_WORDS, num_topics=NUM_TOPICS, T=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "CPU times: total: 3min 49s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "z_star_train = np.array([])\n",
    "for i in range(X_train.shape[0]):\n",
    "    if i == 1:\n",
    "        break\n",
    "    print(i)\n",
    "    for iter in range(N_ITER_EM):\n",
    "        if iter == 0:\n",
    "            previous_z_1_T_star = np.random.choice(a=range(1,NUM_TOPICS+1), size=SEQUENCE_LENGTH)\n",
    "        z_star = particle_gibbs(NUM_PARTICULES, NUM_TOPICS, NUM_WORDS, SEQUENCE_LENGTH, lstm_model, ssm_model, X_train[i], previous_z_1_T_star)\n",
    "        if i == 0:\n",
    "            z_star_train = np.append(z_star_train, z_star)\n",
    "        else:\n",
    "            z_star_train = np.vstack((z_star_train, z_star))\n",
    "        previous_z_1_T_star = z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_model.compute_MLE_SSM(X_train, z_star_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_star_one_hot_train = to_categorical(z_star_train-1, num_classes=NUM_TOPICS)\n",
    "dataset=Dataset(topics=z_star_one_hot_train, model_length=MODEL_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "lstm_model.train_model(dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "{'loss': 3.914372205734253}\n",
      "{'loss': 3.909734010696411}\n",
      "{'loss': 3.9126381874084473}\n",
      "{'loss': 3.91269588470459}\n",
      "{'loss': 3.90933895111084}\n",
      "{'loss': 3.9110610485076904}\n",
      "{'loss': 3.9138944149017334}\n",
      "{'loss': 3.913745164871216}\n",
      "{'loss': 3.912118434906006}\n",
      "{'loss': 3.910916805267334}\n",
      "{'loss': 3.9138753414154053}\n",
      "{'loss': 3.9104983806610107}\n",
      "{'loss': 3.9115207195281982}\n",
      "{'loss': 3.91195011138916}\n",
      "{'loss': 3.9108874797821045}\n",
      "{'loss': 3.9134819507598877}\n",
      "{'loss': 3.912675619125366}\n",
      "{'loss': 3.9135377407073975}\n",
      "{'loss': 3.913383960723877}\n",
      "{'loss': 3.914472818374634}\n",
      "{'loss': 3.9092602729797363}\n",
      "{'loss': 3.9137513637542725}\n",
      "{'loss': 3.9116079807281494}\n",
      "{'loss': 3.9128592014312744}\n",
      "{'loss': 3.913630962371826}\n",
      "{'loss': 3.911527395248413}\n",
      "{'loss': 3.9123942852020264}\n",
      "{'loss': 3.911792278289795}\n",
      "{'loss': 3.91120982170105}\n",
      "{'loss': 3.9110963344573975}\n",
      "{'loss': 3.913402795791626}\n",
      "{'loss': 3.913447618484497}\n",
      "{'loss': 3.911445379257202}\n",
      "{'loss': 3.9135000705718994}\n",
      "{'loss': 3.914548397064209}\n",
      "{'loss': 3.9133827686309814}\n",
      "{'loss': 3.9129042625427246}\n",
      "{'loss': 3.9122960567474365}\n",
      "{'loss': 3.9123034477233887}\n",
      "{'loss': 3.9139957427978516}\n",
      "{'loss': 3.9095592498779297}\n",
      "{'loss': 3.9100661277770996}\n",
      "{'loss': 3.913881778717041}\n",
      "{'loss': 3.9141347408294678}\n",
      "{'loss': 3.9142565727233887}\n",
      "{'loss': 3.9137673377990723}\n",
      "{'loss': 3.9102110862731934}\n",
      "{'loss': 3.9125773906707764}\n",
      "{'loss': 3.91198992729187}\n",
      "{'loss': 3.9130971431732178}\n",
      "{'loss': 3.9093000888824463}\n",
      "{'loss': 3.9114091396331787}\n",
      "{'loss': 3.912569761276245}\n",
      "{'loss': 3.91051983833313}\n",
      "{'loss': 3.911515951156616}\n",
      "{'loss': 3.910341501235962}\n",
      "{'loss': 3.9131624698638916}\n",
      "{'loss': 3.9093661308288574}\n",
      "{'loss': 3.9110591411590576}\n",
      "{'loss': 3.9124443531036377}\n",
      "{'loss': 3.9132564067840576}\n",
      "{'loss': 3.909304141998291}\n",
      "{'loss': 3.9114391803741455}\n",
      "{'loss': 3.9125640392303467}\n",
      "{'loss': 3.911344051361084}\n",
      "{'loss': 3.9147531986236572}\n",
      "{'loss': 3.9118828773498535}\n",
      "{'loss': 3.911966323852539}\n",
      "{'loss': 3.912367343902588}\n",
      "{'loss': 3.9130570888519287}\n",
      "{'loss': 3.914799451828003}\n",
      "{'loss': 3.9110708236694336}\n",
      "{'loss': 3.913533926010132}\n",
      "{'loss': 3.910442590713501}\n",
      "{'loss': 3.9106130599975586}\n",
      "{'loss': 3.911855459213257}\n",
      "{'loss': 3.9095778465270996}\n",
      "{'loss': 3.9099793434143066}\n",
      "{'loss': 3.910869836807251}\n",
      "{'loss': 3.9124903678894043}\n",
      "{'loss': 3.913759469985962}\n",
      "{'loss': 3.913586378097534}\n",
      "{'loss': 3.9121947288513184}\n",
      "{'loss': 3.9135892391204834}\n",
      "{'loss': 3.913872718811035}\n",
      "{'loss': 3.909055709838867}\n",
      "{'loss': 3.9134089946746826}\n",
      "{'loss': 3.9092190265655518}\n",
      "{'loss': 3.912759304046631}\n",
      "{'loss': 3.91068172454834}\n"
     ]
    }
   ],
   "source": [
    "previous_z_1_T_star = np.random.choice(a=range(1,NUM_TOPICS+1), size=SEQUENCE_LENGTH)\n",
    "for iter in range(N_ITER_EM):\n",
    "    z_star = particle_gibbs(NUM_PARTICULES, NUM_TOPICS, NUM_WORDS, SEQUENCE_LENGTH, lstm_model, ssm_model, x, previous_z_1_T_star)\n",
    "    z_star_one_hot = to_categorical(z_star-1, num_classes=NUM_TOPICS)\n",
    "    dataset=Dataset(topics=z_star_one_hot, model_length=MODEL_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=1)\n",
    "    lstm_model.train_model(dataloader, optimizer, criterion)\n",
    "    ech_x = np.expand_dims(x, axis=0)\n",
    "    ech_z = np.expand_dims(z_star, axis=0)\n",
    "    ssm_model.compute_MLE_SSM(ech_x, ech_z)\n",
    "    previous_z_1_T_star = z_star_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Initialisation\n",
    "# P=50\n",
    "# NUM_TOPICS=50\n",
    "# NUM_WORDS=5000\n",
    "# T=100\n",
    "\n",
    "# lstm = lstm_model\n",
    "# ssm = ssm_model\n",
    "\n",
    "# Z_matrix=np.zeros((P, T+1))\n",
    "# alpha_matrix=np.zeros((P,T+1))\n",
    "# ancestor_matrix=np.zeros((P,T+1))\n",
    "\n",
    "\n",
    "# z_1_T_star = np.random.choice(a=range(1,NUM_TOPICS+1), size=T)\n",
    "# x = X_train[0]\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##t=0\n",
    "# z_0 = np.random.choice(a=range(1,NUM_TOPICS+1), size=P)\n",
    "# alpha_0 = np.repeat(1/P, P)\n",
    "# Z_matrix[:,0] = z_0\n",
    "# alpha_matrix[:,0] = alpha_0\n",
    "\n",
    "# #z[k:n]: du k-ème au n-1 ème\n",
    "# for t in range(1,T+1):\n",
    "#     print(t)\n",
    "#     a_t_minus_1 = 1 #ok\n",
    "#     z_1_t = z_1_T_star[:t] #ok\n",
    "#     ancestor_matrix[0,t-1] = a_t_minus_1 #ok\n",
    "#     Z_matrix[0, 1:t+1] = z_1_t #ok\n",
    "\n",
    "#     for p in range(2,P+1):\n",
    "#         alpha_t_minus_1_p=alpha_matrix[:,t-1]\n",
    "#         #a_t_minus_1_p = np.random.choice(a=range(1,P+1), p=alpha_t_minus_1_p, size=1)[0] #ok\n",
    "#         a_t_minus_1_p = np.argmax(alpha_t_minus_1_p)+1\n",
    "#         ancestor_matrix[p-1, t-1] = a_t_minus_1_p #ok\n",
    "#         if t ==1:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "#             gamma_t_p = compute_gamma_normalized(t=t,\n",
    "#                                              xt=x[t-1], \n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics = NUM_TOPICS,\n",
    "#                                              lstm = lstm,\n",
    "#                                              ssm = ssm)\n",
    "#             z_t_p = np.argmax(gamma_t_p)+1\n",
    "#             z_1_t_p = z_t_p\n",
    "#         else:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1] #ok\n",
    "#             gamma_t_p = compute_gamma_normalized(t=t,\n",
    "#                                              xt=x[t-1], \n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics = NUM_TOPICS,\n",
    "#                                              lstm = lstm,\n",
    "#                                              ssm = ssm)\n",
    "#             z_t_p = np.argmax(gamma_t_p)+1\n",
    "#             #z_t_p = np.random.choice(a=range(1, NUM_TOPICS+1), p=gamma_t_p, size=1)[0]\n",
    "#             z_1_t_p = np.append(z_1_t_minus_1_a_t_minus_1_p, z_t_p)\n",
    "        \n",
    "#         Z_matrix[p-1, 1:t+1] = z_1_t_p\n",
    "\n",
    "    \n",
    "#     for p in range(1, P+1):\n",
    "#         a_t_minus_1_p = ancestor_matrix[p-1, t-1]\n",
    "#         if t ==1:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "#         else:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1]\n",
    "        \n",
    "#         alpha_t_p = compute_alpha_normalized(t=t,\n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics=NUM_TOPICS,\n",
    "#                                              num_voc=NUM_WORDS,\n",
    "#                                              lstm=lstm,\n",
    "#                                              ssm=ssm)\n",
    "#         alpha_t_p = alpha_t_p[x[t-1]-1]\n",
    "#         alpha_matrix[p-1,t] = alpha_t_p\n",
    "\n",
    "# alpha_T=alpha_matrix[:,-1]\n",
    "# alpha_T = alpha_T / np.sum(alpha_T+1e-6)\n",
    "# r = np.argmax(alpha_T)+1\n",
    "# a_T_r = ancestor_matrix[int(r)-1, -1]\n",
    "# z_1_T = Z_matrix[int(a_T_r)-1, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
