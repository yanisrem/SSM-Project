{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données et pré-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 5000\n",
    "max_review_length = 100\n",
    "INDEX_FROM = 3\n",
    "\n",
    "# --- Import the IMDB data and only consider the ``top_words``` most used words\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "np.load.__defaults__=(None, False, True, 'ASCII')\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train[0]): 100\n",
      "len(X_train[1]): 100\n",
      "X_train[0]: [   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      "    2   16]\n"
     ]
    }
   ],
   "source": [
    "# --- truncate and pad input sequences\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post', truncating='post', value=0)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post', truncating='post', value=0)\n",
    "\n",
    "print(\"len(X_train[0]):\", len(X_train[0]))\n",
    "print(\"len(X_train[1]):\", len(X_train[1]))\n",
    "print(\"X_train[0]:\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, model_length):\n",
    "        self.topics=topics\n",
    "        self.model_length=model_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.topics)-self.model_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_sequence=torch.tensor(self.topics[index:index+self.model_length, :])\n",
    "        target_sequence=torch.tensor(self.topics[index+1:index+self.model_length+1, :])\n",
    "\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size #dimension d'entrée (NUM_TOPICS)\n",
    "        self.hidden_size = hidden_size #nombre de neurones de la couche cachée\n",
    "        self.output_size = output_size #dimension d'outputs (NUM_TOPICS)\n",
    "        self.model_length=model_length\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        # self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        output=self.fc(output)\n",
    "        probabilities = F.softmax(output[:, -1, :], dim=1)\n",
    "        return probabilities, state\n",
    "    \n",
    "    def init_state(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size), #(NUM_LAYERS, BATCH SIZE, NUM_NEURONES)\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n",
    "    def train_model(self, dataset, optimizer, criterion):\n",
    "        state_h, state_c = self.init_state()\n",
    "        self.train()\n",
    "        for t, (x, y) in enumerate(dataset):\n",
    "            optimizer.zero_grad()\n",
    "            softmax , (state_h, state_c) = self(x, (state_h, state_c)) #softmax= p(z_{t+1}|z_1:t)\n",
    "            loss = criterion(softmax, y[:, -1, :])\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print({'loss': loss.item() })\n",
    "    \n",
    "    def predict_next_probability(self, input_sequence):\n",
    "        state_h, state_c = self.init_state()\n",
    "\n",
    "        # Forward pass jusqu'à t-1\n",
    "        for t in range(len(input_sequence)):\n",
    "            input_t = input_sequence[t].unsqueeze(0).unsqueeze(0)\n",
    "            _, (state_h, state_c) = self(input_t, (state_h, state_c))\n",
    "\n",
    "        # Obtenez les probabilités pour x_t\n",
    "        input_t = input_sequence[-1].unsqueeze(0).unsqueeze(0)\n",
    "        probabilities, _ = self(input_t, (state_h, state_c))\n",
    "\n",
    "        return probabilities\n",
    "    \n",
    "    def sample_next_z(self, input_sequence):\n",
    "        proba = self.predict_next_probability(input_sequence)\n",
    "        return torch.multinomial(proba, 1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSM:\n",
    "    def __init__(self, num_words, num_topics, T):\n",
    "        self.num_words = num_words\n",
    "        self.num_topics = num_topics\n",
    "        self.T = T\n",
    "        self.phi = np.random.randn(T, num_words, num_topics) * 0.01\n",
    "        self.phi = np.exp(self.phi - np.max(self.phi, axis=1, keepdims=True))\n",
    "        self.phi = self.phi / np.sum(self.phi, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    def compute_MLE_SSM(self, ech_x, ech_z):\n",
    "        def compute_MLE_SSM_time_t_zt(t, z_t, ech_x, ech_z, num_words):\n",
    "            phi = np.zeros(num_words)\n",
    "            list_probas = []\n",
    "            for j in range(1,num_words+1):\n",
    "                num = len(np.where((ech_x[:,t-1]==j)&(ech_z[:,t-1]==z_t))[0])/ech_x.shape[0]\n",
    "                phi[j-1] = num\n",
    "                list_probas.append(num)\n",
    "\n",
    "            denom = np.sum(np.array(list_probas))\n",
    "            phi = phi / (denom + 1e-6)\n",
    "            return phi\n",
    "        \n",
    "        def compute_MLE_SSM_time_t(t, ech_x, ech_z, num_words, num_topics):\n",
    "            phi = np.zeros((num_words, num_topics))\n",
    "            for k in range(1, num_topics+1):\n",
    "                z_t = k\n",
    "                phi_zt = compute_MLE_SSM_time_t_zt(t=t, z_t=z_t, ech_x=ech_x, ech_z=ech_z, num_words=num_words)\n",
    "                phi[:,k-1] = phi_zt\n",
    "            return phi\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            self.phi[t] = compute_MLE_SSM_time_t(t, ech_x, ech_z, self.num_words, self.num_topics)\n",
    "    \n",
    "    def predict_proba(self, t, z_t):\n",
    "        return self.phi[t-1][:,z_t-1]\n",
    "    \n",
    "    def sample_xt(self, t, z_t):\n",
    "        proba = self.predict_proba(t, z_t)\n",
    "        sampled_xt = np.random.choice(len(proba), p=proba)\n",
    "        return sampled_xt\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    z_1_t_minus_1=z_1_t_minus_1-1\n",
    "    z_one_hot = to_categorical(z_1_t_minus_1, num_classes=num_topics)\n",
    "    z_one_hot_tensor = torch.FloatTensor(z_one_hot)\n",
    "    softmax = lstm.predict_next_probability(z_one_hot_tensor).detach().numpy()[0]\n",
    "    phi_t = ssm.phi[t-1]\n",
    "    alpha = np.array([np.dot(softmax, phi_t[j,:]) for j in range(num_voc)])\n",
    "    return alpha\n",
    "\n",
    "def compute_alpha_normalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    num = compute_alpha_unnormalized(t, z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\n",
    "    denom = np.sum(num)\n",
    "    return num/(denom+1e-6)\n",
    "\n",
    "def compute_gamma_unnormalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    z_1_t_minus_1=z_1_t_minus_1-1\n",
    "    z_one_hot = to_categorical(z_1_t_minus_1, num_classes=num_topics)\n",
    "    z_one_hot_tensor = torch.FloatTensor(z_one_hot)\n",
    "    softmax = lstm.predict_next_probability(z_one_hot_tensor).detach().numpy()[0]\n",
    "    phi_t = ssm.phi[t-1]\n",
    "    phi_xt = phi_t[xt-1, :]\n",
    "    return np.multiply(softmax, phi_xt)\n",
    "\n",
    "def compute_gamma_normalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    num = compute_gamma_unnormalized(t, xt, z_1_t_minus_1, num_topics, lstm, ssm)\n",
    "    denom = np.sum(num)\n",
    "    return num/(denom+1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_gibbs(P, num_topics, num_words, T, lstm_model, ssm_model, x, previous_z_1_T_star):\n",
    "    ##Init\n",
    "    Z_matrix=np.zeros((P, T+1))\n",
    "    alpha_matrix=np.zeros((P,T+1))\n",
    "    ancestor_matrix=np.ones((P,T+1))\n",
    "    ##t=0\n",
    "    z_0 = np.random.choice(a=range(1,num_topics+1), size=P)\n",
    "    alpha_0 = np.repeat(1/P, P)\n",
    "    Z_matrix[:,0] = z_0\n",
    "    alpha_matrix[:,0] = alpha_0\n",
    "\n",
    "    #z[k:n]: du k-ème au n-1 ème\n",
    "    for t in range(1,T+1):\n",
    "        a_t_minus_1 = 1 #ok\n",
    "        z_1_t = previous_z_1_T_star[:t] #ok\n",
    "        ancestor_matrix[0,t-1] = a_t_minus_1 #ok\n",
    "        Z_matrix[0, 1:t+1] = z_1_t #ok\n",
    "\n",
    "        for p in range(2,P+1):\n",
    "            alpha_t_minus_1_p=alpha_matrix[:,t-1]\n",
    "            #a_t_minus_1_p = np.random.choice(a=range(1,P+1), p=alpha_t_minus_1_p, size=1)[0] #ok\n",
    "            a_t_minus_1_p = np.argmax(alpha_t_minus_1_p)+1\n",
    "            ancestor_matrix[p-1, t-1] = a_t_minus_1_p #ok\n",
    "            if t ==1:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "                z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "                gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                                xt=x[t-1], \n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics = num_topics,\n",
    "                                                lstm = lstm_model,\n",
    "                                                ssm = ssm_model)\n",
    "                z_t_p = np.argmax(gamma_t_p)+1\n",
    "                z_1_t_p = z_t_p\n",
    "            else:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1] #ok\n",
    "                gamma_t_p = compute_gamma_normalized(t=t,\n",
    "                                                xt=x[t-1], \n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics = num_topics,\n",
    "                                                lstm = lstm_model,\n",
    "                                                ssm = ssm_model)\n",
    "                z_t_p = np.argmax(gamma_t_p)+1\n",
    "                #z_t_p = np.random.choice(a=range(1, NUM_TOPICS+1), p=gamma_t_p, size=1)[0]\n",
    "                z_1_t_p = np.append(z_1_t_minus_1_a_t_minus_1_p, z_t_p)\n",
    "            \n",
    "            Z_matrix[p-1, 1:t+1] = z_1_t_p\n",
    "\n",
    "        \n",
    "        for p in range(1, P+1):\n",
    "            a_t_minus_1_p = ancestor_matrix[p-1, t-1]\n",
    "            if t ==1:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "                z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "            else:\n",
    "                z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1]\n",
    "            \n",
    "            alpha_t_p = compute_alpha_normalized(t=t,\n",
    "                                                z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                num_topics=num_topics,\n",
    "                                                num_voc=num_words,\n",
    "                                                lstm=lstm_model,\n",
    "                                                ssm=ssm_model)\n",
    "            alpha_t_p = alpha_t_p[x[t-1]-1]\n",
    "            alpha_matrix[p-1,t] = alpha_t_p\n",
    "    alpha_T=alpha_matrix[:,-1]\n",
    "    alpha_T = alpha_T / np.sum(alpha_T+1e-6)\n",
    "    r = np.argmax(alpha_T)+1\n",
    "    a_T_r = ancestor_matrix[int(r)-1, -1]\n",
    "    z_1_T = Z_matrix[int(a_T_r)-1, 1:]\n",
    "    return z_1_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=64\n",
    "SEQUENCE_LENGTH = 100\n",
    "MODEL_LENGTH = 10\n",
    "NUM_TOPICS = 50\n",
    "NUM_WORDS = 5000\n",
    "N_ITER_EM = 1\n",
    "NUM_PARTICULES = 10\n",
    "\n",
    "lstm_model=LSTM(input_size=NUM_TOPICS, hidden_size=HIDDEN_SIZE, output_size=NUM_TOPICS, model_length=MODEL_LENGTH)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "ssm_model = SSM(num_words=NUM_WORDS, num_topics=NUM_TOPICS, T=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5000, 50)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssm_model.phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = torch.randn((100, 5000, 50), requires_grad=True) * 0.01\n",
    "phi\n",
    "# Normalisation softmax\n",
    "phi = F.softmax(phi, dim=1)\n",
    "torch.sum(phi[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 0\n",
      "{'loss': 3.914184093475342}\n",
      "{'loss': 3.9120841026306152}\n",
      "{'loss': 3.910663366317749}\n",
      "{'loss': 3.911644220352173}\n",
      "{'loss': 3.9115757942199707}\n",
      "{'loss': 3.9136569499969482}\n",
      "{'loss': 3.9102933406829834}\n",
      "{'loss': 3.911639928817749}\n",
      "{'loss': 3.9094855785369873}\n",
      "{'loss': 3.914017915725708}\n",
      "{'loss': 3.909961462020874}\n",
      "{'loss': 3.9099209308624268}\n",
      "{'loss': 3.9139115810394287}\n",
      "{'loss': 3.9113500118255615}\n",
      "{'loss': 3.9088540077209473}\n",
      "{'loss': 3.912597179412842}\n",
      "{'loss': 3.9115169048309326}\n",
      "{'loss': 3.9126017093658447}\n",
      "{'loss': 3.915341377258301}\n",
      "{'loss': 3.911907911300659}\n",
      "{'loss': 3.9115161895751953}\n",
      "{'loss': 3.9141688346862793}\n",
      "{'loss': 3.913803815841675}\n",
      "{'loss': 3.913529396057129}\n",
      "{'loss': 3.911194086074829}\n",
      "{'loss': 3.909653425216675}\n",
      "{'loss': 3.9093902111053467}\n",
      "{'loss': 3.910464286804199}\n",
      "{'loss': 3.9083778858184814}\n",
      "{'loss': 3.9146018028259277}\n",
      "{'loss': 3.911693572998047}\n",
      "{'loss': 3.9101462364196777}\n",
      "{'loss': 3.914497137069702}\n",
      "{'loss': 3.9116764068603516}\n",
      "{'loss': 3.9114274978637695}\n",
      "{'loss': 3.910040855407715}\n",
      "{'loss': 3.9128611087799072}\n",
      "{'loss': 3.910020112991333}\n",
      "{'loss': 3.91206955909729}\n",
      "{'loss': 3.910818099975586}\n",
      "{'loss': 3.910510778427124}\n",
      "{'loss': 3.9133107662200928}\n",
      "{'loss': 3.9111814498901367}\n",
      "{'loss': 3.9128315448760986}\n",
      "{'loss': 3.9130699634552}\n",
      "{'loss': 3.9095847606658936}\n",
      "{'loss': 3.913428544998169}\n",
      "{'loss': 3.9110774993896484}\n",
      "{'loss': 3.9086952209472656}\n",
      "{'loss': 3.909227132797241}\n",
      "{'loss': 3.911878824234009}\n",
      "{'loss': 3.9140286445617676}\n",
      "{'loss': 3.9136505126953125}\n",
      "{'loss': 3.911424398422241}\n",
      "{'loss': 3.9098172187805176}\n",
      "{'loss': 3.912088632583618}\n",
      "{'loss': 3.9105122089385986}\n",
      "{'loss': 3.911586284637451}\n",
      "{'loss': 3.9106268882751465}\n",
      "{'loss': 3.9112308025360107}\n",
      "{'loss': 3.911830186843872}\n",
      "{'loss': 3.9139187335968018}\n",
      "{'loss': 3.9112484455108643}\n",
      "{'loss': 3.9117281436920166}\n",
      "{'loss': 3.912853479385376}\n",
      "{'loss': 3.9115700721740723}\n",
      "{'loss': 3.9118573665618896}\n",
      "{'loss': 3.9146037101745605}\n",
      "{'loss': 3.913949728012085}\n",
      "{'loss': 3.9133872985839844}\n",
      "{'loss': 3.910224676132202}\n",
      "{'loss': 3.9103007316589355}\n",
      "{'loss': 3.911829710006714}\n",
      "{'loss': 3.913370132446289}\n",
      "{'loss': 3.9115004539489746}\n",
      "{'loss': 3.9155263900756836}\n",
      "{'loss': 3.9145452976226807}\n",
      "{'loss': 3.915052652359009}\n",
      "{'loss': 3.9098587036132812}\n",
      "{'loss': 3.912748336791992}\n",
      "{'loss': 3.9103598594665527}\n",
      "{'loss': 3.9113056659698486}\n",
      "{'loss': 3.9115383625030518}\n",
      "{'loss': 3.9131245613098145}\n",
      "{'loss': 3.9128048419952393}\n",
      "{'loss': 3.9143083095550537}\n",
      "{'loss': 3.9112930297851562}\n",
      "{'loss': 3.911813974380493}\n",
      "{'loss': 3.9135026931762695}\n",
      "{'loss': 3.914030075073242}\n",
      "i : 1\n",
      "{'loss': 3.911647319793701}\n",
      "{'loss': 3.9104220867156982}\n",
      "{'loss': 3.9128921031951904}\n",
      "{'loss': 3.9146323204040527}\n",
      "{'loss': 3.9146883487701416}\n",
      "{'loss': 3.909677267074585}\n",
      "{'loss': 3.9139678478240967}\n",
      "{'loss': 3.9125375747680664}\n",
      "{'loss': 3.9149551391601562}\n",
      "{'loss': 3.9102869033813477}\n",
      "{'loss': 3.913799524307251}\n",
      "{'loss': 3.9141831398010254}\n",
      "{'loss': 3.911066770553589}\n",
      "{'loss': 3.9122374057769775}\n",
      "{'loss': 3.9088549613952637}\n",
      "{'loss': 3.914604663848877}\n",
      "{'loss': 3.910933494567871}\n",
      "{'loss': 3.912719488143921}\n",
      "{'loss': 3.909306764602661}\n",
      "{'loss': 3.9139862060546875}\n",
      "{'loss': 3.9125893115997314}\n",
      "{'loss': 3.911716938018799}\n",
      "{'loss': 3.913057327270508}\n",
      "{'loss': 3.913539171218872}\n",
      "{'loss': 3.9099841117858887}\n",
      "{'loss': 3.909820556640625}\n",
      "{'loss': 3.9101505279541016}\n",
      "{'loss': 3.909707546234131}\n",
      "{'loss': 3.9114749431610107}\n",
      "{'loss': 3.9138331413269043}\n",
      "{'loss': 3.9096124172210693}\n",
      "{'loss': 3.912799119949341}\n",
      "{'loss': 3.9101455211639404}\n",
      "{'loss': 3.9107956886291504}\n",
      "{'loss': 3.909334182739258}\n",
      "{'loss': 3.913719415664673}\n",
      "{'loss': 3.9111640453338623}\n",
      "{'loss': 3.912546396255493}\n",
      "{'loss': 3.909318447113037}\n",
      "{'loss': 3.910538911819458}\n",
      "{'loss': 3.9151415824890137}\n",
      "{'loss': 3.913057804107666}\n",
      "{'loss': 3.9109623432159424}\n",
      "{'loss': 3.9100077152252197}\n",
      "{'loss': 3.910169839859009}\n",
      "{'loss': 3.9104063510894775}\n",
      "{'loss': 3.9098825454711914}\n",
      "{'loss': 3.915268898010254}\n",
      "{'loss': 3.9114317893981934}\n",
      "{'loss': 3.9116222858428955}\n",
      "{'loss': 3.9143388271331787}\n",
      "{'loss': 3.9147560596466064}\n",
      "{'loss': 3.9102840423583984}\n",
      "{'loss': 3.913332462310791}\n",
      "{'loss': 3.91457200050354}\n",
      "{'loss': 3.9121530055999756}\n",
      "{'loss': 3.912821054458618}\n",
      "{'loss': 3.9097094535827637}\n",
      "{'loss': 3.9110724925994873}\n",
      "{'loss': 3.9120473861694336}\n",
      "{'loss': 3.910872459411621}\n",
      "{'loss': 3.914780616760254}\n",
      "{'loss': 3.9111602306365967}\n",
      "{'loss': 3.909911632537842}\n",
      "{'loss': 3.9112915992736816}\n",
      "{'loss': 3.909928321838379}\n",
      "{'loss': 3.914525032043457}\n",
      "{'loss': 3.911696434020996}\n",
      "{'loss': 3.913735866546631}\n",
      "{'loss': 3.9104316234588623}\n",
      "{'loss': 3.9147112369537354}\n",
      "{'loss': 3.9126529693603516}\n",
      "{'loss': 3.911261796951294}\n",
      "{'loss': 3.911406993865967}\n",
      "{'loss': 3.9135642051696777}\n",
      "{'loss': 3.9098849296569824}\n",
      "{'loss': 3.9104740619659424}\n",
      "{'loss': 3.912893533706665}\n",
      "{'loss': 3.9137120246887207}\n",
      "{'loss': 3.9111180305480957}\n",
      "{'loss': 3.9103376865386963}\n",
      "{'loss': 3.910369634628296}\n",
      "{'loss': 3.9113337993621826}\n",
      "{'loss': 3.9122443199157715}\n",
      "{'loss': 3.9105260372161865}\n",
      "{'loss': 3.9084737300872803}\n",
      "{'loss': 3.909388303756714}\n",
      "{'loss': 3.9102516174316406}\n",
      "{'loss': 3.9143319129943848}\n",
      "{'loss': 3.911749839782715}\n",
      "i : 2\n",
      "{'loss': 3.9145193099975586}\n",
      "{'loss': 3.9139082431793213}\n",
      "{'loss': 3.9120707511901855}\n",
      "{'loss': 3.9119668006896973}\n",
      "{'loss': 3.9108777046203613}\n",
      "{'loss': 3.9119863510131836}\n",
      "{'loss': 3.91159725189209}\n",
      "{'loss': 3.91146183013916}\n",
      "{'loss': 3.912770986557007}\n",
      "{'loss': 3.9120240211486816}\n",
      "{'loss': 3.9119768142700195}\n",
      "{'loss': 3.9146947860717773}\n",
      "{'loss': 3.911329984664917}\n",
      "{'loss': 3.9105160236358643}\n",
      "{'loss': 3.9102957248687744}\n",
      "{'loss': 3.910881757736206}\n",
      "{'loss': 3.9132771492004395}\n",
      "{'loss': 3.9096972942352295}\n",
      "{'loss': 3.914508819580078}\n",
      "{'loss': 3.9096763134002686}\n",
      "{'loss': 3.9136464595794678}\n",
      "{'loss': 3.9086694717407227}\n",
      "{'loss': 3.914539098739624}\n",
      "{'loss': 3.9137089252471924}\n",
      "{'loss': 3.9132556915283203}\n",
      "{'loss': 3.9105064868927}\n",
      "{'loss': 3.912186861038208}\n",
      "{'loss': 3.9109737873077393}\n",
      "{'loss': 3.9116899967193604}\n",
      "{'loss': 3.9088399410247803}\n",
      "{'loss': 3.910853624343872}\n",
      "{'loss': 3.914771795272827}\n",
      "{'loss': 3.910573959350586}\n",
      "{'loss': 3.9145522117614746}\n",
      "{'loss': 3.910630702972412}\n",
      "{'loss': 3.9114794731140137}\n",
      "{'loss': 3.913736581802368}\n",
      "{'loss': 3.9092769622802734}\n",
      "{'loss': 3.9116880893707275}\n",
      "{'loss': 3.9099884033203125}\n",
      "{'loss': 3.9141550064086914}\n",
      "{'loss': 3.909914970397949}\n",
      "{'loss': 3.9134578704833984}\n",
      "{'loss': 3.91288685798645}\n",
      "{'loss': 3.914011240005493}\n",
      "{'loss': 3.912079095840454}\n",
      "{'loss': 3.9124138355255127}\n",
      "{'loss': 3.9117677211761475}\n",
      "{'loss': 3.9143450260162354}\n",
      "{'loss': 3.911370277404785}\n",
      "{'loss': 3.909573554992676}\n",
      "{'loss': 3.9104278087615967}\n",
      "{'loss': 3.914055347442627}\n",
      "{'loss': 3.913438558578491}\n",
      "{'loss': 3.9114420413970947}\n",
      "{'loss': 3.9109325408935547}\n",
      "{'loss': 3.9120211601257324}\n",
      "{'loss': 3.9103269577026367}\n",
      "{'loss': 3.9116992950439453}\n",
      "{'loss': 3.9124033451080322}\n",
      "{'loss': 3.909148693084717}\n",
      "{'loss': 3.911839008331299}\n",
      "{'loss': 3.911497116088867}\n",
      "{'loss': 3.9115490913391113}\n",
      "{'loss': 3.9087915420532227}\n",
      "{'loss': 3.914276123046875}\n",
      "{'loss': 3.911289691925049}\n",
      "{'loss': 3.9138643741607666}\n",
      "{'loss': 3.914132833480835}\n",
      "{'loss': 3.912623882293701}\n",
      "{'loss': 3.911449670791626}\n",
      "{'loss': 3.9121429920196533}\n",
      "{'loss': 3.911548137664795}\n",
      "{'loss': 3.9113783836364746}\n",
      "{'loss': 3.9117658138275146}\n",
      "{'loss': 3.9141147136688232}\n",
      "{'loss': 3.911466360092163}\n",
      "{'loss': 3.9151082038879395}\n",
      "{'loss': 3.909796714782715}\n",
      "{'loss': 3.9143142700195312}\n",
      "{'loss': 3.914245843887329}\n",
      "{'loss': 3.912940502166748}\n",
      "{'loss': 3.9147562980651855}\n",
      "{'loss': 3.9099724292755127}\n",
      "{'loss': 3.912999391555786}\n",
      "{'loss': 3.914407253265381}\n",
      "{'loss': 3.911004066467285}\n",
      "{'loss': 3.9115140438079834}\n",
      "{'loss': 3.9107320308685303}\n",
      "{'loss': 3.911992073059082}\n",
      "i : 3\n",
      "{'loss': 3.911256790161133}\n",
      "{'loss': 3.911975383758545}\n",
      "{'loss': 3.914745569229126}\n",
      "{'loss': 3.91467547416687}\n",
      "{'loss': 3.914278268814087}\n",
      "{'loss': 3.9118809700012207}\n",
      "{'loss': 3.9104604721069336}\n",
      "{'loss': 3.9120399951934814}\n",
      "{'loss': 3.9116268157958984}\n",
      "{'loss': 3.911763906478882}\n",
      "{'loss': 3.910064935684204}\n",
      "{'loss': 3.9154374599456787}\n",
      "{'loss': 3.9125845432281494}\n",
      "{'loss': 3.9103729724884033}\n",
      "{'loss': 3.91207218170166}\n",
      "{'loss': 3.9143683910369873}\n",
      "{'loss': 3.911637783050537}\n",
      "{'loss': 3.9130067825317383}\n",
      "{'loss': 3.91294527053833}\n",
      "{'loss': 3.91176700592041}\n",
      "{'loss': 3.911482810974121}\n",
      "{'loss': 3.9098541736602783}\n",
      "{'loss': 3.9108810424804688}\n",
      "{'loss': 3.910728693008423}\n",
      "{'loss': 3.9116370677948}\n",
      "{'loss': 3.9144811630249023}\n",
      "{'loss': 3.9129507541656494}\n",
      "{'loss': 3.909431219100952}\n",
      "{'loss': 3.9135022163391113}\n",
      "{'loss': 3.911757230758667}\n",
      "{'loss': 3.9112019538879395}\n",
      "{'loss': 3.9117014408111572}\n",
      "{'loss': 3.914736747741699}\n",
      "{'loss': 3.9141063690185547}\n",
      "{'loss': 3.9098968505859375}\n",
      "{'loss': 3.9117414951324463}\n",
      "{'loss': 3.9115798473358154}\n",
      "{'loss': 3.9095940589904785}\n",
      "{'loss': 3.910252571105957}\n",
      "{'loss': 3.911149740219116}\n",
      "{'loss': 3.914125442504883}\n",
      "{'loss': 3.9112837314605713}\n",
      "{'loss': 3.9116201400756836}\n",
      "{'loss': 3.9097938537597656}\n",
      "{'loss': 3.9134862422943115}\n",
      "{'loss': 3.909834384918213}\n",
      "{'loss': 3.911957025527954}\n",
      "{'loss': 3.9123761653900146}\n",
      "{'loss': 3.910578727722168}\n",
      "{'loss': 3.9086477756500244}\n",
      "{'loss': 3.914346694946289}\n",
      "{'loss': 3.9135029315948486}\n",
      "{'loss': 3.9099926948547363}\n",
      "{'loss': 3.9105966091156006}\n",
      "{'loss': 3.9112167358398438}\n",
      "{'loss': 3.9145901203155518}\n",
      "{'loss': 3.9105846881866455}\n",
      "{'loss': 3.9125897884368896}\n",
      "{'loss': 3.9136884212493896}\n",
      "{'loss': 3.9109439849853516}\n",
      "{'loss': 3.9132206439971924}\n",
      "{'loss': 3.9092376232147217}\n",
      "{'loss': 3.9094653129577637}\n",
      "{'loss': 3.909858226776123}\n",
      "{'loss': 3.9106502532958984}\n",
      "{'loss': 3.9100608825683594}\n",
      "{'loss': 3.9124186038970947}\n",
      "{'loss': 3.914644479751587}\n",
      "{'loss': 3.9103705883026123}\n",
      "{'loss': 3.9101321697235107}\n",
      "{'loss': 3.91278076171875}\n",
      "{'loss': 3.9137301445007324}\n",
      "{'loss': 3.909942388534546}\n",
      "{'loss': 3.910011053085327}\n",
      "{'loss': 3.9118714332580566}\n",
      "{'loss': 3.9090030193328857}\n",
      "{'loss': 3.9140799045562744}\n",
      "{'loss': 3.911961317062378}\n",
      "{'loss': 3.912721633911133}\n",
      "{'loss': 3.9114010334014893}\n",
      "{'loss': 3.9143588542938232}\n",
      "{'loss': 3.91390061378479}\n",
      "{'loss': 3.9099671840667725}\n",
      "{'loss': 3.910244941711426}\n",
      "{'loss': 3.9122812747955322}\n",
      "{'loss': 3.9118764400482178}\n",
      "{'loss': 3.9096839427948}\n",
      "{'loss': 3.9121341705322266}\n",
      "{'loss': 3.9100236892700195}\n",
      "{'loss': 3.910531759262085}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m ech_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m ech_z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(z_star, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m ssm_model\u001b[38;5;241m.\u001b[39mcompute_MLE_SSM(ech_x, ech_z)\n\u001b[0;32m     14\u001b[0m previous_z_1_T_star \u001b[38;5;241m=\u001b[39m z_star_one_hot\n",
      "Cell \u001b[1;32mIn[38], line 30\u001b[0m, in \u001b[0;36mSSM.compute_MLE_SSM\u001b[1;34m(self, ech_x, ech_z)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m phi\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi[t] \u001b[38;5;241m=\u001b[39m compute_MLE_SSM_time_t(t, ech_x, ech_z, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_words, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_topics)\n",
      "Cell \u001b[1;32mIn[38], line 25\u001b[0m, in \u001b[0;36mSSM.compute_MLE_SSM.<locals>.compute_MLE_SSM_time_t\u001b[1;34m(t, ech_x, ech_z, num_words, num_topics)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_topics\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     24\u001b[0m     z_t \u001b[38;5;241m=\u001b[39m k\n\u001b[1;32m---> 25\u001b[0m     phi_zt \u001b[38;5;241m=\u001b[39m compute_MLE_SSM_time_t_zt(t\u001b[38;5;241m=\u001b[39mt, z_t\u001b[38;5;241m=\u001b[39mz_t, ech_x\u001b[38;5;241m=\u001b[39mech_x, ech_z\u001b[38;5;241m=\u001b[39mech_z, num_words\u001b[38;5;241m=\u001b[39mnum_words)\n\u001b[0;32m     26\u001b[0m     phi[:,k\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m phi_zt\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m phi\n",
      "Cell \u001b[1;32mIn[38], line 13\u001b[0m, in \u001b[0;36mSSM.compute_MLE_SSM.<locals>.compute_MLE_SSM_time_t_zt\u001b[1;34m(t, z_t, ech_x, ech_z, num_words)\u001b[0m\n\u001b[0;32m     11\u001b[0m list_probas \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_words\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere((ech_x[:,t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39mj)\u001b[38;5;241m&\u001b[39m(ech_z[:,t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39mz_t))[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m/\u001b[39mech_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m     phi[j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m num\n\u001b[0;32m     15\u001b[0m     list_probas\u001b[38;5;241m.\u001b[39mappend(num)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\Lib\\site-packages\\numpy\\core\\multiarray.py:345\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(condition, x, y)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    print(\"i : {}\".format(i))\n",
    "    x = X_train[i]\n",
    "    previous_z_1_T_star = np.random.choice(a=range(1,NUM_TOPICS+1), size=SEQUENCE_LENGTH)\n",
    "    for iter in range(N_ITER_EM):\n",
    "        z_star = particle_gibbs(NUM_PARTICULES, NUM_TOPICS, NUM_WORDS, SEQUENCE_LENGTH, lstm_model, ssm_model, x, previous_z_1_T_star)\n",
    "        z_star_one_hot = to_categorical(z_star-1, num_classes=NUM_TOPICS)\n",
    "        dataset=Dataset(topics=z_star_one_hot, model_length=MODEL_LENGTH)\n",
    "        dataloader = DataLoader(dataset, batch_size=1)\n",
    "        lstm_model.train_model(dataloader, optimizer, criterion)\n",
    "        ech_x = np.expand_dims(x, axis=0)\n",
    "        ech_z = np.expand_dims(z_star, axis=0)\n",
    "        ssm_model.compute_MLE_SSM(ech_x, ech_z)\n",
    "        previous_z_1_T_star = z_star_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Initialisation\n",
    "# P=50\n",
    "# NUM_TOPICS=50\n",
    "# NUM_WORDS=5000\n",
    "# T=100\n",
    "\n",
    "# lstm = lstm_model\n",
    "# ssm = ssm_model\n",
    "\n",
    "# Z_matrix=np.zeros((P, T+1))\n",
    "# alpha_matrix=np.zeros((P,T+1))\n",
    "# ancestor_matrix=np.zeros((P,T+1))\n",
    "\n",
    "\n",
    "# z_1_T_star = np.random.choice(a=range(1,NUM_TOPICS+1), size=T)\n",
    "# x = X_train[0]\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##t=0\n",
    "# z_0 = np.random.choice(a=range(1,NUM_TOPICS+1), size=P)\n",
    "# alpha_0 = np.repeat(1/P, P)\n",
    "# Z_matrix[:,0] = z_0\n",
    "# alpha_matrix[:,0] = alpha_0\n",
    "\n",
    "# #z[k:n]: du k-ème au n-1 ème\n",
    "# for t in range(1,T+1):\n",
    "#     print(t)\n",
    "#     a_t_minus_1 = 1 #ok\n",
    "#     z_1_t = z_1_T_star[:t] #ok\n",
    "#     ancestor_matrix[0,t-1] = a_t_minus_1 #ok\n",
    "#     Z_matrix[0, 1:t+1] = z_1_t #ok\n",
    "\n",
    "#     for p in range(2,P+1):\n",
    "#         alpha_t_minus_1_p=alpha_matrix[:,t-1]\n",
    "#         #a_t_minus_1_p = np.random.choice(a=range(1,P+1), p=alpha_t_minus_1_p, size=1)[0] #ok\n",
    "#         a_t_minus_1_p = np.argmax(alpha_t_minus_1_p)+1\n",
    "#         ancestor_matrix[p-1, t-1] = a_t_minus_1_p #ok\n",
    "#         if t ==1:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "#             gamma_t_p = compute_gamma_normalized(t=t,\n",
    "#                                              xt=x[t-1], \n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics = NUM_TOPICS,\n",
    "#                                              lstm = lstm,\n",
    "#                                              ssm = ssm)\n",
    "#             z_t_p = np.argmax(gamma_t_p)+1\n",
    "#             z_1_t_p = z_t_p\n",
    "#         else:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1] #ok\n",
    "#             gamma_t_p = compute_gamma_normalized(t=t,\n",
    "#                                              xt=x[t-1], \n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics = NUM_TOPICS,\n",
    "#                                              lstm = lstm,\n",
    "#                                              ssm = ssm)\n",
    "#             z_t_p = np.argmax(gamma_t_p)+1\n",
    "#             #z_t_p = np.random.choice(a=range(1, NUM_TOPICS+1), p=gamma_t_p, size=1)[0]\n",
    "#             z_1_t_p = np.append(z_1_t_minus_1_a_t_minus_1_p, z_t_p)\n",
    "        \n",
    "#         Z_matrix[p-1, 1:t+1] = z_1_t_p\n",
    "\n",
    "    \n",
    "#     for p in range(1, P+1):\n",
    "#         a_t_minus_1_p = ancestor_matrix[p-1, t-1]\n",
    "#         if t ==1:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 0] #ok\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = np.array([z_1_t_minus_1_a_t_minus_1_p]) #ok\n",
    "#         else:\n",
    "#             z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p)-1, 1:(t-1)+1]\n",
    "        \n",
    "#         alpha_t_p = compute_alpha_normalized(t=t,\n",
    "#                                              z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "#                                              num_topics=NUM_TOPICS,\n",
    "#                                              num_voc=NUM_WORDS,\n",
    "#                                              lstm=lstm,\n",
    "#                                              ssm=ssm)\n",
    "#         alpha_t_p = alpha_t_p[x[t-1]-1]\n",
    "#         alpha_matrix[p-1,t] = alpha_t_p\n",
    "\n",
    "# alpha_T=alpha_matrix[:,-1]\n",
    "# alpha_T = alpha_T / np.sum(alpha_T+1e-6)\n",
    "# r = np.argmax(alpha_T)+1\n",
    "# a_T_r = ancestor_matrix[int(r)-1, -1]\n",
    "# z_1_T = Z_matrix[int(a_T_r)-1, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
