{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow\n",
    "# ! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MfO30nSoUlsg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanis\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNTWEQ_MUlsi"
   },
   "source": [
    "# Import and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37zg2hZNUlsi",
    "outputId": "7bf71b0f-cd6b-413c-a07d-d917099f4b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n",
      "X shape: (200, 170)\n",
      "len(X[0]): 170\n",
      "len(X[1]): 170\n",
      "X[0]: [   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      "    2   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117    2   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56]\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 5000\n",
    "max_review_length = 100\n",
    "INDEX_FROM = 3\n",
    "\n",
    "# --- Import the IMDB data and only consider the ``top_words``` most used words\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "(X, y), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "np.load.__defaults__=(None, False, True, 'ASCII')\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in X[1000] ))\n",
    "\n",
    "# --- filter sequences with at least 170 words\n",
    "TOTAL_LENGTH = 170\n",
    "X = [lst for lst in X if len(lst) >= TOTAL_LENGTH]\n",
    "# --- truncate input sequences\n",
    "X = sequence.pad_sequences(X, maxlen=TOTAL_LENGTH, truncating='post')\n",
    "# --- Keep only 200 sequences\n",
    "N = 200\n",
    "X = X[:N]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"len(X[0]):\", len(X[0]))\n",
    "print(\"len(X[1]):\", len(X[1]))\n",
    "print(\"X[0]:\", X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsNUlEQVR4nO3dfXBUVZ7/8U8CdEOE7hAw6WQJD4qCAQIaNfaqrA4ZGsw6OlJbPlDCKkLBBGsgLg/ZZQF1a8LiroojYk25GrcWRJgSxyE8GIMElQCaMUMITlZY2DBCJ65MugEhhOT8/rByfzQGSCAhOeH9qrpl+p7vPfecQ2b6U7fv7UQZY4wAAAAsEt3eAwAAAGgpAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDpd23sAbaWhoUGHDx9Wr169FBUV1d7DAQAAzWCM0bFjx5SUlKTo6PNfZ+m0Aebw4cNKTk5u72EAAIBLcOjQIfXr1++87Z02wPTq1UvSDwvg8XjaeTQAAKA5wuGwkpOTnffx8+m0AabxYyOPx0OAAQDAMhe7/YObeAEAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4C5RAPn57f3EAAAuGoRYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA67QowKxYsUKpqanyeDzyeDzy+/3auHGj037PPfcoKioqYps+fXpEH5WVlcrMzFRMTIzi4+M1Z84cnTlzJqJm69atuuWWW+R2uzV48GDl5eVd+gwBAECn07Ulxf369dOSJUt0ww03yBijt99+Ww888IC+/PJLDRs2TJI0depUPffcc84xMTExzs/19fXKzMyUz+fT9u3bdeTIEU2aNEndunXTr371K0nSgQMHlJmZqenTp2vlypUqLCzUU089pcTERAUCgdaYMwAAsFyUMcZcTgdxcXF64YUXNGXKFN1zzz0aNWqUXn755SZrN27cqL/927/V4cOHlZCQIEl6/fXXNW/ePH377bdyuVyaN2+e8vPztWfPHue4Rx55RDU1Ndq0aVOzxxUOh+X1ehUKheTxeC5nik0aOD9fB5dktnq/AABczZr7/n3J98DU19dr9erVOnHihPx+v7N/5cqV6tu3r4YPH66cnBx9//33TltxcbFGjBjhhBdJCgQCCofDKi8vd2oyMjIizhUIBFRcXHzB8dTW1iocDkdsAACgc2rRR0iSVFZWJr/fr1OnTqlnz55at26dUlJSJEmPPfaYBgwYoKSkJO3evVvz5s1TRUWF3nvvPUlSMBiMCC+SnNfBYPCCNeFwWCdPnlSPHj2aHFdubq6effbZlk4HAABYqMUBZsiQISotLVUoFNJvf/tbTZ48WUVFRUpJSdG0adOcuhEjRigxMVFjxozR/v37df3117fqwM+Vk5Oj7Oxs53U4HFZycnKbnhMAALSPFn+E5HK5NHjwYKWlpSk3N1cjR47UsmXLmqxNT0+XJO3bt0+S5PP5VFVVFVHT+Nrn812wxuPxnPfqiyS53W7n6ajGDQAAdE6X/T0wDQ0Nqq2tbbKttLRUkpSYmChJ8vv9KisrU3V1tVNTUFAgj8fjfAzl9/tVWFgY0U9BQUHEfTYAAODq1qKPkHJycjR+/Hj1799fx44d06pVq7R161Zt3rxZ+/fv16pVq3TfffepT58+2r17t2bPnq3Ro0crNTVVkjR27FilpKTo8ccf19KlSxUMBrVgwQJlZWXJ7XZLkqZPn65XX31Vc+fO1ZNPPqktW7ZozZo1ys/Pb/3ZAwAAK7UowFRXV2vSpEk6cuSIvF6vUlNTtXnzZv30pz/VoUOH9NFHH+nll1/WiRMnlJycrAkTJmjBggXO8V26dNH69es1Y8YM+f1+XXPNNZo8eXLE98YMGjRI+fn5mj17tpYtW6Z+/frpjTfe4DtgAACA47K/B6aj4ntgAACwT5t/DwwAAEB7IcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYJ0WBZgVK1YoNTVVHo9HHo9Hfr9fGzdudNpPnTqlrKws9enTRz179tSECRNUVVUV0UdlZaUyMzMVExOj+Ph4zZkzR2fOnImo2bp1q2655Ra53W4NHjxYeXl5lz5DAADQ6bQowPTr109LlixRSUmJvvjiC/3kJz/RAw88oPLycknS7Nmz9fvf/15r165VUVGRDh8+rIceesg5vr6+XpmZmTp9+rS2b9+ut99+W3l5eVq4cKFTc+DAAWVmZuree+9VaWmpZs2apaeeekqbN29upSkDAADrmcvUu3dv88Ybb5iamhrTrVs3s3btWqftq6++MpJMcXGxMcaYDRs2mOjoaBMMBp2aFStWGI/HY2pra40xxsydO9cMGzYs4hwPP/ywCQQCLRpXKBQykkwoFLrUqV3QgHnr26RfAACuZs19/77ke2Dq6+u1evVqnThxQn6/XyUlJaqrq1NGRoZTM3ToUPXv31/FxcWSpOLiYo0YMUIJCQlOTSAQUDgcdq7iFBcXR/TRWNPYBwAAQNeWHlBWVia/369Tp06pZ8+eWrdunVJSUlRaWiqXy6XY2NiI+oSEBAWDQUlSMBiMCC+N7Y1tF6oJh8M6efKkevTo0eS4amtrVVtb67wOh8MtnRoAALBEi6/ADBkyRKWlpdq5c6dmzJihyZMna+/evW0xthbJzc2V1+t1tuTk5PYeEgAAaCMtDjAul0uDBw9WWlqacnNzNXLkSC1btkw+n0+nT59WTU1NRH1VVZV8Pp8kyefz/eippMbXF6vxeDznvfoiSTk5OQqFQs526NChlk4NAABY4rK/B6ahoUG1tbVKS0tTt27dVFhY6LRVVFSosrJSfr9fkuT3+1VWVqbq6mqnpqCgQB6PRykpKU7N2X001jT2cT5ut9t5vLtxAwAAnVOL7oHJycnR+PHj1b9/fx07dkyrVq3S1q1btXnzZnm9Xk2ZMkXZ2dmKi4uTx+PR008/Lb/frzvuuEOSNHbsWKWkpOjxxx/X0qVLFQwGtWDBAmVlZcntdkuSpk+frldffVVz587Vk08+qS1btmjNmjXKz89v/dkDAAArtSjAVFdXa9KkSTpy5Ii8Xq9SU1O1efNm/fSnP5UkvfTSS4qOjtaECRNUW1urQCCg1157zTm+S5cuWr9+vWbMmCG/369rrrlGkydP1nPPPefUDBo0SPn5+Zo9e7aWLVumfv366Y033lAgEGilKQMAANtFGWNMew+iLYTDYXm9XoVCoTb5OGng/HwdXJLZ6v0CAHA1a+77N38LCQAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6LQowubm5uu2229SrVy/Fx8frwQcfVEVFRUTNPffco6ioqIht+vTpETWVlZXKzMxUTEyM4uPjNWfOHJ05cyaiZuvWrbrlllvkdrs1ePBg5eXlXdoMAQBAp9OiAFNUVKSsrCzt2LFDBQUFqqur09ixY3XixImIuqlTp+rIkSPOtnTpUqetvr5emZmZOn36tLZv3663335beXl5WrhwoVNz4MABZWZm6t5771VpaalmzZqlp556Sps3b77M6QIAgM6ga0uKN23aFPE6Ly9P8fHxKikp0ejRo539MTEx8vl8Tfbx4Ycfau/evfroo4+UkJCgUaNG6fnnn9e8efO0ePFiuVwuvf766xo0aJD+/d//XZJ000036dNPP9VLL72kQCDQ0jkCAIBO5rLugQmFQpKkuLi4iP0rV65U3759NXz4cOXk5Oj777932oqLizVixAglJCQ4+wKBgMLhsMrLy52ajIyMiD4DgYCKi4vPO5ba2lqFw+GIDQAAdE4tugJztoaGBs2aNUt33nmnhg8f7ux/7LHHNGDAACUlJWn37t2aN2+eKioq9N5770mSgsFgRHiR5LwOBoMXrAmHwzp58qR69Ojxo/Hk5ubq2WefvdTpAAAAi1xygMnKytKePXv06aefRuyfNm2a8/OIESOUmJioMWPGaP/+/br++usvfaQXkZOTo+zsbOd1OBxWcnJym50PAAC0n0v6CGnmzJlav369Pv74Y/Xr1++Ctenp6ZKkffv2SZJ8Pp+qqqoiahpfN943c74aj8fT5NUXSXK73fJ4PBEbAADonFoUYIwxmjlzptatW6ctW7Zo0KBBFz2mtLRUkpSYmChJ8vv9KisrU3V1tVNTUFAgj8ejlJQUp6awsDCin4KCAvn9/pYMFwAAdFItCjBZWVn6r//6L61atUq9evVSMBhUMBjUyZMnJUn79+/X888/r5KSEh08eFAffPCBJk2apNGjRys1NVWSNHbsWKWkpOjxxx/XH//4R23evFkLFixQVlaW3G63JGn69On6n//5H82dO1d/+tOf9Nprr2nNmjWaPXt2K08fAABYybSApCa3t956yxhjTGVlpRk9erSJi4szbrfbDB482MyZM8eEQqGIfg4ePGjGjx9vevToYfr27WueeeYZU1dXF1Hz8ccfm1GjRhmXy2Wuu+465xzNFQqFjKQfnbu1DJi3vk36BQDgatbc9+8oY4xpv/jUdsLhsLxer0KhUJvcDzNwfr4OLsls9X4BALiaNff9m7+FBAAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6LQowubm5uu2229SrVy/Fx8frwQcfVEVFRUTNqVOnlJWVpT59+qhnz56aMGGCqqqqImoqKyuVmZmpmJgYxcfHa86cOTpz5kxEzdatW3XLLbfI7XZr8ODBysvLu7QZAgCATqdFAaaoqEhZWVnasWOHCgoKVFdXp7Fjx+rEiRNOzezZs/X73/9ea9euVVFRkQ4fPqyHHnrIaa+vr1dmZqZOnz6t7du36+2331ZeXp4WLlzo1Bw4cECZmZm69957VVpaqlmzZumpp57S5s2bW2HKAADAeuYyVFdXG0mmqKjIGGNMTU2N6datm1m7dq1T89VXXxlJpri42BhjzIYNG0x0dLQJBoNOzYoVK4zH4zG1tbXGGGPmzp1rhg0bFnGuhx9+2AQCgWaPLRQKGUkmFApd8vwuZMC89W3SLwAAV7Pmvn9f1j0woVBIkhQXFydJKikpUV1dnTIyMpyaoUOHqn///iouLpYkFRcXa8SIEUpISHBqAoGAwuGwysvLnZqz+2isaeyjKbW1tQqHwxEbAADonC45wDQ0NGjWrFm68847NXz4cElSMBiUy+VSbGxsRG1CQoKCwaBTc3Z4aWxvbLtQTTgc1smTJ5scT25urrxer7MlJydf6tQAAEAHd8kBJisrS3v27NHq1atbczyXLCcnR6FQyNkOHTrU3kMCAABtpOulHDRz5kytX79e27ZtU79+/Zz9Pp9Pp0+fVk1NTcRVmKqqKvl8Pqdm165dEf01PqV0ds25Ty5VVVXJ4/GoR48eTY7J7XbL7XZfynQAAIBlWnQFxhijmTNnat26ddqyZYsGDRoU0Z6WlqZu3bqpsLDQ2VdRUaHKykr5/X5Jkt/vV1lZmaqrq52agoICeTwepaSkODVn99FY09gHAAC4urXoCkxWVpZWrVql3/3ud+rVq5dzz4rX61WPHj3k9Xo1ZcoUZWdnKy4uTh6PR08//bT8fr/uuOMOSdLYsWOVkpKixx9/XEuXLlUwGNSCBQuUlZXlXEGZPn26Xn31Vc2dO1dPPvmktmzZojVr1ig/P7+Vpw8AAKzUkkebJDW5vfXWW07NyZMnzS9+8QvTu3dvExMTY37+85+bI0eORPRz8OBBM378eNOjRw/Tt29f88wzz5i6urqImo8//tiMGjXKuFwuc91110Wcozl4jBoAAPs09/07yhhj2i8+tZ1wOCyv16tQKCSPx9Pq/Q+cn6+DSzJbvV8AAK5mzX3/5m8hAQAA6xBgAACAdQgwl2ngfG4sBgDgSiPAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6LQ4w27Zt0/3336+kpCRFRUXp/fffj2j/+7//e0VFRUVs48aNi6g5evSoJk6cKI/Ho9jYWE2ZMkXHjx+PqNm9e7fuvvtude/eXcnJyVq6dGnLZwcAADqlFgeYEydOaOTIkVq+fPl5a8aNG6cjR4442zvvvBPRPnHiRJWXl6ugoEDr16/Xtm3bNG3aNKc9HA5r7NixGjBggEpKSvTCCy9o8eLF+s1vftPS4QIAgE6oa0sPGD9+vMaPH3/BGrfbLZ/P12TbV199pU2bNunzzz/XrbfeKkn69a9/rfvuu0//9m//pqSkJK1cuVKnT5/Wm2++KZfLpWHDhqm0tFQvvvhiRNABAABXpza5B2br1q2Kj4/XkCFDNGPGDH333XdOW3FxsWJjY53wIkkZGRmKjo7Wzp07nZrRo0fL5XI5NYFAQBUVFfrLX/7S5Dlra2sVDocjNgAA0Dm1eoAZN26c/vM//1OFhYX613/9VxUVFWn8+PGqr6+XJAWDQcXHx0cc07VrV8XFxSkYDDo1CQkJETWNrxtrzpWbmyuv1+tsycnJrT01AADQQbT4I6SLeeSRR5yfR4wYodTUVF1//fXaunWrxowZ09qnc+Tk5Cg7O9t5HQ6HCTEAAHRSbf4Y9XXXXae+fftq3759kiSfz6fq6uqImjNnzujo0aPOfTM+n09VVVURNY2vz3dvjdvtlsfjidgAAEDn1OYB5s9//rO+++47JSYmSpL8fr9qampUUlLi1GzZskUNDQ1KT093arZt26a6ujqnpqCgQEOGDFHv3r3besgAAKCDa3GAOX78uEpLS1VaWipJOnDggEpLS1VZWanjx49rzpw52rFjhw4ePKjCwkI98MADGjx4sAKBgCTppptu0rhx4zR16lTt2rVLn332mWbOnKlHHnlESUlJkqTHHntMLpdLU6ZMUXl5ud59910tW7Ys4iMiAABw9WpxgPniiy9088036+abb5YkZWdn6+abb9bChQvVpUsX7d69Wz/72c904403asqUKUpLS9Mnn3wit9vt9LFy5UoNHTpUY8aM0X333ae77ror4jtevF6vPvzwQx04cEBpaWl65plntHDhQh6hBgAAki7hJt577rlHxpjztm/evPmifcTFxWnVqlUXrElNTdUnn3zS0uEBAICrAH8LCQAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1Whxgtm3bpvvvv19JSUmKiorS+++/H9FujNHChQuVmJioHj16KCMjQ19//XVEzdGjRzVx4kR5PB7FxsZqypQpOn78eETN7t27dffdd6t79+5KTk7W0qVLWz47AADQKbU4wJw4cUIjR47U8uXLm2xfunSpXnnlFb3++uvauXOnrrnmGgUCAZ06dcqpmThxosrLy1VQUKD169dr27ZtmjZtmtMeDoc1duxYDRgwQCUlJXrhhRe0ePFi/eY3v7mEKQIAgE7HXAZJZt26dc7rhoYG4/P5zAsvvODsq6mpMW6327zzzjvGGGP27t1rJJnPP//cqdm4caOJiooy33zzjTHGmNdee8307t3b1NbWOjXz5s0zQ4YMafbYQqGQkWRCodClTu+CBsxbH/FfAABw+Zr7/t2q98AcOHBAwWBQGRkZzj6v16v09HQVFxdLkoqLixUbG6tbb73VqcnIyFB0dLR27tzp1IwePVoul8upCQQCqqio0F/+8pcmz11bW6twOByxAQCAzqlVA0wwGJQkJSQkROxPSEhw2oLBoOLj4yPau3btqri4uIiapvo4+xznys3Nldfrdbbk5OTLnxAAAOiQOs1TSDk5OQqFQs526NCh9h4SAABoI60aYHw+nySpqqoqYn9VVZXT5vP5VF1dHdF+5swZHT16NKKmqT7OPse53G63PB5PxAYAADqnVg0wgwYNks/nU2FhobMvHA5r586d8vv9kiS/36+amhqVlJQ4NVu2bFFDQ4PS09Odmm3btqmurs6pKSgo0JAhQ9S7d+/WHDIAALBQiwPM8ePHVVpaqtLSUkk/3LhbWlqqyspKRUVFadasWfqXf/kXffDBByorK9OkSZOUlJSkBx98UJJ00003ady4cZo6dap27dqlzz77TDNnztQjjzyipKQkSdJjjz0ml8ulKVOmqLy8XO+++66WLVum7OzsVps4AACwV9eWHvDFF1/o3nvvdV43horJkycrLy9Pc+fO1YkTJzRt2jTV1NTorrvu0qZNm9S9e3fnmJUrV2rmzJkaM2aMoqOjNWHCBL3yyitOu9fr1YcffqisrCylpaWpb9++WrhwYcR3xQAAgKtXlDHGtPcg2kI4HJbX61UoFGqT+2EGzs/XwSWZzn8BAMDla+77d6d5CgkAAFw9CDAAAMA6BBgAAGAdAkwrGTg/v72HAADAVYMA0woILwAAXFkEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAtCKeRgIA4MogwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYNrIwPn57T0EAAA6LQIMAACwTqsHmMWLFysqKipiGzp0qNN+6tQpZWVlqU+fPurZs6cmTJigqqqqiD4qKyuVmZmpmJgYxcfHa86cOTpz5kxrDxUAAFiqa1t0OmzYMH300Uf//yRd//9pZs+erfz8fK1du1Zer1czZ87UQw89pM8++0ySVF9fr8zMTPl8Pm3fvl1HjhzRpEmT1K1bN/3qV79qi+ECAADLtEmA6dq1q3w+34/2h0Ih/cd//IdWrVqln/zkJ5Kkt956SzfddJN27NihO+64Qx9++KH27t2rjz76SAkJCRo1apSef/55zZs3T4sXL5bL5WqLIQMAAIu0yT0wX3/9tZKSknTddddp4sSJqqyslCSVlJSorq5OGRkZTu3QoUPVv39/FRcXS5KKi4s1YsQIJSQkODWBQEDhcFjl5eXnPWdtba3C4XDEBgAAOqdWDzDp6enKy8vTpk2btGLFCh04cEB33323jh07pmAwKJfLpdjY2IhjEhISFAwGJUnBYDAivDS2N7adT25urrxer7MlJye37sQAAECH0eoBZvz48fq7v/s7paamKhAIaMOGDaqpqdGaNWta+1QRcnJyFAqFnO3QoUNter7m4nFqAABaX5s/Rh0bG6sbb7xR+/btk8/n0+nTp1VTUxNRU1VV5dwz4/P5fvRUUuPrpu6raeR2u+XxeCK29kJoAQCgbbV5gDl+/Lj279+vxMREpaWlqVu3biosLHTaKyoqVFlZKb/fL0ny+/0qKytTdXW1U1NQUCCPx6OUlJS2Hi4AALBAqz+F9A//8A+6//77NWDAAB0+fFiLFi1Sly5d9Oijj8rr9WrKlCnKzs5WXFycPB6Pnn76afn9ft1xxx2SpLFjxyolJUWPP/64li5dqmAwqAULFigrK0tut7u1hwsAACzU6gHmz3/+sx599FF99913uvbaa3XXXXdpx44duvbaayVJL730kqKjozVhwgTV1tYqEAjotddec47v0qWL1q9frxkzZsjv9+uaa67R5MmT9dxzz7X2UAEAgKVaPcCsXr36gu3du3fX8uXLtXz58vPWDBgwQBs2bGjtobWbgfPzdXBJZnsPAwCAToO/hdSGuJkXAIC2QYABAADWIcBcIVyNAQCg9RBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDBXEN/GCwBA6yDAAAAA6xBgAACAdQgwAADAOgSYK4z7YAAAuHwEmHZEmAEA4NIQYNoJ4QUAgEtHgAEAANYhwLSzgfPzuRoDAEALEWAAAIB1CDCXgCsmAAC0LwIMAACwDgGmg2i8qsPVHQAALo4AAwAArEOA6UC4+gIAQPMQYDqgs4MMoQYAgB8jwFjk7PtkCDYAgKtZ1/YeAC6OsAIAQCSuwFiGMAMAAAGmUzj3oyVCDgCgsyPAdGItDTIEHwCALQgwnURzw0dzrtAQZAAAHR038XZS54aQC70+uCTziowJAIDWQoBBhKb+pMHZAefcINTc8DNwfn6TtefbDwDAhfAREi6qOR85NfXlexe66tNUn83ddynjAwB0Lh06wCxfvlwDBw5U9+7dlZ6erl27drX3kHCOloabpo49u6Y5IeZ8Qai5QeZ852hOCAMAdAwdNsC8++67ys7O1qJFi/SHP/xBI0eOVCAQUHV1dXsPDc3QGm/8lxJOGn8+X+i5WDC5UJhqqo/zBbSWjvdiNS1B6AJwNeiwAebFF1/U1KlT9cQTTyglJUWvv/66YmJi9Oabb7b30NBOWvsJqkt5o2/OlZmLBZzzXXk63xWg5n63z4WuLDW1XaqLzb05V8wuZ+0v1GdL59aWXzXQVmsM4Acd8ibe06dPq6SkRDk5Oc6+6OhoZWRkqLi4uMljamtrVVtb67wOhUKSpHA43Orja6j93um78edGTe07V3Nrzj4XNXbUDF+0ucn2/rPXas+zgcs6V//ZayXJ6aepczXWtMaYmzrXnmcDaqj93jnP+cZz9lgbar+PGE9j29l9nN3XhcbTuI7njuHcvsLh8AX7OXdswxdt/tG5mzP3883/3HGdr+bs/efWnP07c/YYztfP+WqaWtMLjflC8z/3HI3Ot25n1zXnXE311VQ/547lYuvT1JjP7etC/xbnjqWpmqb6P3dO557jYuc935ibmuvFxnyhPlrico5trsb/vzDGXLjQdEDffPONkWS2b98esX/OnDnm9ttvb/KYRYsWGUlsbGxsbGxsnWA7dOjQBbNCh7wCcylycnKUnZ3tvG5oaNDRo0fVp08fRUVFtdp5wuGwkpOTdejQIXk8nlbrF5FY5yuDdb4yWOcrg3W+Mtp6nY0xOnbsmJKSki5Y1yEDTN++fdWlSxdVVVVF7K+qqpLP52vyGLfbLbfbHbEvNja2rYYoj8fD/0CuANb5ymCdrwzW+cpgna+Mtlxnr9d70ZoOeROvy+VSWlqaCgsLnX0NDQ0qLCyU3+9vx5EBAICOoENegZGk7OxsTZ48Wbfeeqtuv/12vfzyyzpx4oSeeOKJ9h4aAABoZx02wDz88MP69ttvtXDhQgWDQY0aNUqbNm1SQkJCu47L7XZr0aJFP/q4Cq2Ldb4yWOcrg3W+MljnK6OjrHOUMRd7TgkAAKBj6ZD3wAAAAFwIAQYAAFiHAAMAAKxDgAEAANYhwLTA8uXLNXDgQHXv3l3p6enatWtXew/JKtu2bdP999+vpKQkRUVF6f33349oN8Zo4cKFSkxMVI8ePZSRkaGvv/46oubo0aOaOHGiPB6PYmNjNWXKFB0/fvwKzqLjy83N1W233aZevXopPj5eDz74oCoqKiJqTp06paysLPXp00c9e/bUhAkTfvTFkZWVlcrMzFRMTIzi4+M1Z84cnTlz5kpOpUNbsWKFUlNTnS/z8vv92rhxo9POGreNJUuWKCoqSrNmzXL2sdaXb/HixYqKiorYhg4d6rR3yDVulT9edBVYvXq1cblc5s033zTl5eVm6tSpJjY21lRVVbX30KyxYcMG80//9E/mvffeM5LMunXrItqXLFlivF6vef/9980f//hH87Of/cwMGjTInDx50qkZN26cGTlypNmxY4f55JNPzODBg82jjz56hWfSsQUCAfPWW2+ZPXv2mNLSUnPfffeZ/v37m+PHjzs106dPN8nJyaawsNB88cUX5o477jB//dd/7bSfOXPGDB8+3GRkZJgvv/zSbNiwwfTt29fk5OS0x5Q6pA8++MDk5+eb//7v/zYVFRXmH//xH023bt3Mnj17jDGscVvYtWuXGThwoElNTTW//OUvnf2s9eVbtGiRGTZsmDly5Iizffvtt057R1xjAkwz3X777SYrK8t5XV9fb5KSkkxubm47jspe5waYhoYG4/P5zAsvvODsq6mpMW6327zzzjvGGGP27t1rJJnPP//cqdm4caOJiooy33zzzRUbu22qq6uNJFNUVGSM+WFdu3XrZtauXevUfPXVV0aSKS4uNsb8EDajo6NNMBh0alasWGE8Ho+pra29shOwSO/evc0bb7zBGreBY8eOmRtuuMEUFBSYv/mbv3ECDGvdOhYtWmRGjhzZZFtHXWM+QmqG06dPq6SkRBkZGc6+6OhoZWRkqLi4uB1H1nkcOHBAwWAwYo29Xq/S09OdNS4uLlZsbKxuvfVWpyYjI0PR0dHauXPnFR+zLUKhkCQpLi5OklRSUqK6urqItR46dKj69+8fsdYjRoyI+OLIQCCgcDis8vLyKzh6O9TX12v16tU6ceKE/H4/a9wGsrKylJmZGbGmEr/Prenrr79WUlKSrrvuOk2cOFGVlZWSOu4ad9hv4u1I/u///k/19fU/+hbghIQE/elPf2qnUXUuwWBQkppc48a2YDCo+Pj4iPauXbsqLi7OqUGkhoYGzZo1S3feeaeGDx8u6Yd1dLlcP/pjp+eudVP/Fo1t+EFZWZn8fr9OnTqlnj17at26dUpJSVFpaSlr3IpWr16tP/zhD/r8889/1Mbvc+tIT09XXl6ehgwZoiNHjujZZ5/V3XffrT179nTYNSbAAJ1YVlaW9uzZo08//bS9h9IpDRkyRKWlpQqFQvrtb3+ryZMnq6ioqL2H1akcOnRIv/zlL1VQUKDu3bu393A6rfHjxzs/p6amKj09XQMGDNCaNWvUo0ePdhzZ+fERUjP07dtXXbp0+dEd11VVVfL5fO00qs6lcR0vtMY+n0/V1dUR7WfOnNHRo0f5d2jCzJkztX79en388cfq16+fs9/n8+n06dOqqamJqD93rZv6t2hsww9cLpcGDx6stLQ05ebmauTIkVq2bBlr3IpKSkpUXV2tW265RV27dlXXrl1VVFSkV155RV27dlVCQgJr3QZiY2N14403at++fR3295kA0wwul0tpaWkqLCx09jU0NKiwsFB+v78dR9Z5DBo0SD6fL2KNw+Gwdu7c6ayx3+9XTU2NSkpKnJotW7aooaFB6enpV3zMHZUxRjNnztS6deu0ZcsWDRo0KKI9LS1N3bp1i1jriooKVVZWRqx1WVlZRGAsKCiQx+NRSkrKlZmIhRoaGlRbW8sat6IxY8aorKxMpaWlznbrrbdq4sSJzs+sdes7fvy49u/fr8TExI77+9wmtwZ3QqtXrzZut9vk5eWZvXv3mmnTppnY2NiIO65xYceOHTNffvml+fLLL40k8+KLL5ovv/zS/O///q8x5ofHqGNjY83vfvc7s3v3bvPAAw80+Rj1zTffbHbu3Gk+/fRTc8MNN/AY9TlmzJhhvF6v2bp1a8Qjkd9//71TM336dNO/f3+zZcsW88UXXxi/32/8fr/T3vhI5NixY01paanZtGmTufbaa3ns9Czz5883RUVF5sCBA2b37t1m/vz5Jioqynz44YfGGNa4LZ39FJIxrHVreOaZZ8zWrVvNgQMHzGeffWYyMjJM3759TXV1tTGmY64xAaYFfv3rX5v+/fsbl8tlbr/9drNjx472HpJVPv74YyPpR9vkyZONMT88Sv3P//zPJiEhwbjdbjNmzBhTUVER0cd3331nHn30UdOzZ0/j8XjME088YY4dO9YOs+m4mlpjSeatt95yak6ePGl+8YtfmN69e5uYmBjz85//3Bw5ciSin4MHD5rx48ebHj16mL59+5pnnnnG1NXVXeHZdFxPPvmkGTBggHG5XObaa681Y8aMccKLMaxxWzo3wLDWl+/hhx82iYmJxuVymb/6q78yDz/8sNm3b5/T3hHXOMoYY9rm2g4AAEDb4B4YAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKzz/wB+k1yRJab1hQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_elements, counts = np.unique(X, return_counts=True)\n",
    "occurrences_dict = dict(zip(unique_elements, counts))\n",
    "\n",
    "keys = list(occurrences_dict.keys())[:500]\n",
    "values = list(occurrences_dict.values())[:500]\n",
    "\n",
    "plt.bar(keys, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4Y9Tr9dIClo",
    "outputId": "4450cdaf-6cfe-459a-f97b-133e96ece774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (200, 100)\n",
      "X_test shape: (200, 70)\n"
     ]
    }
   ],
   "source": [
    "# Train - test split\n",
    "LENGTH_TRAIN = 100\n",
    "X_train = X[:, :LENGTH_TRAIN]\n",
    "X_test = X[:, LENGTH_TRAIN:]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty6AqZwAUlsj"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "23wnOVFHUlsj"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, model_length):\n",
    "        self.topics=topics\n",
    "        self.model_length=model_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.topics)-self.model_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_sequence=torch.tensor(self.topics[index:index+self.model_length, :])\n",
    "        target_sequence=torch.tensor(self.topics[index+1:index+self.model_length+1, :])\n",
    "\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y7qp8RODUlsj"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model_length, batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size #input size (NUM_TOPICS)\n",
    "        self.hidden_size = hidden_size #number of hidden neurons\n",
    "        self.output_size = output_size #output size (NUM_TOPICS)\n",
    "        self.model_length = model_length\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        output = self.dropout(output)\n",
    "        output=self.fc(output)\n",
    "        return output, state\n",
    "\n",
    "    def init_state(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_size), #(NUM_LAYERS, BATCH SIZE, NUM_NEURONES)\n",
    "                torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "\n",
    "    def train_model(self, dataset, optimizer, criterion):\n",
    "        self.train()\n",
    "        state_h, state_c = self.init_state()\n",
    "        lst_output=[]\n",
    "        lst_y_true=[]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for t, (x, y) in enumerate(dataset):\n",
    "            output , (state_h, state_c) = self.forward(x, (state_h, state_c))\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            lst_output.append(output[:, -1, :])\n",
    "            lst_y_true.append(y[:, -1, :])\n",
    "        torch_output = torch.stack(lst_output).view(-1, self.output_size)  \n",
    "        torch_y_true = torch.stack(lst_y_true).view(-1, self.output_size)\n",
    "        loss = criterion(torch_output, torch_y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()           \n",
    "\n",
    "    def predict_next_probability(self, input_sequence):\n",
    "        self.eval()\n",
    "        state_h, state_c = self.init_state()\n",
    "        with torch.no_grad():\n",
    "            for t in range(len(input_sequence)):\n",
    "                input_t = input_sequence[t].unsqueeze(0).unsqueeze(0)\n",
    "                _, (state_h, state_c) = self(input_t, (state_h, state_c))\n",
    "\n",
    "        input_t = input_sequence[-1].unsqueeze(0).unsqueeze(0)\n",
    "        output, _ = self.forward(input_t, (state_h, state_c))\n",
    "        probabilities = F.softmax(output[:, -1, :], dim=1).detach().squeeze(0)\n",
    "        return probabilities\n",
    "\n",
    "    def sample_next_z(self, input_sequence):\n",
    "        self.eval()\n",
    "        proba = self.predict_next_probability(input_sequence)\n",
    "        return torch.multinomial(proba, 1).item()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqrifMS5Ulsk"
   },
   "source": [
    "## SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DfZO1YK1Ulsk"
   },
   "outputs": [],
   "source": [
    "class SSMPytorch:\n",
    "    def __init__(self, num_words, num_topics):\n",
    "        self.num_words = num_words\n",
    "        self.num_topics = num_topics\n",
    "        self.phi = torch.randn(num_words, num_topics) * 0.01\n",
    "        self.phi = torch.exp(self.phi - torch.max(self.phi, dim=0, keepdim=True).values)\n",
    "        self.phi = self.phi / torch.sum(self.phi, dim=0, keepdim=True)\n",
    "\n",
    "    def compute_MLE_SSM(self, ech_x, ech_z):\n",
    "        n_samples = ech_x.shape[0]\n",
    "        proba_matrix = torch.zeros((self.num_words, self.num_topics), dtype=torch.float)\n",
    "        for i in range(n_samples):\n",
    "            index_x = ech_x[i] - 1\n",
    "            index_z = ech_z[i] - 1\n",
    "            proba_matrix[index_x, index_z] += 1.0\n",
    "        proba_matrix = proba_matrix + 1e-6\n",
    "        row_sums = proba_matrix.sum(dim=0, keepdim=True)\n",
    "        proba_matrix_normalized = proba_matrix / row_sums\n",
    "        self.phi = proba_matrix_normalized\n",
    "\n",
    "    def predict_proba(self, z_t):\n",
    "        return self.phi[:, int(z_t) - 1]\n",
    "\n",
    "    def sample_xt(self, z_t):\n",
    "        proba = self.predict_proba(z_t)\n",
    "        sampled_xt = torch.multinomial(proba, 1).item() + 1\n",
    "        return sampled_xt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33fdzrVuUlsl"
   },
   "source": [
    "## Particle Gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RYDvZJPNUlsl"
   },
   "outputs": [],
   "source": [
    "def compute_alpha_unnormalized(z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    z_1_t_minus_1 = z_1_t_minus_1 - 1\n",
    "    z_one_hot = F.one_hot(z_1_t_minus_1, num_classes=num_topics).float()\n",
    "    softmax = lstm.predict_next_probability(z_one_hot).detach()\n",
    "    phi = ssm.phi\n",
    "    alpha = torch.tensor([torch.matmul(softmax, phi[j, :]) for j in range(num_voc)])\n",
    "    return alpha\n",
    "\n",
    "def compute_alpha_normalized(z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n",
    "    num = compute_alpha_unnormalized(z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\n",
    "    denom = torch.sum(num) + 1e-6\n",
    "    return num / denom\n",
    "\n",
    "def compute_gamma_unnormalized(xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    z_1_t_minus_1 = z_1_t_minus_1 - 1\n",
    "    z_one_hot = F.one_hot(z_1_t_minus_1, num_classes=num_topics).float()\n",
    "    softmax = lstm.predict_next_probability(z_one_hot).detach()\n",
    "    phi = ssm.phi\n",
    "    phi_xt = phi[xt - 1, :]\n",
    "    return torch.mul(softmax, phi_xt)\n",
    "\n",
    "def compute_gamma_normalized(xt, z_1_t_minus_1, num_topics, lstm, ssm):\n",
    "    num = compute_gamma_unnormalized(xt, z_1_t_minus_1, num_topics, lstm, ssm)\n",
    "    denom = torch.sum(num) + 1e-6\n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OldruShfhzBV"
   },
   "outputs": [],
   "source": [
    "def particle_gibbs(x, previous_z_1_T_star, P, num_topics, num_words, T, lstm_model, ssm_model):\n",
    "    # Init\n",
    "    Z_matrix = torch.zeros((P, T + 1), dtype=torch.long)\n",
    "    alpha_matrix = torch.zeros((P, T + 1), dtype=torch.float)\n",
    "    ancestor_matrix = torch.ones((P, T + 1), dtype=torch.long)\n",
    "\n",
    "    # t=0\n",
    "    Z_matrix[:, 0] = torch.randint(1, num_topics + 1, (P,))\n",
    "    alpha_matrix[:, 0] = torch.full((P,), 1 / P)\n",
    "\n",
    "    # t=1\n",
    "    t = 1\n",
    "    ancestor_matrix[0, t - 1] = torch.tensor(1)\n",
    "    Z_matrix[0, 1:t + 1] = previous_z_1_T_star[:t]\n",
    "\n",
    "    for p in range(2, P + 1):\n",
    "        alpha_t_minus_1_p = alpha_matrix[:, t - 1]\n",
    "        a_t_minus_1_p = torch.multinomial(alpha_t_minus_1_p, 1).item()+1\n",
    "        ancestor_matrix[p - 1, t - 1] = a_t_minus_1_p\n",
    "        z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p) - 1, 0]\n",
    "        z_1_t_minus_1_a_t_minus_1_p = torch.tensor([z_1_t_minus_1_a_t_minus_1_p])\n",
    "        gamma_t_p = compute_gamma_normalized(xt=x[t - 1],\n",
    "                                             z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                             num_topics=num_topics,\n",
    "                                             lstm=lstm_model,\n",
    "                                             ssm=ssm_model)\n",
    "        Z_matrix[p - 1, 1:t + 1] = torch.multinomial(gamma_t_p, 1).item()+1\n",
    "\n",
    "    for p in range(1, P + 1):\n",
    "        a_t_minus_1_p = ancestor_matrix[p - 1, t - 1]\n",
    "        z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p) - 1, 0]\n",
    "        z_1_t_minus_1_a_t_minus_1_p = torch.tensor([z_1_t_minus_1_a_t_minus_1_p])\n",
    "        alpha_t_p = compute_alpha_normalized(z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                             num_topics=num_topics,\n",
    "                                             num_voc=num_words,\n",
    "                                             lstm=lstm_model,\n",
    "                                             ssm=ssm_model)\n",
    "        alpha_t_p = alpha_t_p[x[t - 1] - 1]\n",
    "        alpha_matrix[p - 1, t] = alpha_t_p\n",
    "\n",
    "    alpha_matrix[:, t] = alpha_matrix[:, t] / (alpha_matrix[:, t].sum() + 1e-6)\n",
    "\n",
    "    for t in range(2, T + 1):\n",
    "        a_t_minus_1 = torch.tensor(1)\n",
    "        z_1_t = previous_z_1_T_star[:t]\n",
    "        ancestor_matrix[0, t - 1] = a_t_minus_1\n",
    "        Z_matrix[0, 1:t + 1] = z_1_t\n",
    "\n",
    "        for p in range(2, P + 1):\n",
    "            alpha_t_minus_1_p = alpha_matrix[:, t - 1]\n",
    "            a_t_minus_1_p = torch.multinomial(alpha_t_minus_1_p, 1).item()+1\n",
    "            ancestor_matrix[p - 1, t - 1] = a_t_minus_1_p\n",
    "            z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p) - 1, 1:t]\n",
    "            gamma_t_p = compute_gamma_normalized(xt=x[t - 1],\n",
    "                                                 z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                 num_topics=num_topics,\n",
    "                                                 lstm=lstm_model,\n",
    "                                                 ssm=ssm_model)\n",
    "\n",
    "            z_t_p = torch.multinomial(gamma_t_p, 1).item()+1\n",
    "            z_1_t_p = torch.cat([z_1_t_minus_1_a_t_minus_1_p, torch.tensor([z_t_p])])\n",
    "            Z_matrix[p - 1, 1:t + 1] = z_1_t_p\n",
    "\n",
    "        for p in range(1, P + 1):\n",
    "            a_t_minus_1_p = ancestor_matrix[p - 1, t - 1]\n",
    "            z_1_t_minus_1_a_t_minus_1_p = Z_matrix[int(a_t_minus_1_p) - 1, 1:(t - 1) + 1]\n",
    "            alpha_t_p = compute_alpha_normalized(z_1_t_minus_1=z_1_t_minus_1_a_t_minus_1_p,\n",
    "                                                 num_topics=num_topics,\n",
    "                                                 num_voc=num_words,\n",
    "                                                 lstm=lstm_model,\n",
    "                                                 ssm=ssm_model)\n",
    "            alpha_t_p = alpha_t_p[x[t - 1] - 1]\n",
    "            alpha_matrix[p - 1, t] = alpha_t_p\n",
    "\n",
    "        alpha_matrix[:, t] = alpha_matrix[:, t] / (alpha_matrix[:, t].sum() + 1e-6)\n",
    "\n",
    "    alpha_T = alpha_matrix[:, -1]\n",
    "    alpha_T = alpha_T / (alpha_T.sum() + 1e-6)\n",
    "    r = torch.multinomial(alpha_T, 1).item()+1\n",
    "    a_T_r = ancestor_matrix[int(r) - 1, -1]\n",
    "    z_1_T = Z_matrix[int(a_T_r) - 1, 1:]\n",
    "\n",
    "    return z_1_T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVJyJMZTUlsl"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "X0AKORMeUlsl"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "TRAIN_SEQUENCE_LENGTH = 100\n",
    "TEST_SEQUENCE_LENGTH = 70\n",
    "MODEL_LENGTH = 10\n",
    "NUM_TOPICS = 100\n",
    "NUM_WORDS = 5000\n",
    "N_EPOCHS = 5\n",
    "NUM_PARTICULES = 10\n",
    "N = 200\n",
    "\n",
    "lstm_model=LSTM(input_size=NUM_TOPICS, hidden_size=HIDDEN_SIZE, output_size=NUM_TOPICS, model_length=MODEL_LENGTH, batch_size=1)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = optim.Adam(lstm_model.parameters())\n",
    "ssm_model = SSMPytorch(num_words=NUM_WORDS, num_topics=NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08ig8h2-Uwbe",
    "outputId": "a198bd0d-7866-4648-db49-c2e4138050b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanis\\AppData\\Local\\Temp\\ipykernel_123632\\2488810157.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_sequence=torch.tensor(self.topics[index:index+self.model_length, :])\n",
      "C:\\Users\\yanis\\AppData\\Local\\Temp\\ipykernel_123632\\2488810157.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_sequence=torch.tensor(self.topics[index+1:index+self.model_length+1, :])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m shuffle_sample:\n\u001b[0;32m     13\u001b[0m     X_train_torch_i \u001b[38;5;241m=\u001b[39m X_train_torch[i]\n\u001b[1;32m---> 14\u001b[0m     z_star_train_i \u001b[38;5;241m=\u001b[39m particle_gibbs(X_train_torch_i, previous_z_1_T_star[i], NUM_PARTICULES, NUM_TOPICS, NUM_WORDS, TRAIN_SEQUENCE_LENGTH, lstm_model, ssm_model)\n\u001b[0;32m     15\u001b[0m     z_star_one_hot_train_i \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(z_star_train_i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39mNUM_TOPICS)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     16\u001b[0m     dataset_i\u001b[38;5;241m=\u001b[39mDataset(topics\u001b[38;5;241m=\u001b[39mz_star_one_hot_train_i, model_length\u001b[38;5;241m=\u001b[39mMODEL_LENGTH)\n",
      "Cell \u001b[1;32mIn[10], line 67\u001b[0m, in \u001b[0;36mparticle_gibbs\u001b[1;34m(x, previous_z_1_T_star, P, num_topics, num_words, T, lstm_model, ssm_model)\u001b[0m\n\u001b[0;32m     65\u001b[0m a_t_minus_1_p \u001b[38;5;241m=\u001b[39m ancestor_matrix[p \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     66\u001b[0m z_1_t_minus_1_a_t_minus_1_p \u001b[38;5;241m=\u001b[39m Z_matrix[\u001b[38;5;28mint\u001b[39m(a_t_minus_1_p) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m:(t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 67\u001b[0m alpha_t_p \u001b[38;5;241m=\u001b[39m compute_alpha_normalized(z_1_t_minus_1\u001b[38;5;241m=\u001b[39mz_1_t_minus_1_a_t_minus_1_p,\n\u001b[0;32m     68\u001b[0m                                      num_topics\u001b[38;5;241m=\u001b[39mnum_topics,\n\u001b[0;32m     69\u001b[0m                                      num_voc\u001b[38;5;241m=\u001b[39mnum_words,\n\u001b[0;32m     70\u001b[0m                                      lstm\u001b[38;5;241m=\u001b[39mlstm_model,\n\u001b[0;32m     71\u001b[0m                                      ssm\u001b[38;5;241m=\u001b[39mssm_model)\n\u001b[0;32m     72\u001b[0m alpha_t_p \u001b[38;5;241m=\u001b[39m alpha_t_p[x[t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     73\u001b[0m alpha_matrix[p \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, t] \u001b[38;5;241m=\u001b[39m alpha_t_p\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mcompute_alpha_normalized\u001b[1;34m(z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_alpha_normalized\u001b[39m(z_1_t_minus_1, num_topics, num_voc, lstm, ssm):\n\u001b[1;32m---> 10\u001b[0m     num \u001b[38;5;241m=\u001b[39m compute_alpha_unnormalized(z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\n\u001b[0;32m     11\u001b[0m     denom \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(num) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num \u001b[38;5;241m/\u001b[39m denom\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36mcompute_alpha_unnormalized\u001b[1;34m(z_1_t_minus_1, num_topics, num_voc, lstm, ssm)\u001b[0m\n\u001b[0;32m      4\u001b[0m softmax \u001b[38;5;241m=\u001b[39m lstm\u001b[38;5;241m.\u001b[39mpredict_next_probability(z_one_hot)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m      5\u001b[0m phi \u001b[38;5;241m=\u001b[39m ssm\u001b[38;5;241m.\u001b[39mphi\n\u001b[1;32m----> 6\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([torch\u001b[38;5;241m.\u001b[39mmatmul(softmax, phi[j, :]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_voc)])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m softmax \u001b[38;5;241m=\u001b[39m lstm\u001b[38;5;241m.\u001b[39mpredict_next_probability(z_one_hot)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m      5\u001b[0m phi \u001b[38;5;241m=\u001b[39m ssm\u001b[38;5;241m.\u001b[39mphi\n\u001b[1;32m----> 6\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([torch\u001b[38;5;241m.\u001b[39mmatmul(softmax, phi[j, :]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_voc)])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype = torch.long)\n",
    "X_test_torch = torch.tensor(X_test, dtype = torch.long)\n",
    "\n",
    "list_perplexity = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    shuffle_sample = random.sample(range(N), N)\n",
    "    start_time = time.time()\n",
    "    print(\"epoch: {}\".format(epoch+1))\n",
    "    previous_z_1_T_star = torch.randint(1, NUM_TOPICS + 1, (X_train_torch.shape[0], TRAIN_SEQUENCE_LENGTH))\n",
    "    z_star_train = []\n",
    "    for i in shuffle_sample:\n",
    "        X_train_torch_i = X_train_torch[i]\n",
    "        z_star_train_i = particle_gibbs(X_train_torch_i, previous_z_1_T_star[i], NUM_PARTICULES, NUM_TOPICS, NUM_WORDS, TRAIN_SEQUENCE_LENGTH, lstm_model, ssm_model)\n",
    "        z_star_one_hot_train_i = F.one_hot(z_star_train_i-1, num_classes=NUM_TOPICS).float()\n",
    "        dataset_i=Dataset(topics=z_star_one_hot_train_i, model_length=MODEL_LENGTH)\n",
    "        dataloader_i = DataLoader(dataset_i, batch_size=1)\n",
    "        lstm_model.train_model(dataloader_i, optimizer, criterion)\n",
    "        ssm_model.compute_MLE_SSM(X_train_torch_i, z_star_train_i)\n",
    "        z_star_train.append(z_star_train_i)\n",
    "    \n",
    "    z_star_train = torch.stack(z_star_train)\n",
    "    \n",
    "    z_pred = []\n",
    "    for i in shuffle_sample:\n",
    "        input_sequence = z_star_train[i]\n",
    "        input_seq_one_hot = F.one_hot(input_sequence-1, num_classes=NUM_TOPICS).float()\n",
    "        input_seq_one_hot = input_seq_one_hot\n",
    "        for t in range(TEST_SEQUENCE_LENGTH):\n",
    "            z_next = torch.tensor([lstm_model.sample_next_z(input_seq_one_hot)])\n",
    "            input_sequence = torch.cat([input_sequence, z_next])\n",
    "            input_seq_one_hot = F.one_hot(input_sequence-1, num_classes=NUM_TOPICS).float()\n",
    "        z_pred_i = input_sequence[-TEST_SEQUENCE_LENGTH:]\n",
    "        z_pred.append(z_pred_i)\n",
    "    z_pred = torch.stack(z_pred)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in shuffle_sample:\n",
    "        z_pred_i = z_pred[i]\n",
    "        proba_x_pred_i = []\n",
    "        for t in range(TEST_SEQUENCE_LENGTH):\n",
    "            z_pred_i_t = z_pred_i[t].item()\n",
    "            proba_x_pred_i_t = ssm_model.predict_proba(z_pred_i_t)\n",
    "            proba_x_pred_i.append(proba_x_pred_i_t)\n",
    "        proba_x_pred_i = torch.stack(proba_x_pred_i)\n",
    "        X_test_i = X_test_torch[i]\n",
    "        cross_entropy = F.cross_entropy(proba_x_pred_i, F.one_hot(X_test_i-1, num_classes=NUM_WORDS).float(), reduction = \"sum\")\n",
    "        loss += cross_entropy\n",
    "    mean_loss = loss/z_pred.shape[0]\n",
    "    mean_loss_item = mean_loss.item()\n",
    "    perplexity = np.exp(mean_loss_item)\n",
    "    print(\"Perplexity: {}\".format(perplexity))\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Elapsed Time: {:.2f} seconds\".format(elapsed_time))\n",
    "    list_perplexity.append(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'list_mean_loss' (list)\n"
     ]
    }
   ],
   "source": [
    "%store list_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(range(1, N_EPOCHS + 1), list_mean_loss, 'o-', color='red', label='Perplexity')\n",
    "# plt.title('Topics K = 50')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Perplexity')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# # plt.savefig(\"perplexity_50_topics.png\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
